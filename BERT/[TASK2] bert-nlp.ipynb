{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9939885,"sourceType":"datasetVersion","datasetId":6111192}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Практическое задание 3 \n\n# Классификация с использованием BERT  и Transfer learning\n\n## курс \"Математические методы анализа текстов\"\n\n\n### ФИО: Каратыщев Дмитрий Иванович","metadata":{}},{"cell_type":"markdown","source":"## Введение\n\n\nВ этом задании вы будете определять категории товара по данным из чеков, предоставленным в соревновании [Data Fusion Context](https://boosters.pro/championship/data_fusion/data).\n\n\nДля этого задания вам понадобятся следующие библиотеки:\n - [Pytorch](https://pytorch.org/).\n - [Transformers](https://github.com/huggingface/transformers).\n - [Tokenizers](https://github.com/huggingface/tokenizers).\n\nДанные лежат в архиве data.zip, в котором лежит файл `data.csv`, содержащий тексты и соответствующие им категории товаров. Все объекты поделены между train, test, val и unsupervised. Для unsupervised объектов категории товаров недоступны. \n\nСкачать данные можно здесь: [ссылка на google диск](https://drive.google.com/file/d/1AHs7qJYg2tc8zblGlT0Dpe50e6RW-gAW/view?usp=sharing)","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/nlp-task2/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:20:27.790121Z","iopub.execute_input":"2024-11-21T22:20:27.790480Z","iopub.status.idle":"2024-11-21T22:20:27.798095Z","shell.execute_reply.started":"2024-11-21T22:20:27.790448Z","shell.execute_reply":"2024-11-21T22:20:27.797289Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:20:27.811970Z","iopub.execute_input":"2024-11-21T22:20:27.812485Z","iopub.status.idle":"2024-11-21T22:20:27.820968Z","shell.execute_reply.started":"2024-11-21T22:20:27.812461Z","shell.execute_reply":"2024-11-21T22:20:27.820371Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import importlib.util\nimport os\n\n# Путь к файлу с дефисом в имени\nmodule_path = '/kaggle/input/nlp-task2/tests.py'\n\n# Загрузим модуль динамически\nspec = importlib.util.spec_from_file_location(\"tests\", module_path)\ntests = importlib.util.module_from_spec(spec)\nsys.modules[\"tests\"] = tests\nspec.loader.exec_module(tests)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:20:27.837723Z","iopub.execute_input":"2024-11-21T22:20:27.837955Z","iopub.status.idle":"2024-11-21T22:20:31.343098Z","shell.execute_reply.started":"2024-11-21T22:20:27.837932Z","shell.execute_reply":"2024-11-21T22:20:31.342403Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"sys.modules['tests']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:20:31.344645Z","iopub.execute_input":"2024-11-21T22:20:31.344971Z","iopub.status.idle":"2024-11-21T22:20:31.351269Z","shell.execute_reply.started":"2024-11-21T22:20:31.344947Z","shell.execute_reply":"2024-11-21T22:20:31.350368Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<module 'tests' from '/kaggle/input/nlp-task2/tests.py'>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Часть 1. Подготовка данных (2 балла)\n\nКлассические методы NLP (например, как мы делали в первом и втором домашнем задании) преобразуют тексты в списки индексов следующим образом:\n1. \"Очистка текста\" от плохих символов, приводим (или не приводим) текст к нижнему регистру.\n2. Текст делится по пробелам на слова.\n3. По полученной коллекции текстов строится словарь вида \"слово -> индекс\", редкие слова выбрасываются, стопслова иногда тоже\n4. Побитый на слова текст превращается в список индексов с помощью этого словаря.\n\nДля трансформеров схема выглядит немного по-другому — используются более продвинутые методы токенизации типа `wordpiece, bpe, sentencepiece`. Основное концептуальное отличие — текст делится не только на слова по пробелам, но и сами слова делятся на \"подслова\" (читай subwords). Это верно для BPE и wordpiece, а sentencepiece вообще не учитывает пробелы. Более подробно ознакомиться с этими методами токенизации можно в наших лекциях.\n\nВ данном задании предлагается использовать wordpiece токенизатор, который использовали в оригинальной статье про BERT. Построить его можно с помощью библиотеки `tokenizers`:\n1. Считайте данные с помощью `pandas`\n2. Используя метод `tokenizers.BertWordPieceTokenizer.train` и список сырых текстов постройте токенизатор. Используйте нижний регистр (lowercase), чистый текст (clean_text), без акцентов (strip_accents), размера словаря 30000 (vocab_size).\n3. Сохраните построенный токенизатор (метод `tokenizer.save_model`) и создайте объект класса `transformers.BertTokenizerFast`, который работает быстрее стандартной реализации, но не позволяет её обучать.\n\n**Важно:** нужно при обучении c помощью параметра `special_tokens` завести индексы для токенов `[PAD], [UNK], [CLS], [SEP], [MASK]`, которые понадобятся нам дальше для обучения и использования модели.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv('/kaggle/input/nlp-task2/task3_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:20:31.352165Z","iopub.execute_input":"2024-11-21T22:20:31.352439Z","iopub.status.idle":"2024-11-21T22:20:41.228348Z","shell.execute_reply.started":"2024-11-21T22:20:31.352413Z","shell.execute_reply":"2024-11-21T22:20:41.227391Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:20:41.230833Z","iopub.execute_input":"2024-11-21T22:20:41.231525Z","iopub.status.idle":"2024-11-21T22:20:41.264210Z","shell.execute_reply.started":"2024-11-21T22:20:41.231484Z","shell.execute_reply":"2024-11-21T22:20:41.263400Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3155764 entries, 0 to 3155763\nData columns (total 3 columns):\n #   Column  Dtype \n---  ------  ----- \n 0   text    object\n 1   label   int64 \n 2   split   object\ndtypes: int64(1), object(2)\nmemory usage: 72.2+ MB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizerFast\n\nimport os, shutil\n\n\nclass WordpieceTokenizer:\n\n    def __init__(\n            self, \n            vocab_path, \n            strip_accents=True, \n            clean_text=True, \n            lowercase=True\n    ):\n        \"\"\"\n            vocab_path: путь к словарю\n            strip_accents: очистка текста от акцентов\n            clean_text: просто чистка текста от непонятных символов\n            lowercase: приведение текста к нижнему регистру\n            \n            Подгружает токенизатор с помощью BertTokenizerFast.\n        \"\"\"\n        self._tokenizer = BertTokenizerFast(\n            vocab_file=vocab_path,\n            do_lower_case=lowercase,\n            clean_text=clean_text,\n            strip_accents=strip_accents\n        )\n\n    @classmethod\n    def from_corpus(\n            cls,\n            corpus,\n            corpus_save_path,\n            tokenizer_save_path,\n            tokenizer_name,\n            vocab_size=30000,\n            min_frequency=2,\n            strip_accents=True,\n            clean_text=True,\n            lowercase=True\n    ):\n        \"\"\"\n            corpus: список текстов\n            corpus_save_path: временный путь для сохранения текстов в текстовом файле\n            tokenizer_save_path: путь для сохранения файлов токенизатора\n            tokenizer_name: название токенизатора, влияет на названия файлов токенизатора\n            vocab_size: размер словаря\n            min_frequency: минимальная частота элемента в словаре\n            strip_accents: очистка текста от акцентов\n            clean_text: просто чистка текста от непонятных символов\n            lowercase: приведение текста к нижнему регистру\n            \n            С помощью списка сырых текстов формирует токенизатор\n        \"\"\"\n        os.makedirs(corpus_save_path, exist_ok=True)\n        os.makedirs(tokenizer_save_path, exist_ok=True)\n        corpus_file_name = os.path.join(corpus_save_path, 'corpus.txt')\n        with open(corpus_file_name, \"w\", encoding=\"utf-8\") as file:\n            for text in corpus:\n                file.write(text + \"\\n\")\n        tokenizer = BertWordPieceTokenizer(\n            clean_text=clean_text,\n            lowercase=lowercase,\n            strip_accents=strip_accents\n        )\n        tokenizer.train(\n            files=corpus_file_name,\n            vocab_size=vocab_size,\n            min_frequency=min_frequency,\n        )\n        tokenizer.save_model(tokenizer_save_path, tokenizer_name)\n        vocab_path = os.path.join(tokenizer_save_path, f'{tokenizer_name}-vocab.txt')\n        return cls(\n            vocab_path,\n            strip_accents,\n            clean_text,\n            lowercase\n        )\n\n\n    def __call__(self, text):\n        \"\"\"\n            text: str. Сырой текст\n            \n            returns: list of ints. Список индексов\n            \n            C помощью метода .encode преобразует текст в индексы.\n        \"\"\"\n        return self._tokenizer.encode(text, add_special_tokens=False)\n\n    @property\n    def cls_token_id(self):\n        \"\"\"\n            returns: индекс CLS токена\n        \"\"\"\n        return self._tokenizer.cls_token_id\n\n    @property\n    def pad_token_id(self):\n        return self._tokenizer.pad_token_id\n\n    @property\n    def mask_token_id(self):\n        return self._tokenizer.mask_token_id\n\n    @property\n    def sep_token_id(self):\n        return self._tokenizer.sep_token_id\n\n    def all_special_tokens(self):\n        return [self.cls_token_id, self.pad_token_id, \n                self.mask_token_id, self.sep_token_id]\n\n    @property\n    def vocab_size(self):\n        \"\"\"\n            returns: размер словаря\n        \"\"\"\n        return self._tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:20:41.265510Z","iopub.execute_input":"2024-11-21T22:20:41.265829Z","iopub.status.idle":"2024-11-21T22:20:42.370790Z","shell.execute_reply.started":"2024-11-21T22:20:41.265804Z","shell.execute_reply":"2024-11-21T22:20:42.369857Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Создайте токенизатор:","metadata":{}},{"cell_type":"code","source":"tokenizer = WordpieceTokenizer.from_corpus(\n    corpus=data.text.to_list(),\n    corpus_save_path='/kaggle/working/corpus_saved/',\n    tokenizer_save_path='/kaggle/working/tokenizer_saved/',\n    tokenizer_name='bert-wordpiece-tokenizer'\n)\n\ntests.test_tokenizer(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:20:42.372015Z","iopub.execute_input":"2024-11-21T22:20:42.372805Z","iopub.status.idle":"2024-11-21T22:21:27.525500Z","shell.execute_reply.started":"2024-11-21T22:20:42.372764Z","shell.execute_reply":"2024-11-21T22:21:27.524486Z"}},"outputs":[{"name":"stdout","text":"\n\n\nCorrect.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"tokenizer(\"Фантазёр. Ты меня называла!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:21:27.526731Z","iopub.execute_input":"2024-11-21T22:21:27.527001Z","iopub.status.idle":"2024-11-21T22:21:27.532972Z","shell.execute_reply.started":"2024-11-21T22:21:27.526975Z","shell.execute_reply":"2024-11-21T22:21:27.532118Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[3040, 16166, 139, 18, 3544, 17164, 1423, 6767, 252, 5]"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Нам доступно довольно большое количество неразмеченных данных, которое можно использовать для предобучения модели. Мы рассмотрим две задачи предобучения:\n1. **Masked Language Modeling** — задача из BERT. Выбираем 15% слов, из них 80% заменяем на токен [MASK], 10% меняем на другие случайные слова, 10% оставляем как есть. Эти 15% слов предсказываем моделью. Вспомним пример из оригинальной статьи:\n    * Исходный текст: `my dog is hairy`\n    * Выбираем случайным образом 15% токенов для задачи. Допустим, выбрали четвертый токен - `hairy`\n    * В 80% случаев заменяем токен на `[MASK]`: `my dog is [MASK]`\n    * В 10% случаев на другой случайный токен: `my dog is apple`\n    * В 10% случаев оставляем неизменным: `my dog is hairy`\n    \n    \n2. **Sentence Order Prediction** — задача из ALBERT. Делим текст на два сегмента, с вероятностью 50% меняет сегменты местами. Предсказываем, в правильном ли порядке находятся сегменты.\n    * Текст: `the man went to the store. he bought a gallon of milk`\n    * Токенизируем и делим его на два сегмента: `the man went to the store` и `he bought a gallon of milk`\n    * C вероятностью 50% меняем их местами: `[CLS] he bought a gallon of milk [SEP] the man went to the store`\n    * С вероятностью 50% оставляем на месте: `[CLS] the man went to the store [SEP] he bought a gallon of milk`\n    * \"Левому\" сегменту соответствует нулевой индекс сегмента, \"правому\" - индекс 1\n\nБольшая часть логики предобучения реализуется при подготовке данных.\n\nРеализуйте **PretrainDataset**, который токенизирует поданные сырые тексты и умеет возвращать для текста с конкретным индексом случайный сегмент длины, не большей чем `maxlen`. Логика для задачи **SOP** должна быть реализована в `__getitem__`: выбранный сегмент надо поделить на два равных сегмента, подбросить монетку, и с 50% вероятностью поменять сегменты местами. Нужно также добавить `[CLS]` и `[SEP]` токены.\n\n**hint:** чтобы существенно ускорить обучение (не потеряв при этом в качестве), после токенизации отсортируйте датасет по длине текстов.\n**hint:** токенизация датасета для предобучения занимает существенное время (5 минут), поэтому во время отладки стоит сделать её один раз и сохранить результат на диск","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport random\nfrom copy import deepcopy\n\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass PretrainDataset(Dataset):\n    \n    def __init__(\n            self, \n            corpus, \n            tokenizer, \n            minlen,\n            maxlen,\n            permute_prob=0.5, \n            verbose=False, \n            presort=False,\n            ds=None\n    ):\n        \"\"\"\n            corpus: list of strings. Список сырых текстов\n            tokenizer: токенизатор\n            minlen: минимально допустимая длина текста\n            permute_prob: вероятность, с которой два сегмента меняются местами (происходит swap)\n            maxlen: максимальная длина текста\n            verbose: вывод прогресса токенизации текстов с помощью tqdm\n            presort: отсортировать датасет по длинам токенизированных текстов (т.е. ds[0] выдает самый короткий текст)\n        \"\"\"\n        super().__init__()\n        attrs = [corpus, tokenizer, minlen, maxlen, permute_prob, verbose, presort]\n        attrs_names = [\"_corpus\", \"_tokenizer\", \"_minlen\", \"_maxlen\", \n                       \"_permute_prob\", \"_verbose\", \"_presort\"]\n        for idx, name in enumerate(attrs_names):\n            setattr(self, name, attrs[idx])\n\n        self._ds = []\n\n        if ds is not None:\n            self._ds = deepcopy(ds)\n        else:\n            for text in tqdm(self._corpus, disable=(not verbose)):\n                tokenized = self._tokenizer(text)\n                if len(tokenized) >= self._minlen:\n                    self._ds.append(tokenized)\n    \n            if presort:\n                self._ds.sort(key=len)\n        \n    def __len__(self):\n        return len(self._ds)\n    \n    @property\n    def tokenizer(self):\n        \"\"\"\n            returns: tokenizer. Нужно для тестов\n        \"\"\"\n        return self._tokenizer\n    \n    def set_maxlen(self, maxlen):\n        \"\"\"\n            maxlen: максимальная длина текста\n            \n            поставить новое максимальное значение длины\n        \"\"\"\n        self._maxlen = maxlen\n\n    def save_model(self):\n        names = [attr for attr in dir(self)]\n        state_dict = {name[1:]: getattr(self, name) for name in names}\n        torch.save(state_dict, '/kaggle/working/pretrain_dataset.pth')\n\n    @classmethod\n    def from_pretrained(cls, state_dict):\n        return cls(**state_dict)\n        \n    def __getitem__(self, idx):\n        \"\"\"\n            returns: \n                input_ids - тензор с индексами, \n                token_type_ids - тензор с сегментными айдишниками (0 у левого сегмента, 1 у правого),\n                permuted - был ли swap сегментов\n        \"\"\"\n        input_ids = self._ds[idx][:(self._maxlen - 3)]\n        left_segment = input_ids[:len(input_ids) // 2]\n        right_segment = input_ids[len(input_ids) // 2:]\n        cls = [self._tokenizer.cls_token_id]\n        sep = [self._tokenizer.sep_token_id]\n        permuted = bool(np.random.binomial(1, 0.5, 1)[0])\n\n        left_v = 0\n        right_v = 1\n        if permuted:\n            left_segment, right_segment = right_segment, left_segment\n            left_v, right_v = right_v, left_v\n            \n        token_type_ids = torch.tensor([left_v] * (len(left_segment) + 2) + [right_v] * (len(right_segment) + 1), \n                                      dtype=torch.int64)\n        input_ids = torch.tensor(cls + left_segment + sep + right_segment + sep, \n                                 dtype=torch.int64)\n        \n        return input_ids, token_type_ids, permuted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:21:27.533922Z","iopub.execute_input":"2024-11-21T22:21:27.534317Z","iopub.status.idle":"2024-11-21T22:21:27.546769Z","shell.execute_reply.started":"2024-11-21T22:21:27.534291Z","shell.execute_reply":"2024-11-21T22:21:27.545994Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Длину текстов нужно ограничить снизу. Нет смысла рассматривать слишком короткие тексты (например, единичную длину), для которых задачи предобучения вообще не работают. Длинные тексты более эффективны для задач типа MLM, так как у модели больше контекста для предсказания и больше таргетов на один объект.\n\nРазумный способ определить минимальную длину текстов для MLM  — подобрать такую минимальную длину, чтобы вероятность замаскировать хотя бы одно слово в тексте была больше заданного порога.\n\nТ.е. если мы каждое слово маскируем с вероятностью 15%, какой длины должен быть текст, чтобы с вероятностью $\\geqslant$ 50% было замаскировано хотя бы одно слово?\n\nИспользуйте ответ на данный вопрос как минимальную допустимую длину текстов:","metadata":{}},{"cell_type":"markdown","source":"Имеем следущие условия: $n$ - длина текста, $p = 0.15$, $\\xi \\sim Bi(n, p)$. Нам нужно найти такое $n$, чтобы $\\mathbb{P}(\\xi \\geq 1) \\geq 0.5$.\n\nВ общем случае имеем $\\mathbb{P}(\\xi \\leq k) = \\begin{pmatrix} n \\\\ k \\end{pmatrix} p^k (1 - p)^{n - k}$\n\nТогда $\\mathbb{P}(\\xi \\geq 1) = 1 - \\mathbb{P}(\\xi < 1) = 1 - \\mathbb{P}(\\xi = 0) = 1 - (1 - p)^{n}$\n\nТо есть нам нужно такое $n$, что $1 - (1 - p)^n \\geq 0.5 \\iff (1 - p)^n \\leq 0.5 \\iff n \\log_2(1 - p) \\leq \\log_2(0.5) \\iff$\n\n$\\iff n \\geq \\dfrac{\\log_2(0.5)}{\\log_2(1 - p)} \\iff n \\geq -\\dfrac{1}{\\log_2(1 - p)}$\n\nПолучаем, что $n = \\left\\lceil -\\dfrac{1}{\\log_2(1 - p)} \\right\\rceil = 5$","metadata":{}},{"cell_type":"code","source":"minlen = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:21:27.547690Z","iopub.execute_input":"2024-11-21T22:21:27.548038Z","iopub.status.idle":"2024-11-21T22:21:27.561384Z","shell.execute_reply.started":"2024-11-21T22:21:27.548001Z","shell.execute_reply":"2024-11-21T22:21:27.560774Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Создайте датасет (с произвольным разумным значением maxlen):","metadata":{}},{"cell_type":"code","source":"ds = PretrainDataset(\n    corpus=data.text.to_list(), \n    tokenizer=tokenizer, \n    minlen=5,\n    maxlen=512,\n    permute_prob=0.5, \n    verbose=True, \n    presort=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:21:27.563951Z","iopub.execute_input":"2024-11-21T22:21:27.564237Z","iopub.status.idle":"2024-11-21T22:25:20.174257Z","shell.execute_reply.started":"2024-11-21T22:21:27.564214Z","shell.execute_reply":"2024-11-21T22:25:20.173408Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 3155764/3155764 [03:52<00:00, 13594.41it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"tests.test_dataset(ds)","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:25:20.175266Z","iopub.execute_input":"2024-11-21T22:25:20.175566Z","iopub.status.idle":"2024-11-21T22:25:20.205240Z","shell.execute_reply.started":"2024-11-21T22:25:20.175539Z","shell.execute_reply":"2024-11-21T22:25:20.204567Z"}},"outputs":[{"name":"stdout","text":"Correct.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Длину текстов нужно как-то ограничить сверху. Иначе, если встретится какой-то очень-очень длинный текст, он не поместится в видеопамять. Самый простой способ определить ограничение по длине  — после токенизации построить гистограмму длин (например, используя **sns.distplot**) и методом пристального взгляда определить разумное ограничение длины. Другой вариант  — взять большое значение квантили.\n\n**Вопрос:** какая максимальная длина текста подходит для этого датасета?\n\n**Ответ:** 27","metadata":{}},{"cell_type":"code","source":"lens = [item.shape[0] for item, _, _ in ds]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:25:20.206215Z","iopub.execute_input":"2024-11-21T22:25:20.206487Z","iopub.status.idle":"2024-11-21T22:26:08.184890Z","shell.execute_reply.started":"2024-11-21T22:25:20.206463Z","shell.execute_reply":"2024-11-21T22:26:08.184069Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import seaborn as sns\n\nsns.displot(lens, bins=20)\nplt.xlabel('Длина токенизированных текстов')\nplt.ylabel('Частота')\nplt.title('Распределение длины токенизированных текстов')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:26:08.186135Z","iopub.execute_input":"2024-11-21T22:26:08.186430Z","iopub.status.idle":"2024-11-21T22:26:11.888665Z","shell.execute_reply.started":"2024-11-21T22:26:08.186404Z","shell.execute_reply":"2024-11-21T22:26:11.887645Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x500 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"359.446358pt\" height=\"380.872812pt\" viewBox=\"0 0 359.446358 380.872812\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-11-21T22:26:11.836416</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.5, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 380.872812 \nL 359.446358 380.872812 \nL 359.446358 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 343.316562 \nL 340.322716 343.316562 \nL 340.322716 32.916562 \nL 43.78125 32.916562 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 57.260408 343.316562 \nL 70.739565 343.316562 \nL 70.739565 47.697515 \nL 57.260408 47.697515 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 70.739565 343.316562 \nL 84.218723 343.316562 \nL 84.218723 194.676143 \nL 70.739565 194.676143 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 84.218723 343.316562 \nL 97.69788 343.316562 \nL 97.69788 274.599081 \nL 84.218723 274.599081 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 97.69788 343.316562 \nL 111.177038 343.316562 \nL 111.177038 323.184196 \nL 97.69788 323.184196 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 111.177038 343.316562 \nL 124.656195 343.316562 \nL 124.656195 334.004481 \nL 111.177038 334.004481 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 124.656195 343.316562 \nL 138.135353 343.316562 \nL 138.135353 340.495048 \nL 124.656195 340.495048 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 138.135353 343.316562 \nL 151.61451 343.316562 \nL 151.61451 341.834024 \nL 138.135353 341.834024 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 151.61451 343.316562 \nL 165.093668 343.316562 \nL 165.093668 342.853796 \nL 151.61451 342.853796 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 165.093668 343.316562 \nL 178.572825 343.316562 \nL 178.572825 343.087585 \nL 165.093668 343.087585 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 178.572825 343.316562 \nL 192.051983 343.316562 \nL 192.051983 343.242776 \nL 178.572825 343.242776 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 192.051983 343.316562 \nL 205.53114 343.316562 \nL 205.53114 343.251198 \nL 192.051983 343.251198 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 205.53114 343.316562 \nL 219.010298 343.316562 \nL 219.010298 343.257213 \nL 205.53114 343.257213 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 219.010298 343.316562 \nL 232.489455 343.316562 \nL 232.489455 343.294707 \nL 219.010298 343.294707 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 232.489455 343.316562 \nL 245.968613 343.316562 \nL 245.968613 343.302928 \nL 232.489455 343.302928 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path d=\"M 245.968613 343.316562 \nL 259.447771 343.316562 \nL 259.447771 343.312552 \nL 245.968613 343.312552 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 259.447771 343.316562 \nL 272.926928 343.316562 \nL 272.926928 343.31576 \nL 259.447771 343.31576 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 272.926928 343.316562 \nL 286.406086 343.316562 \nL 286.406086 343.316161 \nL 272.926928 343.316161 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 286.406086 343.316562 \nL 299.885243 343.316562 \nL 299.885243 343.316562 \nL 286.406086 343.316562 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 299.885243 343.316562 \nL 313.364401 343.316562 \nL 313.364401 343.316562 \nL 299.885243 343.316562 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path d=\"M 313.364401 343.316562 \nL 326.843558 343.316562 \nL 326.843558 343.316362 \nL 313.364401 343.316362 \nz\n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: #1f77b4; fill-opacity: 0.75; stroke: #000000; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 93.608698 343.316562 \nL 93.608698 32.916562 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m1fc4bfa9bd\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m1fc4bfa9bd\" x=\"93.608698\" y=\"343.316562\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 20 -->\n      <g transform=\"translate(87.246198 357.915) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 154.189181 343.316562 \nL 154.189181 32.916562 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m1fc4bfa9bd\" x=\"154.189181\" y=\"343.316562\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 40 -->\n      <g transform=\"translate(147.826681 357.915) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 214.769664 343.316562 \nL 214.769664 32.916562 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m1fc4bfa9bd\" x=\"214.769664\" y=\"343.316562\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 60 -->\n      <g transform=\"translate(208.407164 357.915) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 275.350147 343.316562 \nL 275.350147 32.916562 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m1fc4bfa9bd\" x=\"275.350147\" y=\"343.316562\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 80 -->\n      <g transform=\"translate(268.987647 357.915) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 335.930631 343.316562 \nL 335.930631 32.916562 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m1fc4bfa9bd\" x=\"335.930631\" y=\"343.316562\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <g transform=\"translate(326.386881 357.915) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Длина токенизированных текстов -->\n     <g transform=\"translate(101.328545 371.593125) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-414\" d=\"M 1459 531 \nL 3522 531 \nL 3522 4134 \nL 2006 4134 \nL 2006 3472 \nQ 2006 1913 1656 878 \nQ 1584 666 1459 531 \nz\nM 538 531 \nQ 956 728 1075 1103 \nQ 1378 2066 1378 3784 \nL 1378 4666 \nL 4153 4666 \nL 4153 531 \nL 4684 531 \nL 4684 -1003 \nL 4153 -1003 \nL 4153 0 \nL 847 0 \nL 847 -1003 \nL 316 -1003 \nL 316 531 \nL 538 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-43b\" d=\"M 238 0 \nL 238 478 \nQ 806 566 981 959 \nQ 1194 1513 1194 2928 \nL 1194 3500 \nL 3559 3500 \nL 3559 0 \nL 2984 0 \nL 2984 3041 \nL 1769 3041 \nL 1769 2694 \nQ 1769 1344 1494 738 \nQ 1200 91 238 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-438\" d=\"M 3578 3500 \nL 3578 0 \nL 3006 0 \nL 3006 2809 \nL 1319 0 \nL 581 0 \nL 581 3500 \nL 1153 3500 \nL 1153 697 \nL 2838 3500 \nL 3578 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-43d\" d=\"M 581 3500 \nL 1159 3500 \nL 1159 2072 \nL 3025 2072 \nL 3025 3500 \nL 3603 3500 \nL 3603 0 \nL 3025 0 \nL 3025 1613 \nL 1159 1613 \nL 1159 0 \nL 581 0 \nL 581 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-430\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-442\" d=\"M 188 3500 \nL 3541 3500 \nL 3541 3041 \nL 2147 3041 \nL 2147 0 \nL 1581 0 \nL 1581 3041 \nL 188 3041 \nL 188 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-43e\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-43a\" d=\"M 581 3500 \nL 1153 3500 \nL 1153 1856 \nL 2775 3500 \nL 3481 3500 \nL 2144 2147 \nL 3653 0 \nL 3009 0 \nL 1769 1766 \nL 1153 1141 \nL 1153 0 \nL 581 0 \nL 581 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-435\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-437\" d=\"M 2206 1888 \nQ 2594 1813 2809 1584 \nQ 3025 1356 3025 1019 \nQ 3025 478 2622 193 \nQ 2219 -91 1475 -91 \nQ 1225 -91 961 -47 \nQ 697 -3 416 84 \nL 416 619 \nQ 638 503 903 445 \nQ 1169 388 1459 388 \nQ 1931 388 2195 563 \nQ 2459 738 2459 1019 \nQ 2459 1294 2225 1461 \nQ 1991 1628 1563 1628 \nL 1100 1628 \nL 1100 2103 \nL 1584 2103 \nQ 1947 2103 2158 2242 \nQ 2369 2381 2369 2600 \nQ 2369 2800 2151 2944 \nQ 1934 3088 1563 3088 \nQ 1341 3088 1089 3047 \nQ 838 3006 534 2916 \nL 534 3438 \nQ 841 3513 1106 3550 \nQ 1372 3588 1609 3588 \nQ 2222 3588 2576 3342 \nQ 2931 3097 2931 2656 \nQ 2931 2366 2743 2164 \nQ 2556 1963 2206 1888 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-440\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-432\" d=\"M 1156 1613 \nL 1156 459 \nL 1975 459 \nQ 2369 459 2575 607 \nQ 2781 756 2781 1038 \nQ 2781 1319 2575 1466 \nQ 2369 1613 1975 1613 \nL 1156 1613 \nz\nM 1156 3041 \nL 1156 2072 \nL 1913 2072 \nQ 2238 2072 2444 2201 \nQ 2650 2331 2650 2563 \nQ 2650 2794 2444 2917 \nQ 2238 3041 1913 3041 \nL 1156 3041 \nz\nM 581 3500 \nL 1950 3500 \nQ 2566 3500 2897 3275 \nQ 3228 3050 3228 2634 \nQ 3228 2313 3059 2123 \nQ 2891 1934 2559 1888 \nQ 2956 1813 3175 1575 \nQ 3394 1338 3394 981 \nQ 3394 513 3033 256 \nQ 2672 0 2003 0 \nL 581 0 \nL 581 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-44b\" d=\"M 3907 3500 \nL 4482 3500 \nL 4482 0 \nL 3907 0 \nL 3907 3500 \nz\nM 4195 3584 \nL 4195 3584 \nz\nM 2781 1038 \nQ 2781 1319 2576 1466 \nQ 2372 1613 1978 1613 \nL 1159 1613 \nL 1159 459 \nL 1978 459 \nQ 2372 459 2576 607 \nQ 2781 756 2781 1038 \nz\nM 581 3500 \nL 1159 3500 \nL 1159 2072 \nL 2003 2072 \nQ 2672 2072 3033 1817 \nQ 3394 1563 3394 1038 \nQ 3394 513 3033 256 \nQ 2672 0 2003 0 \nL 581 0 \nL 581 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-445\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-441\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-414\"/>\n      <use xlink:href=\"#DejaVuSans-43b\" x=\"78.125\"/>\n      <use xlink:href=\"#DejaVuSans-438\" x=\"142.041016\"/>\n      <use xlink:href=\"#DejaVuSans-43d\" x=\"207.03125\"/>\n      <use xlink:href=\"#DejaVuSans-430\" x=\"272.412109\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"333.691406\"/>\n      <use xlink:href=\"#DejaVuSans-442\" x=\"365.478516\"/>\n      <use xlink:href=\"#DejaVuSans-43e\" x=\"423.730469\"/>\n      <use xlink:href=\"#DejaVuSans-43a\" x=\"484.912109\"/>\n      <use xlink:href=\"#DejaVuSans-435\" x=\"545.3125\"/>\n      <use xlink:href=\"#DejaVuSans-43d\" x=\"606.835938\"/>\n      <use xlink:href=\"#DejaVuSans-438\" x=\"672.216797\"/>\n      <use xlink:href=\"#DejaVuSans-437\" x=\"737.207031\"/>\n      <use xlink:href=\"#DejaVuSans-438\" x=\"790.380859\"/>\n      <use xlink:href=\"#DejaVuSans-440\" x=\"855.371094\"/>\n      <use xlink:href=\"#DejaVuSans-43e\" x=\"918.847656\"/>\n      <use xlink:href=\"#DejaVuSans-432\" x=\"980.029297\"/>\n      <use xlink:href=\"#DejaVuSans-430\" x=\"1038.964844\"/>\n      <use xlink:href=\"#DejaVuSans-43d\" x=\"1100.244141\"/>\n      <use xlink:href=\"#DejaVuSans-43d\" x=\"1165.625\"/>\n      <use xlink:href=\"#DejaVuSans-44b\" x=\"1231.005859\"/>\n      <use xlink:href=\"#DejaVuSans-445\" x=\"1309.960938\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"1369.140625\"/>\n      <use xlink:href=\"#DejaVuSans-442\" x=\"1400.927734\"/>\n      <use xlink:href=\"#DejaVuSans-435\" x=\"1459.179688\"/>\n      <use xlink:href=\"#DejaVuSans-43a\" x=\"1520.703125\"/>\n      <use xlink:href=\"#DejaVuSans-441\" x=\"1581.103516\"/>\n      <use xlink:href=\"#DejaVuSans-442\" x=\"1636.083984\"/>\n      <use xlink:href=\"#DejaVuSans-43e\" x=\"1694.335938\"/>\n      <use xlink:href=\"#DejaVuSans-432\" x=\"1755.517578\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 43.78125 343.316562 \nL 340.322716 343.316562 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"m502f840aa0\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m502f840aa0\" x=\"43.78125\" y=\"343.316562\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 347.115781) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 43.78125 303.215442 \nL 340.322716 303.215442 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m502f840aa0\" x=\"43.78125\" y=\"303.215442\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 307.01466) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 43.78125 263.114321 \nL 340.322716 263.114321 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m502f840aa0\" x=\"43.78125\" y=\"263.114321\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 266.913539) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 43.78125 223.0132 \nL 340.322716 223.0132 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m502f840aa0\" x=\"43.78125\" y=\"223.0132\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 226.812418) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 43.78125 182.912079 \nL 340.322716 182.912079 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m502f840aa0\" x=\"43.78125\" y=\"182.912079\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 186.711297) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path d=\"M 43.78125 142.810958 \nL 340.322716 142.810958 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#m502f840aa0\" x=\"43.78125\" y=\"142.810958\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 146.610176) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_23\">\n      <path d=\"M 43.78125 102.709837 \nL 340.322716 102.709837 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#m502f840aa0\" x=\"43.78125\" y=\"102.709837\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.2 -->\n      <g transform=\"translate(20.878125 106.509055) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_25\">\n      <path d=\"M 43.78125 62.608716 \nL 340.322716 62.608716 \n\" clip-path=\"url(#pb9bc83988f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#m502f840aa0\" x=\"43.78125\" y=\"62.608716\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.4 -->\n      <g transform=\"translate(20.878125 66.407934) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Частота -->\n     <g transform=\"translate(14.798438 209.306406) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-427\" d=\"M 3125 0 \nL 3125 1913 \nL 1822 1913 \nQ 1241 1913 894 2277 \nQ 547 2641 547 3384 \nL 547 4666 \nL 1175 4666 \nL 1175 3434 \nQ 1175 2938 1369 2691 \nQ 1563 2444 1950 2444 \nL 3125 2444 \nL 3125 4666 \nL 3759 4666 \nL 3759 0 \nL 3125 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-427\"/>\n      <use xlink:href=\"#DejaVuSans-430\" x=\"68.554688\"/>\n      <use xlink:href=\"#DejaVuSans-441\" x=\"129.833984\"/>\n      <use xlink:href=\"#DejaVuSans-442\" x=\"184.814453\"/>\n      <use xlink:href=\"#DejaVuSans-43e\" x=\"243.066406\"/>\n      <use xlink:href=\"#DejaVuSans-442\" x=\"304.248047\"/>\n      <use xlink:href=\"#DejaVuSans-430\" x=\"362.5\"/>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- 1e6 -->\n     <g transform=\"translate(43.78125 29.916562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-36\" x=\"125.146484\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 43.78125 343.316562 \nL 43.78125 32.916562 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 43.78125 343.316562 \nL 340.322716 343.316562 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Распределение длины токенизированных текстов -->\n    <g transform=\"translate(31.857608 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-420\" d=\"M 1259 4147 \nL 1259 2394 \nL 2053 2394 \nQ 2494 2394 2734 2622 \nQ 2975 2850 2975 3272 \nQ 2975 3691 2734 3919 \nQ 2494 4147 2053 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 2053 4666 \nQ 2838 4666 3239 4311 \nQ 3641 3956 3641 3272 \nQ 3641 2581 3239 2228 \nQ 2838 1875 2053 1875 \nL 1259 1875 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-43f\" d=\"M 3603 3500 \nL 3603 0 \nL 3025 0 \nL 3025 3041 \nL 1159 3041 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 3603 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-434\" d=\"M 1384 459 \nL 3053 459 \nL 3053 3041 \nL 1844 3041 \nL 1844 2603 \nQ 1844 1316 1475 628 \nL 1384 459 \nz\nM 550 459 \nQ 834 584 959 850 \nQ 1266 1509 1266 2838 \nL 1266 3500 \nL 3631 3500 \nL 3631 459 \nL 4091 459 \nL 4091 -884 \nL 3631 -884 \nL 3631 0 \nL 794 0 \nL 794 -884 \nL 334 -884 \nL 334 459 \nL 550 459 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-420\"/>\n     <use xlink:href=\"#DejaVuSans-430\" x=\"60.302734\"/>\n     <use xlink:href=\"#DejaVuSans-441\" x=\"121.582031\"/>\n     <use xlink:href=\"#DejaVuSans-43f\" x=\"176.5625\"/>\n     <use xlink:href=\"#DejaVuSans-440\" x=\"241.943359\"/>\n     <use xlink:href=\"#DejaVuSans-435\" x=\"305.419922\"/>\n     <use xlink:href=\"#DejaVuSans-434\" x=\"366.943359\"/>\n     <use xlink:href=\"#DejaVuSans-435\" x=\"436.083984\"/>\n     <use xlink:href=\"#DejaVuSans-43b\" x=\"497.607422\"/>\n     <use xlink:href=\"#DejaVuSans-435\" x=\"561.523438\"/>\n     <use xlink:href=\"#DejaVuSans-43d\" x=\"623.046875\"/>\n     <use xlink:href=\"#DejaVuSans-438\" x=\"688.427734\"/>\n     <use xlink:href=\"#DejaVuSans-435\" x=\"753.417969\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"814.941406\"/>\n     <use xlink:href=\"#DejaVuSans-434\" x=\"846.728516\"/>\n     <use xlink:href=\"#DejaVuSans-43b\" x=\"915.869141\"/>\n     <use xlink:href=\"#DejaVuSans-438\" x=\"979.785156\"/>\n     <use xlink:href=\"#DejaVuSans-43d\" x=\"1044.775391\"/>\n     <use xlink:href=\"#DejaVuSans-44b\" x=\"1110.15625\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"1189.111328\"/>\n     <use xlink:href=\"#DejaVuSans-442\" x=\"1220.898438\"/>\n     <use xlink:href=\"#DejaVuSans-43e\" x=\"1279.150391\"/>\n     <use xlink:href=\"#DejaVuSans-43a\" x=\"1340.332031\"/>\n     <use xlink:href=\"#DejaVuSans-435\" x=\"1400.732422\"/>\n     <use xlink:href=\"#DejaVuSans-43d\" x=\"1462.255859\"/>\n     <use xlink:href=\"#DejaVuSans-438\" x=\"1527.636719\"/>\n     <use xlink:href=\"#DejaVuSans-437\" x=\"1592.626953\"/>\n     <use xlink:href=\"#DejaVuSans-438\" x=\"1645.800781\"/>\n     <use xlink:href=\"#DejaVuSans-440\" x=\"1710.791016\"/>\n     <use xlink:href=\"#DejaVuSans-43e\" x=\"1774.267578\"/>\n     <use xlink:href=\"#DejaVuSans-432\" x=\"1835.449219\"/>\n     <use xlink:href=\"#DejaVuSans-430\" x=\"1894.384766\"/>\n     <use xlink:href=\"#DejaVuSans-43d\" x=\"1955.664062\"/>\n     <use xlink:href=\"#DejaVuSans-43d\" x=\"2021.044922\"/>\n     <use xlink:href=\"#DejaVuSans-44b\" x=\"2086.425781\"/>\n     <use xlink:href=\"#DejaVuSans-445\" x=\"2165.380859\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"2224.560547\"/>\n     <use xlink:href=\"#DejaVuSans-442\" x=\"2256.347656\"/>\n     <use xlink:href=\"#DejaVuSans-435\" x=\"2314.599609\"/>\n     <use xlink:href=\"#DejaVuSans-43a\" x=\"2376.123047\"/>\n     <use xlink:href=\"#DejaVuSans-441\" x=\"2436.523438\"/>\n     <use xlink:href=\"#DejaVuSans-442\" x=\"2491.503906\"/>\n     <use xlink:href=\"#DejaVuSans-43e\" x=\"2549.755859\"/>\n     <use xlink:href=\"#DejaVuSans-432\" x=\"2610.9375\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb9bc83988f\">\n   <rect x=\"43.78125\" y=\"32.916562\" width=\"296.541466\" height=\"310.4\"/>\n  </clipPath>\n </defs>\n</svg>\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"Присвойте максимально допустимое значение длины:","metadata":{}},{"cell_type":"code","source":"print(np.quantile(lens, [0.95, 0.97, 0.98]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:26:11.889797Z","iopub.execute_input":"2024-11-21T22:26:11.891210Z","iopub.status.idle":"2024-11-21T22:26:12.027109Z","shell.execute_reply.started":"2024-11-21T22:26:11.891164Z","shell.execute_reply":"2024-11-21T22:26:12.026404Z"}},"outputs":[{"name":"stdout","text":"[23. 25. 27.]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"ds.set_maxlen(27)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:26:12.028120Z","iopub.execute_input":"2024-11-21T22:26:12.028449Z","iopub.status.idle":"2024-11-21T22:26:12.032626Z","shell.execute_reply.started":"2024-11-21T22:26:12.028421Z","shell.execute_reply":"2024-11-21T22:26:12.031730Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"MAX_SEQLEN = 27","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:26:12.033603Z","iopub.execute_input":"2024-11-21T22:26:12.033880Z","iopub.status.idle":"2024-11-21T22:26:12.043310Z","shell.execute_reply.started":"2024-11-21T22:26:12.033854Z","shell.execute_reply":"2024-11-21T22:26:12.042470Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"Если для задачи **SOP** мы готовим данные при индексации датасета `PretrainDataset`, то маскирование для задачи **MLM** удобней делать в Collator'е в тензорном виде.\n\nКак с вероятностью `15%` заменить в тензоре `input_ids` значения на `0`: \n\n1. `mask = torch.rand(input_ids.shape) < 0.15`\n2. `input_ids = torch.where(mask, 0, input_ids)`\n\nКак сгенерировать случайные элементы словаря на каждый элемент батча: `torch.randint_like(input_ids, low=num_special_tokens, high=self._tokenizer.vocab_size)`.\n\nВ `Collator` нужно также:\n1. сделать паддинг.\n2. из (примерно) 15% выбранных токенов 10% поменять на случайные и 10% оставить в исходном виде, остальные замаскировать\n3. сформировать таргеты. Нам нужно понимать, какие именно 15% токенов мы выбрали для предсказания + какие были исходные метки для них.\n\nВажно: `[CLS]` и другие специальные токены токены маскировать не надо","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass Collator:\n    \n    def __init__(\n            self,\n            tokenizer,\n            non_target_idx=-100,\n            mask_prob=0.15,\n            random_prob=0.1,\n            keep_unchanged_prob=0.1\n    ):\n        \"\"\"\n            tokenizer: токенизатор\n            non_target_idx: значение для индексов, не использующихся как таргеты. \n                Используйте его, чтобы пометить \"не таргет\" токены\n            mask_prob: вероятность выбрать индекс как таргет\n            random_prob: вероятность для уже выбранного индекса поменять его на случайное значение вместо маскирования\n            keen_unchanged_prob: вероятность оставить индекс в исходном виде вместо маскирования\n        \"\"\"\n        names = [\"_tokenizer\", \"_non_target_idx\", \"_mask_prob\", \"_random_prob\", \"_keep_unchanged_prob\"]\n        values = [tokenizer, non_target_idx, mask_prob, random_prob, keep_unchanged_prob]\n        for name, value in zip(names, values):\n            setattr(self, name, value)\n\n    def pad_sequence_with_last_element(self, sequences, batch_first=False):\n        \"\"\"\n        Дополняет список последовательностей последними элементами до длины самой длинной последовательности.\n        \n        Args:\n            sequences (list of torch.Tensor): Список последовательностей, каждая из которых — это 1D тензор.\n            batch_first (bool): Если True, возвращает тензор с размерностью (batch_size, max_len).\n                                Если False, возвращает тензор с размерностью (max_len, batch_size).\n        \n        Returns:\n            torch.Tensor: Батч дополненных последовательностей.\n        \"\"\"\n        # Определяем длины всех последовательностей\n        lengths = [seq.size(0) for seq in sequences]\n        max_len = max(lengths)\n    \n        # Создаём пустой тензор для дополненных последовательностей\n        padded_sequences = torch.zeros((len(sequences), max_len), dtype=sequences[0].dtype)\n    \n        for i, seq in enumerate(sequences):\n            seq_len = seq.size(0)\n            padded_sequences[i, :seq_len] = seq\n            if seq_len < max_len:\n                # Дополняем последовательность последним элементом\n                padded_sequences[i, seq_len:] = seq[-1]\n    \n        if not batch_first:\n            padded_sequences = padded_sequences.transpose(0, 1)\n    \n        return padded_sequences\n        \n    def __call__(self, batch):\n        \"\"\"\n            batch: список вида [ds[i] for i in [12, 3, 2, 5]]\n            \n            returns: \n                input_ids: испорченные входные индексы токенов с замаскированными значениями\n                token_type_ids: сегментные эмбеддинги\n                labels: истинные значения входных индексов, как таргеты\n                permuted: был ли свап сегментов\n        \"\"\"\n        input_ids, token_type_ids, permuted = zip(*batch)\n\n        input_ids = pad_sequence(input_ids, \n                                 batch_first=True, \n                                 padding_value=self._tokenizer.pad_token_id)\n        token_type_ids = self.pad_sequence_with_last_element(token_type_ids, batch_first=True)\n\n        target_mask = (torch.rand(input_ids.shape) < self._mask_prob).to(input_ids.device)\n        labels = torch.zeros_like(input_ids, dtype=torch.int64)\n        labels = torch.where(target_mask, input_ids, self._non_target_idx).to(input_ids.device)\n\n        special_token_mask = torch.tensor([\n            [1 if elem in self._tokenizer.all_special_tokens() else 0 for elem in item]\n            for item in input_ids\n        ], dtype=torch.bool, device=input_ids.device)\n\n        target_mask &= ~special_token_mask\n\n         # Генерация случайных замен\n        random_tokens = torch.randint_like(\n            input_ids, low=len(self._tokenizer.all_special_tokens()), high=self._tokenizer.vocab_size\n        )\n\n        # Маски для замены: 10% случайные токены, 10% оставить как есть, остальное маскировать\n        random_mask = (torch.rand(input_ids.shape) < self._random_prob) & target_mask\n        unchanged_mask = (torch.rand(input_ids.shape) < self._keep_unchanged_prob) & target_mask & ~random_mask\n        mask_token = self._tokenizer.mask_token_id\n\n        # Применение масок к input_ids\n        input_ids = torch.where(random_mask, random_tokens, input_ids)\n        input_ids = torch.where(target_mask & ~unchanged_mask & ~random_mask, mask_token, input_ids)\n        \n        return input_ids, token_type_ids, labels, torch.tensor(permuted, device=input_ids.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:26:12.044687Z","iopub.execute_input":"2024-11-21T22:26:12.044935Z","iopub.status.idle":"2024-11-21T22:26:12.058164Z","shell.execute_reply.started":"2024-11-21T22:26:12.044912Z","shell.execute_reply":"2024-11-21T22:26:12.057385Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"Создайте `collator` и `dataloader`. Для предобучения предлагается использовать большой `batch_size`.","metadata":{}},{"cell_type":"code","source":"collator = Collator(tokenizer, non_target_idx=-100)\n\ndl = DataLoader(\n    ds, \n    collate_fn=collator, \n    batch_size=256, \n    shuffle=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:26:12.059146Z","iopub.execute_input":"2024-11-21T22:26:12.059411Z","iopub.status.idle":"2024-11-21T22:26:12.072096Z","shell.execute_reply.started":"2024-11-21T22:26:12.059373Z","shell.execute_reply":"2024-11-21T22:26:12.071241Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tests.test_collator(ds, collator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:26:12.073031Z","iopub.execute_input":"2024-11-21T22:26:12.073236Z","iopub.status.idle":"2024-11-21T22:26:12.288169Z","shell.execute_reply.started":"2024-11-21T22:26:12.073215Z","shell.execute_reply":"2024-11-21T22:26:12.287284Z"}},"outputs":[{"name":"stdout","text":"Correct.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Часть 2. Создание модели (2 балла)","metadata":{}},{"cell_type":"markdown","source":"В качестве модели мы будем использовать энкодер трансформера точь-в-точь в таком же виде, как его использовали в оригинальной статье про BERT. \n\nПрежде чем начать писать составляющие энкодера, обсудим инициализацию весов. Для трансформера нам понадобится большое количество линейных слов (`nn.Linear`), у которых для инициализации по дефолту используется равномерное распределение и отсутствует зануление bias'ов: $$\\text{Uniform} \\left( -\\frac{1}{ \\sqrt{N_{\\text{in\\_features}}} }, \\frac{1}{ \\sqrt{N_{\\text{in\\_features}}} } \\right)$$\n\nВ оригинальной статье про BERT для весов используется **TruncatedNormal** со стандартным отклонением 0.02, bias'ы инициализируются нулями и модель обучается значительно лучше (это можно в ходе домашнего задания проверить).\n\nПоэтому, после создания линейных слоев и матрицы эмбеддингов, необходимо в явном виде вызывать для них TruncatedNormal инициализацию:\n\n1. `layer = ...`\n2. `nn.init.trunc_normal_(layer.weight.data, std=0.02, a=-2 * 0.02, b=2 * 0.02)`.\n\nДля линейных слоев нужно также вызывать `layer.bias.data.zero_()`.\n\n\n**TruncatedNormal** распределение отличается от нормального тем, что если величины выходят за границы отрезка [a, b], для этих величин повторно происходит сэмплирование до тех пор, пока они не попадут в нужный отрезок. Для BERT stddev = 0.02:\n\n$$[a; b] = [- 2  \\cdot \\text{stddev}; 2 \\cdot \\text{stddev}].$$\n\nНапишите функцию для инициализации линейных слоев и матрицы эмбеддингов **TruncatedNormal** распределением:","metadata":{}},{"cell_type":"code","source":"from torch import nn\n\n\ndef init_layer(layer, initializer_range=0.02, zero_out_bias=True):\n    \"\"\"\n        layer: наследник nn.Module, т.е. слой в pytorch\n        initializer_range: stddev для truncated normal\n        zero_out_bias: True для линейных слоев, False для матрицы эмбеддингов\n    \"\"\"\n    nn.init.trunc_normal_(layer.weight.data, \n                          std=initializer_range, \n                          a=-2 * initializer_range, \n                          b=2 * initializer_range)\n    if zero_out_bias:\n        layer.bias.data.zero_()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:52.797345Z","iopub.execute_input":"2024-11-21T22:43:52.797756Z","iopub.status.idle":"2024-11-21T22:43:52.803521Z","shell.execute_reply.started":"2024-11-21T22:43:52.797725Z","shell.execute_reply":"2024-11-21T22:43:52.802524Z"}},"outputs":[],"execution_count":185},{"cell_type":"markdown","source":"Приступим к созданию энкодера трансформера.\n<img src=\"https://raw.githubusercontent.com/jserdyuk/mmp_NLP/refs/heads/main/Task2/images/transformer.png\" />","metadata":{}},{"cell_type":"markdown","source":"Сформируем слой, создающий входные векторы токенов. Для этого нам нужны:\n1. Эмбеддинги токенов (`nn.Embedding`)\n2. Позиционные эмбеддинги (можно либо использовать `nn.Embedding`, либо явно создать матрицу эмбеддингов с помощью `nn.Parameter`)\n3. Сегментные эмбеддинги\n\nЭти три сущности складываются, затем идет layernorm и dropout.\n\n<img src=\"https://raw.githubusercontent.com/jserdyuk/mmp_NLP/refs/heads/main/Task2/images/bert_input.png\" />","metadata":{}},{"cell_type":"code","source":"class BertEmbeddings(nn.Module):\n    \n    def __init__(\n            self, \n            vocab_size, \n            hidden_size, \n            max_seqlen,\n            dropout_prob=0., \n            type_vocab_size=2,\n            eps=1e-3\n    ):\n        \"\"\"\n            vocab_size: размер словаря\n            hidden_size: размерность эмбеддингов\n            max_seqlen: количество позиционных эмбеддингов\n            dropout_prob: вероятность дропаута в конце слоя\n            type_vocab_size: количество сегментных эмбеддингов\n            eps: eps для layernorm\n        \"\"\"\n        super().__init__()\n        self._token_embeddings = nn.Embedding(vocab_size, hidden_size)\n        self._pos_embeddings = nn.Embedding(max_seqlen, hidden_size)\n        self._segment_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n        names = [\"_vocab_size\", \"_hidden_size\", \"_max_seqlen\", \n                 \"_dropout_prob\", \"_type_vocab_size\", \"_eps\"]\n        values = [vocab_size, hidden_size, max_seqlen, \n                 dropout_prob, type_vocab_size, eps]\n        for name, value in zip(names, values):\n            setattr(self, name, value)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.layer_norm = nn.LayerNorm(hidden_size, eps)\n        \n    def get_token_embeddings(self):\n        \"\"\"\n            returns: возвращает слой с матрицей эмбеддингов для токенов. Нужен для MLM головы\n        \"\"\"\n        return self._token_embeddings\n    \n    def forward(self, input_ids, token_type_ids=None):\n        \"\"\"\n            input_ids: тензор с индексами токенов\n            token_type_ids: сегментные индексы\n            \n            returns: эмбеддинги токенов\n        \"\"\"\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n        \n        # Сегментные индексы по умолчанию - нули, если не переданы\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # Суммирование эмбеддингов\n        token_embeddings = self._token_embeddings(input_ids)\n        position_embeddings = self._pos_embeddings(position_ids)\n        type_embeddings = self._segment_embeddings(token_type_ids)\n\n        embeddings = token_embeddings + position_embeddings + type_embeddings\n        embeddings = self.dropout(self.layer_norm(embeddings))\n        \n        return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:53.308992Z","iopub.execute_input":"2024-11-21T22:43:53.309836Z","iopub.status.idle":"2024-11-21T22:43:53.318642Z","shell.execute_reply.started":"2024-11-21T22:43:53.309801Z","shell.execute_reply":"2024-11-21T22:43:53.317647Z"}},"outputs":[],"execution_count":186},{"cell_type":"code","source":"tests.test_bert_embeddings(BertEmbeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:53.462403Z","iopub.execute_input":"2024-11-21T22:43:53.462744Z","iopub.status.idle":"2024-11-21T22:43:53.530663Z","shell.execute_reply.started":"2024-11-21T22:43:53.462716Z","shell.execute_reply":"2024-11-21T22:43:53.529774Z"}},"outputs":[{"name":"stdout","text":"Correct. Amount of parameters is: 7746560.\n","output_type":"stream"}],"execution_count":187},{"cell_type":"markdown","source":"Как выглядит одноголовый **SelfAttention**:\n\n<img src=\"https://raw.githubusercontent.com/jserdyuk/mmp_NLP/refs/heads/main/Task2/images/attention.png\" width=\"700\" height=\"700\"/>","metadata":{}},{"cell_type":"markdown","source":"Как выглядит многоголовый (multihead) **SelfAttention:**\n\n<img src=\"https://raw.githubusercontent.com/jserdyuk/mmp_NLP/refs/heads/main/Task2/images/multihead.png\" width=\"400\" height=\"400\"/>","metadata":{}},{"cell_type":"markdown","source":"Реализация **MultiHeadSelfAttention** — самая сложная часть энкодера. Дальше будет проще :)\n\n1. Принимаем на вход посл-ть векторов для каждого объекта в батче, т.е. тензор размера `batch_size x seqlen x dim`\n2. Получаем из исходных векторов векторы `query, key, value` с помощью линейного слоя. $W_q X, W_k X, W_v X$.\n    * **Важно:** не нужно делать три отдельных линейных слоя. Сделайте один линейный слой в три раза шире, затем после его применения разделите результат на три части с помощью метода `.chunk`. $W_{qkv} X$.\n3. Полученные query, key, value векторы делятся между \"головами\" аттеншна c помощью `.view`. Далее операции происходят для каждой головы отдельно.\n4. Нужно посчитать скалярные произведения всех запросов (queries) со всеми ключами (keys): $QK^T$.\n5. Заменить значения для паддинг токенов на очень маленькие (большие отрицательные), чтобы они не влияли на софтмакс:         \n    `attention_scores = attention_mask * attention_scores + (1 - attention_mask) * -100000`\n\n6. Применить Dropout аттеншн скоров, который  выкидывает из аттеншна токены целиком.\n7. Поделить \"аттеншны скоры\" на корень из размерности векторов и взять софтмакс по ключам. Т.е. $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}}, \\text{axis=1})$\n8. Посчитать контекстные векторы запросов $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}}, \\text{axis=1})V$.\n9. Сконкатенировать контекстные векторы всех голов и применить линейный слой той же размерности и dropout.\n10. Сложить со входом **MultiHeadSelfAttention** слоя, применить layernorm: $\\text{layernorm}(x + \\text{dropout}(f(x)))$.\n\n**Про аттеншн маску:**\n* В полном виде аттеншн маска имеет размерность `batch_size x seqlen x seqlen`\n* У нас же если токен не паддинг, то его видят остальные токены, поэтому по сути вся информация содержится в матрице размера `batch_size x seqlen` с предикатом является ли токен паддингом\n* Эту матрицу размера `batch_size x seqlen` можно привести к виду `batch_size x seqlen x seqlen` операцией вида `attention_mask[:, None] * torch.ones_like(attention_mask)[..., None]`\n\n**Вопросы:**\n1. Зачем нужно делить на корень из $d$ результаты скалярных произведений?\n2. Почему одно большое умножение на матрицу лучше, чем три маленьких?\n3. Что будет, если мы не будем заменять значения аттеншн скоров паддинг токенов на большие отрицательные значения?\n4. Какая вычислительная сложность (количество умножений) у операции **MultiheadSelfAttention**?\n\n**Ваши ответы напишите здесь:**\n1. Это нужно, чтобы дисперсия выходов не изменялась. Скалярное произведение - это сумма из $d$ элементов, а в силу независимости слагаемых, дисперсия скалярного произведения будет равна $d$ дисперсий каждого слагаемого. Чтобы дисперсия не изменилась, нужно поделить скалярное произведение на $\\sqrt{d}$\n2. Разделение операции на три маленькие приводит к дополнительным шагам в памяти, так как данные приходится загружать и выгружать трижды на GPU, а это очень затратная операция. При одном большом умножении эти шаги выполняются за один проход. При этом умножение через одну матрицу реализовано более эффективно\n3. Паддинг-токены не содержат значимой информации и не должны вносить вклад в вычисление внимания. Если этого не сделать, модель начнёт уделять внимание бессмысленным токенам, что ухудшит качество эмбеддингов.\n4. $O(\\text{seq\\_len} \\cdot \\text{hidden\\_size}^2 + \\text{seq\\_len}^2 \\cdot \\text{hidden\\_size})$","metadata":{}},{"cell_type":"code","source":"import math\n\n\nclass MultiHeadSelfAttention(nn.Module):\n    \n    def __init__(\n            self,\n            hidden_size,\n            num_attention_heads,\n            attention_probs_dropout_prob=0.,\n            dropout_prob=0.,\n            eps=1e-3\n    ):\n        \"\"\"\n            hidden_size: размерность эмбеддингов\n            num_attention_heads: количество голов аттеншна. Обычно выбирается как hidden_size / num_attention_heads = 64,\n                т.е. размерность векторов у одной головы 64\n            attention_probs_dropout_prob: вероятность дропаута для аттеншн скоров\n            dropout_prob: вероятность дропаута в конце слоя (перед суммой со входами)\n            eps: eps для layernorm\n        \"\"\"\n        super().__init__()\n        names = [\"_hidden_size\", \"_num_attention_heads\", \"_attention_probs_dropout_prob\",\n                 \"_dropout_prob\", \"_eps\"]\n        values = [hidden_size, num_attention_heads, attention_probs_dropout_prob,\n                 dropout_prob, eps]\n        for name, value in zip(names, values):\n            setattr(self, name, value)\n\n        self._projection_linear = nn.Linear(hidden_size, 3 * hidden_size)\n        self._output_linear = nn.Linear(hidden_size, hidden_size)\n        self._dropout_attention = nn.Dropout(self._attention_probs_dropout_prob)\n        self._dropout = nn.Dropout(self._dropout_prob)\n        self._layernorm = nn.LayerNorm(hidden_size, eps)\n        \n    @property\n    def size_per_head(self):\n        \"\"\"\n            returns: размерность векторов для одной головы\n        \"\"\"\n        return self._hidden_size // self._num_attention_heads\n    \n    def forward(self, embeddings, attention_mask):\n        \"\"\"\n            embeddings: входные эмбеддинги\n            attention_mask: тензор из 0, 1 размерности batch_size x seqlen x seqlen\n            \n            returns: контекстные векторы\n        \"\"\"\n        B, L, H = embeddings.shape\n        h = self._num_attention_heads\n        d_h = self.size_per_head\n        Q, K, V = torch.chunk(self._projection_linear(embeddings), 3, dim=-1)\n        Q, K, V = [tensor.view(B, L, h, d_h).permute(0, 2, 1, 3) for tensor in [Q, K, V]]  # B x h x L x d_h\n        attention_scores = (Q @ K.transpose(2, 3)) / (self.size_per_head ** 0.5)\n        attention_scores = self._dropout_attention(attention_mask.unsqueeze(1) * attention_scores + \n                                         (1 - attention_mask.unsqueeze(1)) * -100000)\n        output = (torch.softmax(attention_scores, dim=1) @ V).permute(0, 2, 1, 3).reshape(B, L, H)\n        output = self._dropout(self._output_linear(output))\n        output = self._layernorm(embeddings + output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:54.041788Z","iopub.execute_input":"2024-11-21T22:43:54.042133Z","iopub.status.idle":"2024-11-21T22:43:54.052701Z","shell.execute_reply.started":"2024-11-21T22:43:54.042103Z","shell.execute_reply":"2024-11-21T22:43:54.051686Z"}},"outputs":[],"execution_count":188},{"cell_type":"code","source":"tests.test_attention(MultiHeadSelfAttention)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:54.219880Z","iopub.execute_input":"2024-11-21T22:43:54.220188Z","iopub.status.idle":"2024-11-21T22:43:54.253285Z","shell.execute_reply.started":"2024-11-21T22:43:54.220161Z","shell.execute_reply":"2024-11-21T22:43:54.252299Z"}},"outputs":[{"name":"stdout","text":"Correct. Amount of parameters: 263680.\n","output_type":"stream"}],"execution_count":189},{"cell_type":"markdown","source":"Реализовать **полносвязный слой** гораздо проще - $\\text{layernorm}(\\text{dropout}(W_2 f(W_1 x + b_1) + b_2) + x)$:\n1. Линейный слой, расширяющий входные векторы до *intermediate_size*, который традиционно равен 4 * hidden_size, т.е. происходит расширение в четыре раза\n2. Функция активации (больше вы их нигде в модели не увидите)\n3. Линейный слой, сужающий векторы обратно до *hidden_size*\n4. Dropout, сложение со входом полносвязного слоя, layernorm\n\n**Вопросы:**\n1. Что дает \"расширение\" первым линейным слоем? Нельзя ли делать линейный слой поменьше?\n2. Какая вычислительная сложность (количество умножений) у операции?\n3. Используются ли где-то еще в трансформере функции активации (если не считать softmax функцией активации)?\n\n**Ваши ответы напишите здесь:**\n1. Слишком маленький размер первого линейного слоя в feed-forward приводит к недоиспользованию потенциала self-attention механизмов, а также потере большого количества потенциальных признаков для токенов. Чем меньше размер слоя, тем меньше сложных зависимостей будет учитывать наш энкодер\n2. $O(n \\cdot \\text{hidden\\_size} \\cdot \\text{intermediate\\_size})$, где $n$ - количество токенов\n3. Больше нигде не используются. Следовательно, этот слой является основной нелинейности для энкодера. Единственно, можно отметить MLM голову, но она является \"надстройкой\" над энкодером","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    \n    def __init__(\n            self, \n            hidden_size, \n            intermediate_size, \n            dropout_prob=0., \n            act_func='relu', \n            eps=1e-3\n    ):\n        \"\"\"\n            hidden_size: размерность эмбеддингов\n            intermediate_size: размерность промежуточного слоя. Обычно 4 * hidden_size\n            dropout_prob: вероятность дропаута перед суммой со входными представлениями\n            act_func: функция активации. Должны быть доступны gelu, relu\n            eps: eps для layernorm\n        \"\"\"\n        super().__init__()\n        names = [\"_hidden_size\", \"_intermediate_size\", \"_dropout_prob\", \"_act_func\", \"_eps\"]\n        values = [hidden_size, intermediate_size, dropout_prob, act_func, eps]\n        for name, value in zip(names, values):\n            setattr(self, name, value)\n        self._feedforward = nn.Sequential(\n            nn.Linear(hidden_size, intermediate_size),\n            nn.ReLU() if act_func == 'relu' else nn.GELU(),\n            nn.Linear(intermediate_size, hidden_size),\n            nn.Dropout(dropout_prob)\n        )\n        self._layernorm = nn.LayerNorm(hidden_size, eps)\n\n        \n    def forward(self, embeddings):\n        \"\"\"\n            embeddings: входные эмбеддинги размерности batch_size x seqlen x hidden_size\n            \n            returns: преобразованные эмбеддинги той же размерности\n        \"\"\"\n        return self._layernorm(self._feedforward(embeddings) + embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:54.581993Z","iopub.execute_input":"2024-11-21T22:43:54.582596Z","iopub.status.idle":"2024-11-21T22:43:54.589963Z","shell.execute_reply.started":"2024-11-21T22:43:54.582561Z","shell.execute_reply":"2024-11-21T22:43:54.588889Z"}},"outputs":[],"execution_count":190},{"cell_type":"code","source":"tests.test_feedforward(FeedForward)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:54.854798Z","iopub.execute_input":"2024-11-21T22:43:54.855138Z","iopub.status.idle":"2024-11-21T22:43:54.894913Z","shell.execute_reply.started":"2024-11-21T22:43:54.855108Z","shell.execute_reply":"2024-11-21T22:43:54.893839Z"}},"outputs":[{"name":"stdout","text":"Correct. Amount of parameters is: 526080.\n","output_type":"stream"}],"execution_count":191},{"cell_type":"markdown","source":"Объединим **MultiHeadSelfAttention** и **Feedforward** в один блок энкодера. Они применяются последовательно:","metadata":{}},{"cell_type":"code","source":"class BertLayer(nn.Module):\n    \n    def __init__(\n            self, \n            hidden_size, \n            intermediate_size, \n            num_attention_heads, \n            dropout_prob=0., \n            attention_probs_dropout_prob=0.,\n            act_func='relu',\n            eps=1e-3\n    ):\n        super().__init__()\n        self._multihead_attention = MultiHeadSelfAttention(\n            hidden_size=hidden_size,\n            num_attention_heads=num_attention_heads,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            dropout_prob=dropout_prob,\n            eps=eps\n        )\n        \n        self._feedforward = FeedForward(\n            hidden_size=hidden_size,\n            intermediate_size=intermediate_size,\n            act_func=act_func,\n            eps=eps,\n            dropout_prob=dropout_prob\n        )\n        \n    def forward(self, x, attention_mask=None):\n        x = self._multihead_attention(x, attention_mask)\n        x = self._feedforward(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:55.159892Z","iopub.execute_input":"2024-11-21T22:43:55.160260Z","iopub.status.idle":"2024-11-21T22:43:55.166670Z","shell.execute_reply.started":"2024-11-21T22:43:55.160229Z","shell.execute_reply":"2024-11-21T22:43:55.165662Z"}},"outputs":[],"execution_count":192},{"cell_type":"code","source":"tests.test_bert_layer(BertLayer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:55.356418Z","iopub.execute_input":"2024-11-21T22:43:55.356777Z","iopub.status.idle":"2024-11-21T22:43:55.397541Z","shell.execute_reply.started":"2024-11-21T22:43:55.356746Z","shell.execute_reply":"2024-11-21T22:43:55.396512Z"}},"outputs":[{"name":"stdout","text":"Correct. Amount of parameters is: 789760.\n","output_type":"stream"}],"execution_count":193},{"cell_type":"markdown","source":"Объедините **BertEmbeddings** и произвольное заданное число **BertLayer** слоёв в один слой:","metadata":{}},{"cell_type":"code","source":"class Bert(nn.Module):\n    \n    def __init__(\n            self, \n            vocab_size,\n            max_seqlen,\n            hidden_size,\n            num_hidden_layers,\n            intermediate_size, \n            num_attention_heads, \n            input_dropout_prob=0.,\n            dropout_prob=0., \n            attention_probs_dropout_prob=0.,\n            act_func='relu',\n            eps=1e-3\n    ):\n        super().__init__()\n        self._embeddings = BertEmbeddings(vocab_size, hidden_size, max_seqlen, \n                                          dropout_prob, eps=eps)\n        self._bert = nn.ModuleList([\n            BertLayer(hidden_size, intermediate_size, \n                      num_attention_heads, dropout_prob, \n                      attention_probs_dropout_prob, \n                      act_func, eps) \n            for _ in range(num_hidden_layers)\n        ])\n        \n    def get_token_embeddings(self):\n        \"\"\"\n            returns: эмбеддинги токенов (матрицу эмбеддингов)\n        \"\"\"\n        return self._embeddings.get_token_embeddings()\n    \n    @staticmethod\n    def expand_mask(attention_mask):\n        \"\"\"\n            attention_mask: маска паддинга размерности batch_size x seqlen\n            \n            returns: маска паддинга размерности batch_size x seqlen x seqlen\n        \"\"\"\n        return attention_mask[:, None] * torch.ones_like(attention_mask)[..., None]\n    \n    def forward(self, x, attention_mask=None, token_type_ids=None):\n        embeddings = self._embeddings(x, token_type_ids)\n        attention_mask = torch.ones(x.shape[0], x.shape[1]) if attention_mask is None else attention_mask\n        for bert_layer in self._bert:\n            x = bert_layer(embeddings, Bert.expand_mask(attention_mask))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:55.693147Z","iopub.execute_input":"2024-11-21T22:43:55.693961Z","iopub.status.idle":"2024-11-21T22:43:55.702541Z","shell.execute_reply.started":"2024-11-21T22:43:55.693922Z","shell.execute_reply":"2024-11-21T22:43:55.701397Z"}},"outputs":[],"execution_count":194},{"cell_type":"code","source":"tests.test_bert(Bert)","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:55.875942Z","iopub.execute_input":"2024-11-21T22:43:55.876281Z","iopub.status.idle":"2024-11-21T22:43:55.967746Z","shell.execute_reply.started":"2024-11-21T22:43:55.876252Z","shell.execute_reply":"2024-11-21T22:43:55.966835Z"}},"outputs":[{"name":"stdout","text":"Correct. Amount of parameters is: 10905600.\n","output_type":"stream"}],"execution_count":195},{"cell_type":"markdown","source":"Для предобучения (и для целевых задач) необходимо задать \"головы\" модели:\n\nГолова для **MLM** задачи выглядит как $W_2 \\text{layernorm} (f(W_1 x + b_1)) + b_2$:\n1. Линейный слой $d \\times d$\n2. Функция активации\n3. LayerNorm\n4. Линейный слой $d \\times |V|$, где $|V|$ --- размер словаря. **Важно:** в качестве матрицы, на которую происходит умножение при аффинном преобразовании, берется матрица эмбеддингов токенов.\n5. Функционал ошибки тоже будем считать сразу в голове, для него используется **nn.CrossEntropyLoss**: \n    * `self._criterion(preds.view(-1, self._vocab_size), labels.view(-1))`\n\nЧтобы использовать матрицу входных эмбеддингов вместо последнего линейного слоя в голове, можно использовать присваивание вида`self._decoder.weight = input_embeddings.weight`.","metadata":{}},{"cell_type":"code","source":"class MlmHead(nn.Module):\n\n    def __init__(\n            self, \n            hidden_size, \n            vocab_size, \n            hidden_act, \n            eps=1e-3, \n            ignore_index=-100, \n            input_embeddings=None\n    ):\n        \"\"\"\n            hidden_size: размерность эмбеддингов\n            vocab_size: размер словаря\n            hidden_act: функция активации\n            eps: eps для layernorm\n            ignore_index: индекс таргета, который необходимо игнорировать при подсчете лосса\n            input_embeddings: слой с эмбеддингами токенов, для использования матрицы эмбеддингов вместо линейного слоя\n        \"\"\"\n        super().__init__()\n        self._mlm = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU() if hidden_act == \"relu\" else nn.GELU(),\n            nn.LayerNorm(hidden_size, eps=eps),\n            nn.Linear(hidden_size, vocab_size)\n        )\n        if input_embeddings is not None:\n            self._mlm[-1].weight = input_embeddings.weight\n        self._criterion = nn.CrossEntropyLoss()\n        self._ignore_index = ignore_index\n        self._vocab_size = vocab_size\n\n    def forward(self, hidden_states, labels):\n        \"\"\"\n            hidden_states: эмбеддинги токенов\n            labels: истинные метки, т.е. изначальные индексы токенов\n            \n            returns: посчитанный лосс\n        \"\"\"\n        preds = self._mlm(hidden_states)\n        mask = labels.view(-1) != self._ignore_index\n        return self._criterion(preds.view(-1, self._vocab_size)[mask], labels.view(-1)[mask])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:56.220695Z","iopub.execute_input":"2024-11-21T22:43:56.221017Z","iopub.status.idle":"2024-11-21T22:43:56.229094Z","shell.execute_reply.started":"2024-11-21T22:43:56.220990Z","shell.execute_reply":"2024-11-21T22:43:56.228088Z"}},"outputs":[],"execution_count":196},{"cell_type":"code","source":"tests.test_mlm_head(MlmHead, BertEmbeddings)","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:56.419794Z","iopub.execute_input":"2024-11-21T22:43:56.420120Z","iopub.status.idle":"2024-11-21T22:43:56.569431Z","shell.execute_reply.started":"2024-11-21T22:43:56.420093Z","shell.execute_reply":"2024-11-21T22:43:56.568516Z"}},"outputs":[{"name":"stdout","text":"Correct. Amount of parameters is: 7776304.\n","output_type":"stream"}],"execution_count":197},{"cell_type":"markdown","source":"Голова для **SOP**-задачи выглядит аналогично и в оригинальной статье называется \"pooler-слоем\":\n1. Берем скрытое представление CLS токена\n2. Линейный слой $d \\times d$\n3. Функция активации, причем в качестве функции активации используется гиперболический тангенс **nn.Tanh**\n4. Dropout\n5. Линейный слой\n6. Функционал ошибки (бинарная кросс-энтропия с логитами, **nn.BCEWithLogitsLoss**)\n\nЭту голову (кроме последнего линейного слоя) мы будем использовать также и для целевой задачи (классификации чеков).","metadata":{}},{"cell_type":"code","source":"class ClassifierHead(nn.Module):\n    CLS_POSITION = 0\n    CRITERION = nn.BCEWithLogitsLoss()\n    \n    def __init__(self, \n                 hidden_size, \n                 num_classes=1, \n                 hidden_dropout_prob=0.):\n        \"\"\"\n            hidden_size: размерность эмбеддингов\n            hidden_dropout_prob: вероятность дропаута\n        \"\"\"\n        super().__init__()\n        self._sop = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.Tanh(),\n            nn.Dropout(hidden_dropout_prob),\n            nn.Linear(hidden_size, num_classes)\n        )\n\n    def forward(self, hidden_states, permuted=None):\n        \"\"\"\n            hidden_states: эмбеддинги\n            permuted: таргеты (были ли свапы сегментов). Если их нет, то необходимо выдать предсказания\n        \"\"\"\n        cls_tokens = self._sop(hidden_states)[:, self.CLS_POSITION].squeeze()\n        return cls_tokens if permuted is None else self.CRITERION(cls_tokens, permuted.squeeze().float())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:56.785202Z","iopub.execute_input":"2024-11-21T22:43:56.785566Z","iopub.status.idle":"2024-11-21T22:43:56.793696Z","shell.execute_reply.started":"2024-11-21T22:43:56.785534Z","shell.execute_reply":"2024-11-21T22:43:56.792640Z"}},"outputs":[],"execution_count":198},{"cell_type":"code","source":"tests.test_classifier_head(ClassifierHead)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:56.983947Z","iopub.execute_input":"2024-11-21T22:43:56.984321Z","iopub.status.idle":"2024-11-21T22:43:56.991409Z","shell.execute_reply.started":"2024-11-21T22:43:56.984289Z","shell.execute_reply":"2024-11-21T22:43:56.990380Z"}},"outputs":[{"name":"stdout","text":"Correct. Amount of parameters is: 66049.\n","output_type":"stream"}],"execution_count":199},{"cell_type":"markdown","source":"Объединим **Bert**, **MlmHead** и **ClassifierHead** в единую модель:","metadata":{}},{"cell_type":"code","source":"class BertModel(nn.Module):\n\n    def __init__(\n            self, \n            hidden_size, \n            vocab_size,\n            max_seqlen,\n            num_hidden_layers,\n            intermediate_size,\n            num_attention_heads,\n            act_func='relu',\n            input_dropout_prob=0.,\n            hidden_dropout_prob=0., \n            attention_probs_dropout_prob=0.,\n            eps=1e-3, \n            ignore_index=-100\n    ):\n        super().__init__()\n        self._backbone = Bert(\n            vocab_size=vocab_size,\n            max_seqlen=max_seqlen,\n            hidden_size=hidden_size,\n            num_hidden_layers=num_hidden_layers,\n            intermediate_size=intermediate_size, \n            num_attention_heads=num_attention_heads, \n            input_dropout_prob=input_dropout_prob,\n            dropout_prob=hidden_dropout_prob, \n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            act_func=act_func,\n            eps=eps\n        )\n        self._mlm_head = MlmHead(\n            hidden_size, \n            vocab_size, \n            act_func, \n            eps, \n            ignore_index, \n            input_embeddings=self._backbone.get_token_embeddings()\n        )\n        self._classifier_head = ClassifierHead(\n            hidden_size, \n            hidden_dropout_prob=hidden_dropout_prob, \n            num_classes=1\n        )\n\n    def forward(self, x, attention_mask, labels, permuted, token_type_ids=None):\n        hidden_states = self._backbone(x, attention_mask, token_type_ids)\n        mlm_loss = self._mlm_head(hidden_states, labels)\n        sop_loss = self._classifier_head(hidden_states, permuted)\n        # в оригинальном BERT лоссы MLP и NSP используются с равными весами\n        return 0.5 * mlm_loss + 0.5 * sop_loss, {'MLM': mlm_loss, 'SOP': sop_loss}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:57.336547Z","iopub.execute_input":"2024-11-21T22:43:57.336916Z","iopub.status.idle":"2024-11-21T22:43:57.344570Z","shell.execute_reply.started":"2024-11-21T22:43:57.336884Z","shell.execute_reply":"2024-11-21T22:43:57.343643Z"}},"outputs":[],"execution_count":200},{"cell_type":"markdown","source":"Значения гиперпараметров:\n* для успешного выполнения задания достаточно архитектуры bert-mini: `hidden_size=256`, `num_hidden_layers=4`, в качестве функции активации можно использовать `gelu`\n* стандартные практики: `intermediate_size = 4 * hidden_size`, `num_attention_heads = hidden_size // 64`\n* в оригинальной статье везде dropout равен 0.1, но для bert-mini модели можно попробовать значения поменьше. Вопрос - почему?","metadata":{}},{"cell_type":"code","source":"model = BertModel(\n    hidden_size=256, \n    vocab_size=30000,\n    max_seqlen=MAX_SEQLEN,\n    num_hidden_layers=4,\n    intermediate_size=(4 * 256),\n    num_attention_heads=(256 // 64),\n    act_func='gelu',\n    input_dropout_prob=0.05,\n    hidden_dropout_prob=0.05, \n    attention_probs_dropout_prob=0.05,\n    eps=1e-3, \n    ignore_index=-100\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:57.714189Z","iopub.execute_input":"2024-11-21T22:43:57.714895Z","iopub.status.idle":"2024-11-21T22:43:57.897086Z","shell.execute_reply.started":"2024-11-21T22:43:57.714861Z","shell.execute_reply":"2024-11-21T22:43:57.896296Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"def count_parameters(model):\n    paramcount_dict = dict()\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            paramcount_dict[name] = param.numel()\n    paramcount_dict = dict(sorted(paramcount_dict.items(), key=lambda item: item[1]))\n    for name, param in paramcount_dict.items():\n        print(f\"Layer: {name} | Parameters: {param}\")\n\ncount_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:58.077283Z","iopub.execute_input":"2024-11-21T22:43:58.077699Z","iopub.status.idle":"2024-11-21T22:43:58.085007Z","shell.execute_reply.started":"2024-11-21T22:43:58.077668Z","shell.execute_reply":"2024-11-21T22:43:58.084002Z"}},"outputs":[{"name":"stdout","text":"Layer: _classifier_head._sop.3.bias | Parameters: 1\nLayer: _backbone._embeddings.layer_norm.weight | Parameters: 256\nLayer: _backbone._embeddings.layer_norm.bias | Parameters: 256\nLayer: _backbone._bert.0._multihead_attention._output_linear.bias | Parameters: 256\nLayer: _backbone._bert.0._multihead_attention._layernorm.weight | Parameters: 256\nLayer: _backbone._bert.0._multihead_attention._layernorm.bias | Parameters: 256\nLayer: _backbone._bert.0._feedforward._feedforward.2.bias | Parameters: 256\nLayer: _backbone._bert.0._feedforward._layernorm.weight | Parameters: 256\nLayer: _backbone._bert.0._feedforward._layernorm.bias | Parameters: 256\nLayer: _backbone._bert.1._multihead_attention._output_linear.bias | Parameters: 256\nLayer: _backbone._bert.1._multihead_attention._layernorm.weight | Parameters: 256\nLayer: _backbone._bert.1._multihead_attention._layernorm.bias | Parameters: 256\nLayer: _backbone._bert.1._feedforward._feedforward.2.bias | Parameters: 256\nLayer: _backbone._bert.1._feedforward._layernorm.weight | Parameters: 256\nLayer: _backbone._bert.1._feedforward._layernorm.bias | Parameters: 256\nLayer: _backbone._bert.2._multihead_attention._output_linear.bias | Parameters: 256\nLayer: _backbone._bert.2._multihead_attention._layernorm.weight | Parameters: 256\nLayer: _backbone._bert.2._multihead_attention._layernorm.bias | Parameters: 256\nLayer: _backbone._bert.2._feedforward._feedforward.2.bias | Parameters: 256\nLayer: _backbone._bert.2._feedforward._layernorm.weight | Parameters: 256\nLayer: _backbone._bert.2._feedforward._layernorm.bias | Parameters: 256\nLayer: _backbone._bert.3._multihead_attention._output_linear.bias | Parameters: 256\nLayer: _backbone._bert.3._multihead_attention._layernorm.weight | Parameters: 256\nLayer: _backbone._bert.3._multihead_attention._layernorm.bias | Parameters: 256\nLayer: _backbone._bert.3._feedforward._feedforward.2.bias | Parameters: 256\nLayer: _backbone._bert.3._feedforward._layernorm.weight | Parameters: 256\nLayer: _backbone._bert.3._feedforward._layernorm.bias | Parameters: 256\nLayer: _mlm_head._mlm.0.bias | Parameters: 256\nLayer: _mlm_head._mlm.2.weight | Parameters: 256\nLayer: _mlm_head._mlm.2.bias | Parameters: 256\nLayer: _classifier_head._sop.0.bias | Parameters: 256\nLayer: _classifier_head._sop.3.weight | Parameters: 256\nLayer: _backbone._embeddings._segment_embeddings.weight | Parameters: 512\nLayer: _backbone._bert.0._multihead_attention._projection_linear.bias | Parameters: 768\nLayer: _backbone._bert.1._multihead_attention._projection_linear.bias | Parameters: 768\nLayer: _backbone._bert.2._multihead_attention._projection_linear.bias | Parameters: 768\nLayer: _backbone._bert.3._multihead_attention._projection_linear.bias | Parameters: 768\nLayer: _backbone._bert.0._feedforward._feedforward.0.bias | Parameters: 1024\nLayer: _backbone._bert.1._feedforward._feedforward.0.bias | Parameters: 1024\nLayer: _backbone._bert.2._feedforward._feedforward.0.bias | Parameters: 1024\nLayer: _backbone._bert.3._feedforward._feedforward.0.bias | Parameters: 1024\nLayer: _backbone._embeddings._pos_embeddings.weight | Parameters: 6912\nLayer: _mlm_head._mlm.3.bias | Parameters: 30000\nLayer: _backbone._bert.0._multihead_attention._output_linear.weight | Parameters: 65536\nLayer: _backbone._bert.1._multihead_attention._output_linear.weight | Parameters: 65536\nLayer: _backbone._bert.2._multihead_attention._output_linear.weight | Parameters: 65536\nLayer: _backbone._bert.3._multihead_attention._output_linear.weight | Parameters: 65536\nLayer: _mlm_head._mlm.0.weight | Parameters: 65536\nLayer: _classifier_head._sop.0.weight | Parameters: 65536\nLayer: _backbone._bert.0._multihead_attention._projection_linear.weight | Parameters: 196608\nLayer: _backbone._bert.1._multihead_attention._projection_linear.weight | Parameters: 196608\nLayer: _backbone._bert.2._multihead_attention._projection_linear.weight | Parameters: 196608\nLayer: _backbone._bert.3._multihead_attention._projection_linear.weight | Parameters: 196608\nLayer: _backbone._bert.0._feedforward._feedforward.0.weight | Parameters: 262144\nLayer: _backbone._bert.0._feedforward._feedforward.2.weight | Parameters: 262144\nLayer: _backbone._bert.1._feedforward._feedforward.0.weight | Parameters: 262144\nLayer: _backbone._bert.1._feedforward._feedforward.2.weight | Parameters: 262144\nLayer: _backbone._bert.2._feedforward._feedforward.0.weight | Parameters: 262144\nLayer: _backbone._bert.2._feedforward._feedforward.2.weight | Parameters: 262144\nLayer: _backbone._bert.3._feedforward._feedforward.0.weight | Parameters: 262144\nLayer: _backbone._bert.3._feedforward._feedforward.2.weight | Parameters: 262144\nLayer: _backbone._embeddings._token_embeddings.weight | Parameters: 7680000\n","output_type":"stream"}],"execution_count":202},{"cell_type":"markdown","source":"**Вопросы:**\n1. Какая часть модели содержит наибольшее количество параметров? Эмбеддинги, аттеншн, полносвязные слои, голова?\n2. Зачем объединять параметры в голове и параметры матрицы эмбеддингов?\n\n**Ваши ответы напишите здесь:**\n1. Легко видеть, что больше всего параметров имеет слой Эмбеддингов - 7,680,000 параметров. Дальше идёт FeedForward слой, а потом MLM по количеству параметров.\n2. Матрица эмбеддингов используется для кодирования токенов и может быть повторно использована для декодирования токенов. Если выходная голова обучается независимо, это приводит к дублированию параметров. Также важно отметить, что BERT реализует метод weight tying. Этот метод снижает риск рассогласования между входными и выходными пространствами токенов.","metadata":{}},{"cell_type":"markdown","source":"## Часть 3. Оптимизация (1 балл)","metadata":{}},{"cell_type":"markdown","source":"Для оптимизации будем использовать **AdamW**, отличия которого от ванильного **Adam** можно почитать, например, [вот здесь](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)\n\nПараметры модели, передаваемые в оптимизатор, следует поделить на две группы с помощью `model.named_parameters()`:\n1. Все `bias` и `layernorm` слои, присутствующие в модели (их можно выцепить по названию). Для них $l_2$ регуляризацию стоит выключить, т.е. поставить `weight_decay=0`\n2. Оставшиеся слои, для которых регуляризация не нужна.\n\n\n**Вопрос:** почему $l_2$ регуляризацию не используют для bias'ов? Для layernorm?\n**Ответ:** регуляризация bias'ов может ухудшить способность модели компенсировать смещения в данных. Они мало влияют на переобучение. Параметры в layernorm нормализуют значения перед переходом на следующие слои. Сам по себе слой позволяет плавно обучать модель. Регуляризация параметров в layernorm может испортить нормализацию слоёв, что ухудшит обучение. ","metadata":{}},{"cell_type":"code","source":"def get_optimizer(model, weight_decay=0.01):\n    \"\"\"\n        model: инициализированная модель\n        weight_decay: коэффициент l2 регуляризации\n        \n        returns: оптимизатор\n    \"\"\"\n    decayed_parameters, not_decayed_parameters = [], []\n    for name, param in model.named_parameters():\n        if \"layernorm\" in name or name.endswith(\"bias\"):\n            not_decayed_parameters.append(param)\n        else:\n            decayed_parameters.append(param)\n            \n    grouped_parameters = [\n        {'params': decayed_parameters, 'weight_decay': weight_decay},\n        {'params': not_decayed_parameters, 'weight_decay': 0.}\n    ]\n\n    return torch.optim.AdamW(grouped_parameters)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:59.347610Z","iopub.execute_input":"2024-11-21T22:43:59.347987Z","iopub.status.idle":"2024-11-21T22:43:59.353896Z","shell.execute_reply.started":"2024-11-21T22:43:59.347954Z","shell.execute_reply":"2024-11-21T22:43:59.352851Z"}},"outputs":[],"execution_count":203},{"cell_type":"code","source":"optimizer = get_optimizer(model, weight_decay=1e-2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:59.506153Z","iopub.execute_input":"2024-11-21T22:43:59.507061Z","iopub.status.idle":"2024-11-21T22:43:59.511446Z","shell.execute_reply.started":"2024-11-21T22:43:59.507024Z","shell.execute_reply":"2024-11-21T22:43:59.510507Z"}},"outputs":[],"execution_count":204},{"cell_type":"code","source":"tests.test_optimizer(get_optimizer, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:43:59.836026Z","iopub.execute_input":"2024-11-21T22:43:59.836713Z","iopub.status.idle":"2024-11-21T22:43:59.841957Z","shell.execute_reply.started":"2024-11-21T22:43:59.836679Z","shell.execute_reply":"2024-11-21T22:43:59.841025Z"}},"outputs":[{"name":"stdout","text":"Correct.\n","output_type":"stream"}],"execution_count":205},{"cell_type":"markdown","source":"Как выглядит типичное расписание lr для трансформеров:\n<img src=\"https://raw.githubusercontent.com/jserdyuk/mmp_NLP/refs/heads/main/Task2/images/lr.png\" width=\"500\" height=\"500\"/>","metadata":{}},{"cell_type":"markdown","source":"Почему мы сразу не стартуем с большого значения lr? Для больших архитектур трансформера модель разойдется, произойдет взрыв градиентов. Постепенно же увеличить lr до большого значения — можно. Процедуру линейного увеличения lr до какого-то пикового значения называют `linear warmup`.\n\nРеализуйте такое \"треугольное\" расписание для learning rate в предложенном шаблоне.\n\n**Вопрос:** а зачем нужно убывание learning rate?\n\nЧем ближе мы подходим к локальным экстремумам определённого функционала, тем меньшие шаги нужно делать, чтобы эти экстремумы не перескочить. Если оставлять learning rate постоянным, то модель может легко перескочить через необходимый локальный минимум, ибо градиентный шаг будет слишком большим. Если же уменьшать градиентный шаг, то в силу непрерывной аппроксимации функционалов эмпирического риска малые изменения параметров будут влечь за собой малые изменения функционала, что поможет аккуратнее подходить к локальным экстремумам.","metadata":{}},{"cell_type":"code","source":"class Scheduler:\n    \n    def __init__(\n            self, \n            optimizer, \n            init_lr, \n            peak_lr, \n            final_lr, \n            num_warmup_steps, \n            num_training_steps\n    ):\n        \"\"\"\n            optimizer: оптимизатор\n            init_lr: начальное значение learning rate\n            peak_lr: пиковое значение learning rate\n            final_lr: финальное значение lr\n            num_warmup_steps: количество шагов разогрева (сколько шагов идем от начального до пикового значения)\n            num_training_steps: количество шагов обучения (количество батчей x количество эпох)\n            \n        \"\"\"\n        self._step = 0\n        names = [\"_optimizer\", \"_init_lr\", \"_peak_lr\", \"_final_lr\", \"_num_warmup_steps\", \"_num_training_steps\"]\n        values = [optimizer, init_lr, peak_lr, final_lr, num_warmup_steps, num_training_steps]\n        for name, value in zip(names, values):\n            setattr(self, name, value)\n        for param_group in self._optimizer.param_groups:\n            param_group['lr'] = self._init_lr\n        \n    def step(self):\n        \"\"\"\n            Меняет learning rate для оптимизатора\n            \n            Поменять learning rate для группы параметров в оптимизаторе можно присваиванием вида param_group['lr'] = lr\n        \"\"\"\n        if self._step < self._num_warmup_steps:\n            add_lr = (self._peak_lr - self._init_lr) / self._num_warmup_steps\n        elif self._step < self._num_training_steps:\n            add_lr = (self._final_lr - self._peak_lr) / (self._num_training_steps - self._num_warmup_steps)\n        else:\n            add_lr = 0\n        \n        for param_group in self._optimizer.param_groups:\n            param_group['lr'] += add_lr\n            \n        self._step += 1\n        \n    def get_last_lr(self):\n        \"\"\"\n            returns: текущий learning rate оптимизатора. Нужно для логгирования\n        \"\"\"\n        return [param_group['lr'] for param_group in self._optimizer.param_groups]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:44:00.329239Z","iopub.execute_input":"2024-11-21T22:44:00.330023Z","iopub.status.idle":"2024-11-21T22:44:00.338366Z","shell.execute_reply.started":"2024-11-21T22:44:00.329986Z","shell.execute_reply":"2024-11-21T22:44:00.337321Z"}},"outputs":[],"execution_count":206},{"cell_type":"code","source":"tests.test_scheduler(Scheduler, get_optimizer, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:44:00.538148Z","iopub.execute_input":"2024-11-21T22:44:00.539209Z","iopub.status.idle":"2024-11-21T22:44:00.545282Z","shell.execute_reply.started":"2024-11-21T22:44:00.539160Z","shell.execute_reply":"2024-11-21T22:44:00.544265Z"}},"outputs":[{"name":"stdout","text":"Correct.\n","output_type":"stream"}],"execution_count":207},{"cell_type":"code","source":"print(len(dl))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:44:00.874556Z","iopub.execute_input":"2024-11-21T22:44:00.874934Z","iopub.status.idle":"2024-11-21T22:44:00.880075Z","shell.execute_reply.started":"2024-11-21T22:44:00.874902Z","shell.execute_reply":"2024-11-21T22:44:00.879041Z"}},"outputs":[{"name":"stdout","text":"10670\n","output_type":"stream"}],"execution_count":208},{"cell_type":"code","source":"N_EPOCHS_PRETRAINED = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:44:01.064958Z","iopub.execute_input":"2024-11-21T22:44:01.065572Z","iopub.status.idle":"2024-11-21T22:44:01.069600Z","shell.execute_reply.started":"2024-11-21T22:44:01.065539Z","shell.execute_reply":"2024-11-21T22:44:01.068639Z"}},"outputs":[],"execution_count":209},{"cell_type":"code","source":"scheduler = Scheduler(\n    optimizer=optimizer, \n    init_lr=3e-6, \n    peak_lr=2e-4, \n    final_lr=0, \n    num_warmup_steps=int(0.1 * len(dl) * N_EPOCHS_PRETRAINED), \n    num_training_steps=(len(dl) * N_EPOCHS_PRETRAINED)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:44:01.245997Z","iopub.execute_input":"2024-11-21T22:44:01.246365Z","iopub.status.idle":"2024-11-21T22:44:01.251565Z","shell.execute_reply.started":"2024-11-21T22:44:01.246309Z","shell.execute_reply":"2024-11-21T22:44:01.250600Z"}},"outputs":[],"execution_count":210},{"cell_type":"markdown","source":"От запуска обучения нас отделяет только создание `Trainer`. От объектов класса `Trainer` требуется, чтобы:\n* логгировался лосс на каждом батче (`torch.utils.tensorboard.SummaryWriter`, `writer.add_scalar`)\n* клипались и логгировались нормы градиентов при каждом шаге спуска (`orch.nn.utils.clip_grad_norm_` возвращает нормы градиентов)\n* логгировались значения learning rate\n* была поддержана аккумуляция градиентов, нужная для эмуляции больших батчей\n\nПри предобучении не нужно использовать какую-либо форму валидации, достаточно смотреть на батч лосс.\n\nПредлагается также для ускорения обучения использовать mixed precision из библиотеки `apex`:\n* перед обучением необходимо вызвать строчку вида `model, optimizer = amp.initialize(model, optimizer, opt_level='O1')`\n* при обучении `.backward()` надо делать в контекстном менеджере:     \n   `with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward()`\n        \nЧто такое аккумуляция градиентов:\n* При использовании Adam в видеопамяти необходимо хранить градиенты и квадраты частных производных\n* При подсчете градиента по очередному батчу необязательно сразу делать шаг спуска, можно запомнить градиент, а затем посчитать градиент по другому батчу c теми же параметрами модели\n* Теперь эти два градиента можно сложить и получить градиент, который был посчитан как будто по одному большому батчу (составленному из этих двух). Сэмулировали большой батч. В данном случае количество шагов аккумуляции равно двум.\n* В данном случае количество шагов аккумуляции равно двум.\n\nЗачем нужны большие батчи? Обучение быстрее, оценки градиента точнее, позволяет увеличивать learning rate. Например, при предобучении авторы RoBERTA значительно увеличили размер батча по сравнению с ванильным BERT и получили прирост к качеству решения целевых задач.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nfrom collections import defaultdict\nfrom tqdm.notebook import tqdm\nfrom torch.nn.utils import clip_grad_norm_\nfrom time import time\n\n# from apex import amp\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass Trainer:\n\n    def __init__(\n            self, \n            model, \n            optimizer, \n            scheduler,\n            pad_token_id,\n            device,\n            num_accum_steps=1,\n            logdir=None,\n            max_grad_norm=None\n    ):\n        \"\"\"\n            model: объект класса BertModel\n            optimizer: оптимизатор\n            scheduler: расписание learning rate. Нужно вызывать scheduler.step() ПОСЛЕ optimizer.step()\n            pad_token_id: индекс паддинга. Нужен для создания attention mask\n            device: девайс (cpu или cuda), на котором надо производить вычисления\n            num_accum_steps: количество шагов аккумуляции\n            logdir: директория для записи логов\n            max_grad_norm: максимум нормы градиентов, для клиппинга\n        \"\"\"\n        names = [\"_model\", \"_optimizer\", \"_scheduler\", \n                 \"_pad_token_id\", \"_device\", \"_num_accum_steps\", \n                 \"_logdir\", \"_max_grad_norm\", \"_writer\", \n                 \"_n_epoch\", \"_scaler\", \"_verbose_step\"]\n        values = [model.to(device), optimizer, scheduler, \n                  pad_token_id, device, num_accum_steps, \n                  logdir, max_grad_norm, SummaryWriter(logdir) if logdir else None, \n                  0, torch.amp.GradScaler(\"cuda\"), 100]\n        \n        for name, value in zip(names, values):\n            setattr(self, name, value)\n\n    def train(self, dataloader, n_epochs):\n        print(f\"STEPS: {n_epochs * len(dataloader)}\", end='\\n')\n        for epoch in range(n_epochs):\n            start_time = time()\n            self._train_step(dataloader)\n            end_time = time() - start_time\n            print(f\"\\nTRAIN TIME: {end_time:.4f}\\n\")\n            self._n_epoch += 1\n\n    def _batch_to_device(self, batch):\n        batch = list(batch)\n        for i in range(len(batch)):\n            batch[i] = batch[i].to(self._device)\n        return batch\n\n    def _train_step(self, dataloader):\n        \"\"\"\n            dataloader: объект класса DataLoader для обучения\n        \"\"\"\n        total_loss = 0.0\n        self._optimizer.zero_grad()\n        for step, batch in tqdm(enumerate(dataloader)):\n            input_ids, token_type_ids, labels, permuted = self._batch_to_device(batch)\n            attention_mask = (input_ids != self._pad_token_id).int()\n\n            # Используем mixed precision\n            with torch.amp.autocast(\"cuda\"):\n                loss, _ = self._model(input_ids, attention_mask, labels, \n                                      permuted.int(), token_type_ids=token_type_ids)\n                loss = loss / self._num_accum_steps\n\n            # Backward с помощью scaler\n            self._scaler.scale(loss).backward()\n\n            if (step + 1) % self._num_accum_steps == 0 or (step + 1) == len(dataloader):\n                if self._max_grad_norm is not None:\n                    # Клиппинг градиентов\n                    self._scaler.unscale_(self._optimizer)  # Убираем скалирование перед клиппингом\n                    grad_norm = clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n                else:\n                    grad_norm = None\n\n                # Шаг оптимизатора и обновление scaler\n                self._scaler.step(self._optimizer)\n                self._scaler.update()\n                self._scheduler.step()\n                self._optimizer.zero_grad()\n\n                # Логгирование\n                if self._writer:\n                    current_step = self._n_epoch * len(dataloader) + step\n                    self._writer.add_scalar(\"Loss/batch\", loss.item() * self._num_accum_steps, current_step)\n                    self._writer.add_scalar(\"LearningRate\", self._scheduler.get_last_lr()[0], current_step)\n                    if grad_norm is not None:\n                        self._writer.add_scalar(\"GradientNorm\", grad_norm, current_step)\n\n                    if (step + 1) % self._verbose_step == 0:\n                        print(f\"Current_step {current_step} | BatchLoss {(loss.item() * self._num_accum_steps):.3f} | \", end='')\n                        print(f\"Learning rate {self._scheduler.get_last_lr()[0]:.7f} | GradientNorm {grad_norm:.3f}\")\n\n            total_loss += loss.item() * self._num_accum_steps\n\n        return total_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:44:01.732545Z","iopub.execute_input":"2024-11-21T22:44:01.732899Z","iopub.status.idle":"2024-11-21T22:44:01.747673Z","shell.execute_reply.started":"2024-11-21T22:44:01.732868Z","shell.execute_reply":"2024-11-21T22:44:01.746578Z"}},"outputs":[],"execution_count":211},{"cell_type":"markdown","source":"Обучите и сохраните предобученную модель с помощью `torch.save`. \n\n**Важно:** тензорборд логи успешного обучения необходимо сложить в архив и приложить вместе с решенным заданием.\n\nПро гиперпараметры:\n* `weight_decay` - $0.1, 0.01, 0.001$ и т.д.\n* расписание lr - bert-mini не очень чувствителен к линейному вормапу, поэтому существенное влияние оказывают только пиковое и финальное значение lr. Пиковое значение стоит поискать где-то в масштабе 1e-3 - 1e-4, финальный lr можно сделать очень маленьким.\n* конкретное значение для клиппинга нормы особо ни на что не влияет, как правило (и в оригинальной статье тоже) его всегда ставят единицой","metadata":{}},{"cell_type":"code","source":"assert torch.cuda.is_available()\ndevice = torch.device('cuda')\n\ntrainer = Trainer(\n    model,\n    optimizer,\n    scheduler,\n    tokenizer.pad_token_id,\n    device,\n    num_accum_steps=8,\n    logdir='bert',\n    max_grad_norm=1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:44:02.197916Z","iopub.execute_input":"2024-11-21T22:44:02.198822Z","iopub.status.idle":"2024-11-21T22:44:02.225690Z","shell.execute_reply.started":"2024-11-21T22:44:02.198785Z","shell.execute_reply":"2024-11-21T22:44:02.224922Z"}},"outputs":[],"execution_count":212},{"cell_type":"code","source":"trainer.train(dl, n_epochs=2)\n    \ntorch.save(\n    model.state_dict(),\n    'pretrained_weights.pt'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T22:44:02.629900Z","iopub.execute_input":"2024-11-21T22:44:02.630557Z","iopub.status.idle":"2024-11-21T23:25:21.777713Z","shell.execute_reply.started":"2024-11-21T22:44:02.630524Z","shell.execute_reply":"2024-11-21T23:25:21.776754Z"}},"outputs":[{"name":"stdout","text":"STEPS: 21340\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b277ce3337430eaa68ce4beb6ef8ea"}},"metadata":{}},{"name":"stdout","text":"Current_step 199 | BatchLoss 30.264 | Learning rate 0.0000053 | GradientNorm 71.195\nCurrent_step 399 | BatchLoss 26.368 | Learning rate 0.0000076 | GradientNorm 67.871\nCurrent_step 599 | BatchLoss 18.866 | Learning rate 0.0000099 | GradientNorm 38.114\nCurrent_step 799 | BatchLoss 18.092 | Learning rate 0.0000122 | GradientNorm 10.772\nCurrent_step 999 | BatchLoss 16.392 | Learning rate 0.0000145 | GradientNorm 8.796\nCurrent_step 1199 | BatchLoss 17.216 | Learning rate 0.0000168 | GradientNorm 9.267\nCurrent_step 1399 | BatchLoss 15.343 | Learning rate 0.0000192 | GradientNorm 7.266\nCurrent_step 1599 | BatchLoss 12.991 | Learning rate 0.0000215 | GradientNorm 6.108\nCurrent_step 1799 | BatchLoss 13.265 | Learning rate 0.0000238 | GradientNorm 4.887\nCurrent_step 1999 | BatchLoss 12.753 | Learning rate 0.0000261 | GradientNorm 4.359\nCurrent_step 2199 | BatchLoss 12.458 | Learning rate 0.0000284 | GradientNorm 4.008\nCurrent_step 2399 | BatchLoss 12.769 | Learning rate 0.0000307 | GradientNorm 4.272\nCurrent_step 2599 | BatchLoss 11.313 | Learning rate 0.0000330 | GradientNorm 3.241\nCurrent_step 2799 | BatchLoss 12.024 | Learning rate 0.0000353 | GradientNorm 3.243\nCurrent_step 2999 | BatchLoss 11.076 | Learning rate 0.0000376 | GradientNorm 2.990\nCurrent_step 3199 | BatchLoss 11.025 | Learning rate 0.0000399 | GradientNorm 2.879\nCurrent_step 3399 | BatchLoss 12.617 | Learning rate 0.0000422 | GradientNorm 3.041\nCurrent_step 3599 | BatchLoss 10.011 | Learning rate 0.0000445 | GradientNorm 2.718\nCurrent_step 3799 | BatchLoss 10.622 | Learning rate 0.0000468 | GradientNorm 2.483\nCurrent_step 3999 | BatchLoss 10.104 | Learning rate 0.0000492 | GradientNorm 2.665\nCurrent_step 4199 | BatchLoss 10.085 | Learning rate 0.0000515 | GradientNorm 3.043\nCurrent_step 4399 | BatchLoss 9.223 | Learning rate 0.0000538 | GradientNorm 2.625\nCurrent_step 4599 | BatchLoss 10.262 | Learning rate 0.0000561 | GradientNorm 2.557\nCurrent_step 4799 | BatchLoss 10.260 | Learning rate 0.0000584 | GradientNorm 4.023\nCurrent_step 4999 | BatchLoss 8.637 | Learning rate 0.0000607 | GradientNorm 2.372\nCurrent_step 5199 | BatchLoss 9.260 | Learning rate 0.0000630 | GradientNorm 2.357\nCurrent_step 5399 | BatchLoss 8.496 | Learning rate 0.0000653 | GradientNorm 3.020\nCurrent_step 5599 | BatchLoss 7.957 | Learning rate 0.0000676 | GradientNorm 2.713\nCurrent_step 5799 | BatchLoss 8.182 | Learning rate 0.0000699 | GradientNorm 2.767\nCurrent_step 5999 | BatchLoss 9.129 | Learning rate 0.0000722 | GradientNorm 2.666\nCurrent_step 6199 | BatchLoss 8.475 | Learning rate 0.0000745 | GradientNorm 2.566\nCurrent_step 6399 | BatchLoss 8.064 | Learning rate 0.0000769 | GradientNorm 2.358\nCurrent_step 6599 | BatchLoss 7.846 | Learning rate 0.0000792 | GradientNorm 1.961\nCurrent_step 6799 | BatchLoss 7.248 | Learning rate 0.0000815 | GradientNorm 2.110\nCurrent_step 6999 | BatchLoss 6.593 | Learning rate 0.0000838 | GradientNorm 2.236\nCurrent_step 7199 | BatchLoss 6.043 | Learning rate 0.0000861 | GradientNorm 2.146\nCurrent_step 7399 | BatchLoss 6.744 | Learning rate 0.0000884 | GradientNorm 2.091\nCurrent_step 7599 | BatchLoss 6.639 | Learning rate 0.0000907 | GradientNorm 1.975\nCurrent_step 7799 | BatchLoss 6.251 | Learning rate 0.0000930 | GradientNorm 2.021\nCurrent_step 7999 | BatchLoss 6.181 | Learning rate 0.0000953 | GradientNorm 1.858\nCurrent_step 8199 | BatchLoss 6.046 | Learning rate 0.0000976 | GradientNorm 1.875\nCurrent_step 8399 | BatchLoss 6.173 | Learning rate 0.0000999 | GradientNorm 1.881\nCurrent_step 8599 | BatchLoss 5.721 | Learning rate 0.0001022 | GradientNorm 1.894\nCurrent_step 8799 | BatchLoss 6.004 | Learning rate 0.0001045 | GradientNorm 1.980\nCurrent_step 8999 | BatchLoss 5.405 | Learning rate 0.0001069 | GradientNorm 1.904\nCurrent_step 9199 | BatchLoss 5.557 | Learning rate 0.0001092 | GradientNorm 1.618\nCurrent_step 9399 | BatchLoss 5.168 | Learning rate 0.0001115 | GradientNorm 2.918\nCurrent_step 9599 | BatchLoss 5.376 | Learning rate 0.0001138 | GradientNorm 1.445\nCurrent_step 9799 | BatchLoss 5.276 | Learning rate 0.0001161 | GradientNorm 1.880\nCurrent_step 9999 | BatchLoss 5.019 | Learning rate 0.0001184 | GradientNorm nan\nCurrent_step 10199 | BatchLoss 4.891 | Learning rate 0.0001207 | GradientNorm 1.921\nCurrent_step 10399 | BatchLoss 4.964 | Learning rate 0.0001230 | GradientNorm 1.541\nCurrent_step 10599 | BatchLoss 5.010 | Learning rate 0.0001253 | GradientNorm 1.895\n\nTRAIN TIME: 1239.7154\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05957224cfb449da85cf9625280a6084"}},"metadata":{}},{"name":"stdout","text":"Current_step 10869 | BatchLoss 4.597 | Learning rate 0.0001285 | GradientNorm 1.592\nCurrent_step 11069 | BatchLoss 4.568 | Learning rate 0.0001308 | GradientNorm 1.552\nCurrent_step 11269 | BatchLoss 4.685 | Learning rate 0.0001331 | GradientNorm 1.567\nCurrent_step 11469 | BatchLoss 4.288 | Learning rate 0.0001354 | GradientNorm 1.765\nCurrent_step 11669 | BatchLoss 4.097 | Learning rate 0.0001377 | GradientNorm 1.598\nCurrent_step 11869 | BatchLoss 4.509 | Learning rate 0.0001400 | GradientNorm 1.955\nCurrent_step 12069 | BatchLoss 4.921 | Learning rate 0.0001423 | GradientNorm 1.933\nCurrent_step 12269 | BatchLoss 4.592 | Learning rate 0.0001446 | GradientNorm 2.204\nCurrent_step 12469 | BatchLoss 4.160 | Learning rate 0.0001469 | GradientNorm 1.695\nCurrent_step 12669 | BatchLoss 4.420 | Learning rate 0.0001492 | GradientNorm 1.535\nCurrent_step 12869 | BatchLoss 4.430 | Learning rate 0.0001515 | GradientNorm 1.658\nCurrent_step 13069 | BatchLoss 5.004 | Learning rate 0.0001538 | GradientNorm 2.211\nCurrent_step 13269 | BatchLoss 4.365 | Learning rate 0.0001562 | GradientNorm 1.980\nCurrent_step 13469 | BatchLoss 3.991 | Learning rate 0.0001585 | GradientNorm 1.817\nCurrent_step 13669 | BatchLoss 4.500 | Learning rate 0.0001608 | GradientNorm 1.827\nCurrent_step 13869 | BatchLoss 4.858 | Learning rate 0.0001631 | GradientNorm 1.765\nCurrent_step 14069 | BatchLoss 4.195 | Learning rate 0.0001654 | GradientNorm 1.706\nCurrent_step 14269 | BatchLoss 4.583 | Learning rate 0.0001677 | GradientNorm 1.988\nCurrent_step 14469 | BatchLoss 4.454 | Learning rate 0.0001700 | GradientNorm 1.690\nCurrent_step 14669 | BatchLoss 4.533 | Learning rate 0.0001723 | GradientNorm 1.623\nCurrent_step 14869 | BatchLoss 4.901 | Learning rate 0.0001746 | GradientNorm 1.877\nCurrent_step 15069 | BatchLoss 4.698 | Learning rate 0.0001769 | GradientNorm 2.110\nCurrent_step 15269 | BatchLoss 4.396 | Learning rate 0.0001792 | GradientNorm 1.920\nCurrent_step 15469 | BatchLoss 4.978 | Learning rate 0.0001815 | GradientNorm 1.963\nCurrent_step 15669 | BatchLoss 5.419 | Learning rate 0.0001838 | GradientNorm 1.688\nCurrent_step 15869 | BatchLoss 5.069 | Learning rate 0.0001862 | GradientNorm 2.183\nCurrent_step 16069 | BatchLoss 5.445 | Learning rate 0.0001885 | GradientNorm 2.333\nCurrent_step 16269 | BatchLoss 4.907 | Learning rate 0.0001908 | GradientNorm 1.790\nCurrent_step 16469 | BatchLoss 5.428 | Learning rate 0.0001931 | GradientNorm 2.768\nCurrent_step 16669 | BatchLoss 5.698 | Learning rate 0.0001954 | GradientNorm 2.976\nCurrent_step 16869 | BatchLoss 5.673 | Learning rate 0.0001977 | GradientNorm 3.076\nCurrent_step 17069 | BatchLoss 5.806 | Learning rate 0.0002000 | GradientNorm 2.710\nCurrent_step 17269 | BatchLoss 6.106 | Learning rate 0.0001997 | GradientNorm 3.611\nCurrent_step 17469 | BatchLoss 6.134 | Learning rate 0.0001995 | GradientNorm 6.308\nCurrent_step 17669 | BatchLoss 6.936 | Learning rate 0.0001992 | GradientNorm 6.948\nCurrent_step 17869 | BatchLoss 6.407 | Learning rate 0.0001990 | GradientNorm 3.750\nCurrent_step 18069 | BatchLoss 6.406 | Learning rate 0.0001987 | GradientNorm 3.950\nCurrent_step 18269 | BatchLoss 7.033 | Learning rate 0.0001984 | GradientNorm 5.311\nCurrent_step 18469 | BatchLoss 8.517 | Learning rate 0.0001982 | GradientNorm 5.436\nCurrent_step 18669 | BatchLoss 8.687 | Learning rate 0.0001979 | GradientNorm 2.397\nCurrent_step 18869 | BatchLoss 7.582 | Learning rate 0.0001977 | GradientNorm 7.975\nCurrent_step 19069 | BatchLoss 10.602 | Learning rate 0.0001974 | GradientNorm 3.671\nCurrent_step 19269 | BatchLoss 12.721 | Learning rate 0.0001971 | GradientNorm 3.048\nCurrent_step 19469 | BatchLoss 14.351 | Learning rate 0.0001969 | GradientNorm 0.027\nCurrent_step 19669 | BatchLoss 15.111 | Learning rate 0.0001966 | GradientNorm 0.018\nCurrent_step 19869 | BatchLoss 15.103 | Learning rate 0.0001964 | GradientNorm 0.000\nCurrent_step 20069 | BatchLoss 16.634 | Learning rate 0.0001961 | GradientNorm 0.000\nCurrent_step 20269 | BatchLoss 15.673 | Learning rate 0.0001958 | GradientNorm 0.000\nCurrent_step 20469 | BatchLoss 15.933 | Learning rate 0.0001956 | GradientNorm 0.000\nCurrent_step 20669 | BatchLoss 15.651 | Learning rate 0.0001953 | GradientNorm nan\nCurrent_step 20869 | BatchLoss 15.421 | Learning rate 0.0001951 | GradientNorm 0.000\nCurrent_step 21069 | BatchLoss 16.085 | Learning rate 0.0001948 | GradientNorm 0.000\nCurrent_step 21269 | BatchLoss 15.128 | Learning rate 0.0001945 | GradientNorm 0.000\n\nTRAIN TIME: 1239.3399\n\n","output_type":"stream"}],"execution_count":213},{"cell_type":"markdown","source":"После предобучения вам придется перезапустить ноутбук и снова перепрогнать блоки, нужные для дообучения. Использование apex'а ломает обучение других моделей (которые не передавались в `amp.initialize`) в одном запуске. Если не перезапустить, скор получится гораздо хуже.","metadata":{}},{"cell_type":"markdown","source":"## Часть 4. Дообучение (5 баллов)\n\nСамая сложная часть уже позади, осталось чуть-чуть :)\n\nТак как для дообучения доступно гораздо меньше данных, оно занимает гораздо меньше времени.","metadata":{}},{"cell_type":"code","source":"train = data.loc[data['split'] == 'train'].reset_index(drop=True).copy()\nval = data.loc[data['split'] == 'val'].reset_index(drop=True).copy()\ntest = data.loc[data['split'] == 'test'].reset_index(drop=True).copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:30.795854Z","iopub.execute_input":"2024-11-21T23:51:30.796212Z","iopub.status.idle":"2024-11-21T23:51:31.416800Z","shell.execute_reply.started":"2024-11-21T23:51:30.796180Z","shell.execute_reply":"2024-11-21T23:51:31.416096Z"}},"outputs":[],"execution_count":383},{"cell_type":"markdown","source":"Датасет для дообучения выглядит стандартно: нужно токенизировать и запомнить тексты и соответствующие им метки, и затем в методе `__getitem__` их выдавать:","metadata":{}},{"cell_type":"code","source":"class FinetuneDataset(Dataset):\n    \n    def __init__(\n            self, \n            texts, \n            targets, \n            tokenizer,\n            maxlen, \n            presort=False\n    ):\n        \"\"\"\n            texts: list of strings. Тексты чеков\n            targets: list of ints. Категории товаров\n            tokenizer: токенизатор\n            maxlen: максимальная длина текста\n            presort: отсортировать тексты по длине\n        \"\"\"\n        super().__init__()\n        self._maxlen = maxlen\n        self._ds = [tokenizer(text) for text in texts]\n        \n        if presort:\n            self._ds.sort(key=len)\n\n        self._ds = list(zip(self._ds, targets))\n        \n    def __len__(self):\n        return len(self._ds)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n            returns: input_ids - индексы токенов токенизированного текста, target - категория\n        \"\"\"\n        input_ids, target = self._ds[idx]\n        input_ids = input_ids[:self._maxlen]\n        return torch.tensor(input_ids, dtype=torch.int64), torch.tensor(target, dtype=torch.int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:31.418403Z","iopub.execute_input":"2024-11-21T23:51:31.418688Z","iopub.status.idle":"2024-11-21T23:51:31.425404Z","shell.execute_reply.started":"2024-11-21T23:51:31.418661Z","shell.execute_reply":"2024-11-21T23:51:31.424428Z"}},"outputs":[],"execution_count":384},{"cell_type":"markdown","source":"Создайте датасеты для обучения и валидации:","metadata":{}},{"cell_type":"code","source":"train_ds = FinetuneDataset(\n    train['text'].values, \n    train['label'].values, \n    maxlen=MAX_SEQLEN, \n    tokenizer=tokenizer\n)\nval_ds = FinetuneDataset(\n    val['text'].values, \n    val['label'].values, \n    maxlen=MAX_SEQLEN, \n    tokenizer=tokenizer, \n    presort=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:31.426471Z","iopub.execute_input":"2024-11-21T23:51:31.426855Z","iopub.status.idle":"2024-11-21T23:51:34.273140Z","shell.execute_reply.started":"2024-11-21T23:51:31.426819Z","shell.execute_reply":"2024-11-21T23:51:34.272396Z"}},"outputs":[],"execution_count":385},{"cell_type":"markdown","source":"Коллатор для дообучения делает только паддинг и конвертацию таргетов в тензоры:","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch, pad_idx):\n    input_ids, target = zip(*batch)\n    input_ids = pad_sequence(input_ids, \n                             batch_first=True, \n                             padding_value=pad_idx)\n    \n    return input_ids, torch.tensor(target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.275219Z","iopub.execute_input":"2024-11-21T23:51:34.275493Z","iopub.status.idle":"2024-11-21T23:51:34.279826Z","shell.execute_reply.started":"2024-11-21T23:51:34.275468Z","shell.execute_reply":"2024-11-21T23:51:34.278947Z"}},"outputs":[],"execution_count":386},{"cell_type":"markdown","source":"Создайте даталоадеры для обучения и валидации:","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\n\ndataloaders = {\n    'train': DataLoader(\n        train_ds,\n        batch_size=BATCH_SIZE, \n        shuffle=True, \n        collate_fn=lambda batch: collate_fn(batch, tokenizer.pad_token_id)\n    ),\n    'eval': DataLoader(\n        val_ds,\n        batch_size=BATCH_SIZE, \n        shuffle=False, \n        collate_fn=lambda batch: collate_fn(batch, tokenizer.pad_token_id)\n    )\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.280803Z","iopub.execute_input":"2024-11-21T23:51:34.281013Z","iopub.status.idle":"2024-11-21T23:51:34.291684Z","shell.execute_reply.started":"2024-11-21T23:51:34.280991Z","shell.execute_reply":"2024-11-21T23:51:34.290888Z"}},"outputs":[],"execution_count":387},{"cell_type":"markdown","source":"В модели теперь отсутствует MLM голова, а вместо SOP задачи голова классификации решает задачу определения категорий товаров.","metadata":{}},{"cell_type":"code","source":"class BertFinetuneModel(nn.Module):\n\n    def __init__(\n            self, \n            hidden_size, \n            vocab_size, \n            max_seqlen,\n            num_hidden_layers,\n            intermediate_size,\n            num_attention_heads,\n            num_classes,\n            act_func='relu',\n            input_dropout_prob=0.,\n            hidden_dropout_prob=0., \n            attention_probs_dropout_prob=0.,\n            eps=1e-3\n    ):\n        super().__init__()\n        self._backbone = Bert(\n            vocab_size=vocab_size,\n            max_seqlen=max_seqlen,\n            hidden_size=hidden_size,\n            num_hidden_layers=num_hidden_layers,\n            intermediate_size=intermediate_size, \n            num_attention_heads=num_attention_heads, \n            input_dropout_prob=input_dropout_prob,\n            dropout_prob=hidden_dropout_prob, \n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            act_func=act_func,\n            eps=eps\n        )\n        self._classifier_head = ClassifierHead(hidden_size, num_classes, hidden_dropout_prob)\n\n    def forward(self, x, attention_mask):\n        hidden_states = self._backbone(x, attention_mask)\n        return self._classifier_head(hidden_states)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.292437Z","iopub.execute_input":"2024-11-21T23:51:34.292715Z","iopub.status.idle":"2024-11-21T23:51:34.305679Z","shell.execute_reply.started":"2024-11-21T23:51:34.292673Z","shell.execute_reply":"2024-11-21T23:51:34.304902Z"}},"outputs":[],"execution_count":388},{"cell_type":"markdown","source":"Используйте ту же архитектуру, которую вы выбрали при предобучении. Количество классов - 96:","metadata":{}},{"cell_type":"code","source":"model = BertFinetuneModel(\n    hidden_size=256, \n    vocab_size=30000,\n    max_seqlen=MAX_SEQLEN,\n    num_hidden_layers=4,\n    intermediate_size=(4 * 256),\n    num_attention_heads=(256 // 64),\n    num_classes=96,\n    act_func='gelu',\n    input_dropout_prob=0.05,\n    hidden_dropout_prob=0.05, \n    attention_probs_dropout_prob=0.05,\n    eps=1e-3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.306723Z","iopub.execute_input":"2024-11-21T23:51:34.307040Z","iopub.status.idle":"2024-11-21T23:51:34.401966Z","shell.execute_reply.started":"2024-11-21T23:51:34.307004Z","shell.execute_reply":"2024-11-21T23:51:34.401383Z"}},"outputs":[],"execution_count":389},{"cell_type":"markdown","source":"Подгрузить предобученные веса можно с помощью следующей функции:","metadata":{}},{"cell_type":"code","source":"def load_weights(self, path):\n    found = []\n    with open(path, 'rb') as f:\n        weights = torch.load(f, weights_only=True)\n    for name, param in weights.items():\n        if name in self.state_dict():\n            if param.shape == self.state_dict()[name].shape:\n                self.state_dict()[name].copy_(param)\n                found.append(name)\n\n    return found\n\nfound = load_weights(model, 'pretrained_weights.pt')\n\nprint('Amount of found weights: {}'.format(len(found)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.402756Z","iopub.execute_input":"2024-11-21T23:51:34.402981Z","iopub.status.idle":"2024-11-21T23:51:34.501808Z","shell.execute_reply.started":"2024-11-21T23:51:34.402957Z","shell.execute_reply":"2024-11-21T23:51:34.500907Z"}},"outputs":[{"name":"stdout","text":"Amount of found weights: 55\n","output_type":"stream"}],"execution_count":390},{"cell_type":"markdown","source":"Создайте оптимизатор и расписание лр. Про гиперпараметры:\n* при дообучении используют маленький batch_size $\\in \\{32, 64\\}$\n* маленький learning rate:  $\\{1e-5, 2e-5, 4e-5\\}$ для больших моделей, для моделей вида bert-mini можно использовать и побольше: $\\{1e-4, 2e-4, 4e-4\\}$ \n* финальное значение все также маленькое\n* вормап можно делать где-то 0.06 от всех шагов обучения\n* количество эпох для дообучения - больше шести здесь не нужно\n* weight decay здесь потенциально можно использовать побольше, чем при предобучении","metadata":{}},{"cell_type":"code","source":"optimizer = get_optimizer(model, weight_decay=1e-2)\nscheduler = Scheduler(\n    optimizer,\n    init_lr=1e-6,\n    peak_lr=4e-4,\n    final_lr=0,\n    num_warmup_steps=int(0.1 * len(dataloaders['train']) * 30),\n    num_training_steps=len(dataloaders['train']) * 30\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.503144Z","iopub.execute_input":"2024-11-21T23:51:34.503576Z","iopub.status.idle":"2024-11-21T23:51:34.508976Z","shell.execute_reply.started":"2024-11-21T23:51:34.503532Z","shell.execute_reply":"2024-11-21T23:51:34.508057Z"}},"outputs":[],"execution_count":391},{"cell_type":"markdown","source":"Осталось создать пайплайн обучения:\n* apex использовать не нужно, дообучение быстрое и не требует больших батчей\n* аккумуляция градиентов не нужна т.к. батчи  маленькие\n* лосс теперь считается вне модели, в Trainer нужно использовать torch.nn.CrossEntropyLoss","metadata":{}},{"cell_type":"code","source":"class FinetuneTrainer:\n\n    def __init__(\n            self, \n            model, \n            optimizer, \n            scheduler,\n            pad_token_id,\n            device,\n            logdir=None,\n            max_grad_norm=None\n    ):\n        \"\"\"\n            model: объект класса BertModel\n            optimizer: оптимизатор\n            scheduler: расписание learning rate. Нужно вызывать scheduler.step() ПОСЛЕ optimizer.step()\n            pad_token_id: индекс паддинга. Нужен для создания attention mask\n            device: девайс (cpu или cuda), на котором надо производить вычисления\n            num_accum_steps: количество шагов аккумуляции\n            logdir: директория для записи логов\n            max_grad_norm: максимум нормы градиентов, для клиппинга\n        \"\"\"\n        names = [\"_model\", \"_optimizer\", \"_scheduler\", \n                 \"_pad_token_id\", \"_device\", \"_logdir\", \n                 \"_max_grad_norm\", \"_writer\", \"_criterion\",\n                 \"_verbose_step\", \"_n_epoch\"]\n        values = [model.to(device), optimizer, scheduler, \n                  pad_token_id, device, logdir, \n                  max_grad_norm if max_grad_norm is not None else 1,\n                  SummaryWriter(logdir) if logdir else None,\n                  nn.CrossEntropyLoss(), 200, 0]\n        for name, value in zip(names, values):\n            setattr(self, name, value)\n\n    def train(self, dataloaders, n_epochs, scorer=None):\n        \"\"\"\n            dataloaders: dict of dataloaders, keys 'train', 'eval' should be present.\n            n_epochs: int. Num epochs to train for.\n            scorer: takes trainer, outputs metric name and value as a tuple.\n        \"\"\"\n        print(f\"TrainSteps: {n_epochs * len(dataloaders['train'])}\")\n        print(f\"EvalSteps: {n_epochs * len(dataloaders['eval'])}\")\n        for epoch in range(n_epochs):\n            train_loss = self._train_step(dataloaders['train'])\n            val_loss = self._eval_step(dataloaders['eval'])\n\n            print(f\"Epoch [{epoch}], ValLoss [{val_loss:.3f}], TrainLoss [{train_loss:.3f}]\")\n\n            if self._writer is not None:\n                self._writer.add_scalar('eval', val_loss, global_step=self._n_epoch)\n                \n                if scorer is not None:\n                    name, value = scorer(self)\n                    self._writer.add_scalar(name, value, global_step=self._n_epoch)\n                    \n            self._n_epoch += 1\n\n    def _train_step(self, dataloader):\n        \"\"\"\n            dataloader: training dataloader.\n            \n            returns: train_loss\n        \"\"\"\n        self._model.train()\n        total_loss = 0.0\n        self._optimizer.zero_grad()\n        for step, batch in tqdm(enumerate(dataloader)):\n            tokens, targets = batch[0].to(self._device), batch[1].to(self._device)\n            attention_mask = (tokens != self._pad_token_id).int()\n            output = self._model(tokens, attention_mask)\n            loss = self._criterion(output, targets)\n            loss.backward()\n            if self._max_grad_norm is not None:\n                grad_norm = clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n            else:\n                grad_norm = None\n            self._optimizer.step()\n            self._scheduler.step()\n            self._optimizer.zero_grad()\n            total_loss += loss.item()\n            if (step + 1) % self._verbose_step == 0:\n                current_step = self._n_epoch * len(dataloader) + step\n                print(f\"Current_step {current_step} | BatchLoss {(loss.item()):.3f} | \", end='')\n                print(f\"Learning rate {self._scheduler.get_last_lr()[0]:.7f} | GradientNorm {grad_norm:.3f}\")\n        return total_loss / len(dataloader)\n\n    @torch.no_grad()\n    def _eval_step(self, dataloader):\n        \"\"\"\n            dataloader: evaluation dataloader.\n            \n            returns: eval loss\n        \"\"\"\n        self._model.eval()\n        total_loss = 0.0\n        for step, batch in tqdm(enumerate(dataloader)):\n            tokens, targets = batch[0].to(self._device), batch[1].to(self._device)\n            attention_mask = (tokens != self._pad_token_id).int()\n            output = self._model(tokens, attention_mask)\n            loss = self._criterion(output, targets)\n            total_loss += loss.item()\n            if (step + 1) % self._verbose_step == 0:\n                current_step = self._n_epoch * len(dataloader) + step\n                print(f\"Current_Evalstep {current_step} | EvalBatchLoss {(loss.item()):.3f}\")\n        return total_loss / len(dataloader)\n\n    @torch.no_grad()\n    def predict(self, dataloader):\n        \"\"\"\n            dataloader: inference dataloader. Should not have targets.\n            \n            returns: np.array c предсказанными категориями\n        \"\"\"\n        self._model.eval()\n        predictions = []\n        for tokens in dataloader:\n            tokens = tokens.to(self._device)\n            attention_mask = (tokens != self._pad_token_id).int()\n            output = self._model(tokens, attention_mask)\n            predictions.append(torch.argmax(output, dim=-1))\n        return torch.cat(predictions).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.511256Z","iopub.execute_input":"2024-11-21T23:51:34.511540Z","iopub.status.idle":"2024-11-21T23:51:34.528077Z","shell.execute_reply.started":"2024-11-21T23:51:34.511516Z","shell.execute_reply":"2024-11-21T23:51:34.527285Z"}},"outputs":[],"execution_count":392},{"cell_type":"code","source":"device = torch.device('cuda')\n\ntrainer = FinetuneTrainer(\n    model,\n    optimizer,\n    scheduler,\n    tokenizer.pad_token_id,\n    device,\n    logdir='finetuned_bert_mini',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.528926Z","iopub.execute_input":"2024-11-21T23:51:34.529198Z","iopub.status.idle":"2024-11-21T23:51:34.556726Z","shell.execute_reply.started":"2024-11-21T23:51:34.529159Z","shell.execute_reply":"2024-11-21T23:51:34.556117Z"}},"outputs":[],"execution_count":393},{"cell_type":"markdown","source":"Для мониторинга целевой метрики используйте предоставленный scorer:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n\nclass InferenceDataset(Dataset):\n    \n    def __init__(self, texts, maxlen, tokenizer):\n        \"\"\"\n            texts: list of str. Сырые тексты чеков\n            maxlen: максимальная длина текста\n            tokenizer: токенизатор\n        \"\"\"\n        self._texts = [tokenizer(text) if tokenizer is not None else text for text in texts]\n        self._maxlen = maxlen\n        \n    def __len__(self):\n        return len(self._texts)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n            returns: тензор из индексов токенов токенизированного текста\n        \"\"\"\n        text = self._texts[idx]\n        if self._maxlen is not None:\n            text = text[:self._maxlen]\n        return torch.tensor(text, dtype=torch.long)\n    \ndef make_scorer(texts, targets, tokenizer, maxlen):\n    inference_ds = InferenceDataset(texts, maxlen=maxlen, tokenizer=tokenizer)\n    inference_dl = DataLoader(inference_ds, batch_size=32, shuffle=False, collate_fn=inference_collate_fn)\n    def get_score(trainer):\n        preds = trainer.predict(inference_dl)\n        return 'f1', f1_score(targets, preds, average='weighted')\n    return get_score\n\n\ninference_collate_fn = lambda x: pad_sequence(x, batch_first=True, padding_value=tokenizer.pad_token_id)\n\nval_scorer = make_scorer(val['text'].values, val['label'].values, tokenizer, maxlen=MAX_SEQLEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.557629Z","iopub.execute_input":"2024-11-21T23:51:34.557872Z","iopub.status.idle":"2024-11-21T23:51:34.882102Z","shell.execute_reply.started":"2024-11-21T23:51:34.557849Z","shell.execute_reply":"2024-11-21T23:51:34.881518Z"}},"outputs":[],"execution_count":394},{"cell_type":"markdown","source":"Скор на валидационной выборке до обучения:","metadata":{}},{"cell_type":"code","source":"val_scorer(trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:34.883039Z","iopub.execute_input":"2024-11-21T23:51:34.883271Z","iopub.status.idle":"2024-11-21T23:51:35.484541Z","shell.execute_reply.started":"2024-11-21T23:51:34.883248Z","shell.execute_reply":"2024-11-21T23:51:35.483678Z"}},"outputs":[{"execution_count":395,"output_type":"execute_result","data":{"text/plain":"('f1', 1.4473506503271914e-05)"},"metadata":{}}],"execution_count":395},{"cell_type":"code","source":"trainer.train(dataloaders, n_epochs=30, scorer=val_scorer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:51:35.485619Z","iopub.execute_input":"2024-11-21T23:51:35.486012Z","iopub.status.idle":"2024-11-21T23:58:58.182258Z","shell.execute_reply.started":"2024-11-21T23:51:35.485959Z","shell.execute_reply":"2024-11-21T23:58:58.181565Z"}},"outputs":[{"name":"stdout","text":"TrainSteps: 36210\nEvalSteps: 4530\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b674165fc7fb47e387248997bae9992d"}},"metadata":{}},{"name":"stdout","text":"Current_step 199 | BatchLoss 3.957 | Learning rate 0.0000230 | GradientNorm 2.072\nCurrent_step 399 | BatchLoss 3.499 | Learning rate 0.0000451 | GradientNorm 2.887\nCurrent_step 599 | BatchLoss 3.810 | Learning rate 0.0000671 | GradientNorm 4.155\nCurrent_step 799 | BatchLoss 3.429 | Learning rate 0.0000892 | GradientNorm 6.813\nCurrent_step 999 | BatchLoss 2.789 | Learning rate 0.0001112 | GradientNorm 5.941\nCurrent_step 1199 | BatchLoss 2.997 | Learning rate 0.0001332 | GradientNorm 6.250\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6fc432eed66492d96a11ff408ed4733"}},"metadata":{}},{"name":"stdout","text":"Epoch [0], ValLoss [4.224], TrainLoss [3.440]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3eddbdd4ce2460591383a7beb78d2ed"}},"metadata":{}},{"name":"stdout","text":"Current_step 1406 | BatchLoss 2.756 | Learning rate 0.0001560 | GradientNorm 5.558\nCurrent_step 1606 | BatchLoss 2.722 | Learning rate 0.0001781 | GradientNorm 6.937\nCurrent_step 1806 | BatchLoss 2.573 | Learning rate 0.0002001 | GradientNorm 5.928\nCurrent_step 2006 | BatchLoss 1.868 | Learning rate 0.0002222 | GradientNorm 8.560\nCurrent_step 2206 | BatchLoss 2.821 | Learning rate 0.0002442 | GradientNorm 17.541\nCurrent_step 2406 | BatchLoss 2.192 | Learning rate 0.0002662 | GradientNorm 4.982\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d43a7ab3234a7f89172e2dad201a2c"}},"metadata":{}},{"name":"stdout","text":"Epoch [1], ValLoss [5.245], TrainLoss [2.359]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd5845991d14ee1934504a98e0e9507"}},"metadata":{}},{"name":"stdout","text":"Current_step 2613 | BatchLoss 1.935 | Learning rate 0.0002890 | GradientNorm 9.801\nCurrent_step 2813 | BatchLoss 2.128 | Learning rate 0.0003111 | GradientNorm 7.525\nCurrent_step 3013 | BatchLoss 1.897 | Learning rate 0.0003331 | GradientNorm 7.523\nCurrent_step 3213 | BatchLoss 1.600 | Learning rate 0.0003552 | GradientNorm 7.382\nCurrent_step 3413 | BatchLoss 1.691 | Learning rate 0.0003772 | GradientNorm 5.486\nCurrent_step 3613 | BatchLoss 1.818 | Learning rate 0.0003992 | GradientNorm 6.778\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9a50bcf01e24cf0a343ef849361598d"}},"metadata":{}},{"name":"stdout","text":"Epoch [2], ValLoss [6.158], TrainLoss [1.842]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e015cc4fbac4718be2b2727f7d2e732"}},"metadata":{}},{"name":"stdout","text":"Current_step 3820 | BatchLoss 1.020 | Learning rate 0.0003975 | GradientNorm 10.210\nCurrent_step 4020 | BatchLoss 1.854 | Learning rate 0.0003951 | GradientNorm 6.305\nCurrent_step 4220 | BatchLoss 1.453 | Learning rate 0.0003926 | GradientNorm 6.116\nCurrent_step 4420 | BatchLoss 1.028 | Learning rate 0.0003902 | GradientNorm 7.241\nCurrent_step 4620 | BatchLoss 1.401 | Learning rate 0.0003877 | GradientNorm 7.048\nCurrent_step 4820 | BatchLoss 1.399 | Learning rate 0.0003853 | GradientNorm 12.139\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b13aad75c1d648aeb90bdc9716df4116"}},"metadata":{}},{"name":"stdout","text":"Epoch [3], ValLoss [6.819], TrainLoss [1.475]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25a9eadfbd3a4967871e910ed4566396"}},"metadata":{}},{"name":"stdout","text":"Current_step 5027 | BatchLoss 1.076 | Learning rate 0.0003827 | GradientNorm 5.841\nCurrent_step 5227 | BatchLoss 0.871 | Learning rate 0.0003803 | GradientNorm 6.209\nCurrent_step 5427 | BatchLoss 1.066 | Learning rate 0.0003778 | GradientNorm 6.561\nCurrent_step 5627 | BatchLoss 1.257 | Learning rate 0.0003754 | GradientNorm 8.499\nCurrent_step 5827 | BatchLoss 0.652 | Learning rate 0.0003729 | GradientNorm 7.663\nCurrent_step 6027 | BatchLoss 0.684 | Learning rate 0.0003705 | GradientNorm 6.162\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a5665a917a45b6acb319781dfe72c4"}},"metadata":{}},{"name":"stdout","text":"Epoch [4], ValLoss [7.408], TrainLoss [1.178]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7eb83180bc94707ab92542a11772ee7"}},"metadata":{}},{"name":"stdout","text":"Current_step 6234 | BatchLoss 1.263 | Learning rate 0.0003679 | GradientNorm 8.926\nCurrent_step 6434 | BatchLoss 1.203 | Learning rate 0.0003655 | GradientNorm 8.436\nCurrent_step 6634 | BatchLoss 0.785 | Learning rate 0.0003630 | GradientNorm 7.838\nCurrent_step 6834 | BatchLoss 1.095 | Learning rate 0.0003606 | GradientNorm 9.173\nCurrent_step 7034 | BatchLoss 1.056 | Learning rate 0.0003581 | GradientNorm 11.598\nCurrent_step 7234 | BatchLoss 1.205 | Learning rate 0.0003556 | GradientNorm 7.550\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c92d05136b4e52a078d4fcd4339393"}},"metadata":{}},{"name":"stdout","text":"Epoch [5], ValLoss [8.203], TrainLoss [0.971]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e50ccb469ab479e92e3a6391964088a"}},"metadata":{}},{"name":"stdout","text":"Current_step 7441 | BatchLoss 0.986 | Learning rate 0.0003531 | GradientNorm 8.426\nCurrent_step 7641 | BatchLoss 0.961 | Learning rate 0.0003506 | GradientNorm 10.344\nCurrent_step 7841 | BatchLoss 0.757 | Learning rate 0.0003482 | GradientNorm 6.341\nCurrent_step 8041 | BatchLoss 0.946 | Learning rate 0.0003457 | GradientNorm 10.348\nCurrent_step 8241 | BatchLoss 1.181 | Learning rate 0.0003433 | GradientNorm 7.678\nCurrent_step 8441 | BatchLoss 0.807 | Learning rate 0.0003408 | GradientNorm 6.202\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea359b467e8a465ab43d16fa48c247fe"}},"metadata":{}},{"name":"stdout","text":"Epoch [6], ValLoss [8.647], TrainLoss [0.788]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a139bca614914d84a87252e8cfda5d13"}},"metadata":{}},{"name":"stdout","text":"Current_step 8648 | BatchLoss 0.491 | Learning rate 0.0003383 | GradientNorm 6.655\nCurrent_step 8848 | BatchLoss 0.703 | Learning rate 0.0003358 | GradientNorm 8.414\nCurrent_step 9048 | BatchLoss 0.835 | Learning rate 0.0003334 | GradientNorm 8.488\nCurrent_step 9248 | BatchLoss 0.695 | Learning rate 0.0003309 | GradientNorm 5.824\nCurrent_step 9448 | BatchLoss 0.396 | Learning rate 0.0003285 | GradientNorm 5.984\nCurrent_step 9648 | BatchLoss 0.174 | Learning rate 0.0003260 | GradientNorm 3.451\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93e2202e3be749ecbe3b4ed33e11bf65"}},"metadata":{}},{"name":"stdout","text":"Epoch [7], ValLoss [9.112], TrainLoss [0.640]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05e7d6760cd64173a447cc0d6f2c2f6f"}},"metadata":{}},{"name":"stdout","text":"Current_step 9855 | BatchLoss 0.430 | Learning rate 0.0003235 | GradientNorm 7.089\nCurrent_step 10055 | BatchLoss 0.310 | Learning rate 0.0003210 | GradientNorm 6.795\nCurrent_step 10255 | BatchLoss 0.433 | Learning rate 0.0003186 | GradientNorm 7.015\nCurrent_step 10455 | BatchLoss 0.556 | Learning rate 0.0003161 | GradientNorm 7.702\nCurrent_step 10655 | BatchLoss 0.616 | Learning rate 0.0003137 | GradientNorm 8.828\nCurrent_step 10855 | BatchLoss 1.153 | Learning rate 0.0003112 | GradientNorm 9.954\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45702922e29647ae91495fb7e85f8564"}},"metadata":{}},{"name":"stdout","text":"Epoch [8], ValLoss [9.690], TrainLoss [0.515]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e163a211f4e417c856e60f7512dadf6"}},"metadata":{}},{"name":"stdout","text":"Current_step 11062 | BatchLoss 0.265 | Learning rate 0.0003087 | GradientNorm 4.777\nCurrent_step 11262 | BatchLoss 0.641 | Learning rate 0.0003062 | GradientNorm 7.028\nCurrent_step 11462 | BatchLoss 0.310 | Learning rate 0.0003037 | GradientNorm 8.640\nCurrent_step 11662 | BatchLoss 0.779 | Learning rate 0.0003013 | GradientNorm 7.330\nCurrent_step 11862 | BatchLoss 0.590 | Learning rate 0.0002988 | GradientNorm 6.195\nCurrent_step 12062 | BatchLoss 0.256 | Learning rate 0.0002964 | GradientNorm 5.551\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf6f0500a3d44740a82e6f50d94e7536"}},"metadata":{}},{"name":"stdout","text":"Epoch [9], ValLoss [10.299], TrainLoss [0.416]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36a71a40ea83453ba7a326f8ac01d03e"}},"metadata":{}},{"name":"stdout","text":"Current_step 12269 | BatchLoss 0.266 | Learning rate 0.0002938 | GradientNorm 4.590\nCurrent_step 12469 | BatchLoss 0.443 | Learning rate 0.0002914 | GradientNorm 9.159\nCurrent_step 12669 | BatchLoss 0.245 | Learning rate 0.0002889 | GradientNorm 6.962\nCurrent_step 12869 | BatchLoss 0.314 | Learning rate 0.0002865 | GradientNorm 5.018\nCurrent_step 13069 | BatchLoss 0.313 | Learning rate 0.0002840 | GradientNorm 4.807\nCurrent_step 13269 | BatchLoss 0.199 | Learning rate 0.0002816 | GradientNorm 4.788\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b103a58bc5b04fd1844d2c635f666017"}},"metadata":{}},{"name":"stdout","text":"Epoch [10], ValLoss [10.811], TrainLoss [0.340]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aca2467c8a8e4935b9e00befc21e3faa"}},"metadata":{}},{"name":"stdout","text":"Current_step 13476 | BatchLoss 0.322 | Learning rate 0.0002790 | GradientNorm 6.970\nCurrent_step 13676 | BatchLoss 0.364 | Learning rate 0.0002766 | GradientNorm 8.451\nCurrent_step 13876 | BatchLoss 0.440 | Learning rate 0.0002741 | GradientNorm 9.361\nCurrent_step 14076 | BatchLoss 0.226 | Learning rate 0.0002717 | GradientNorm 4.536\nCurrent_step 14276 | BatchLoss 0.289 | Learning rate 0.0002692 | GradientNorm 6.294\nCurrent_step 14476 | BatchLoss 0.612 | Learning rate 0.0002668 | GradientNorm 9.995\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4970b663b8847bca03882d95ff6851a"}},"metadata":{}},{"name":"stdout","text":"Epoch [11], ValLoss [11.188], TrainLoss [0.277]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ddbc2dd8784a6a92c683d64fb1b261"}},"metadata":{}},{"name":"stdout","text":"Current_step 14683 | BatchLoss 0.177 | Learning rate 0.0002642 | GradientNorm 6.088\nCurrent_step 14883 | BatchLoss 0.189 | Learning rate 0.0002618 | GradientNorm 4.866\nCurrent_step 15083 | BatchLoss 0.082 | Learning rate 0.0002593 | GradientNorm 3.703\nCurrent_step 15283 | BatchLoss 0.153 | Learning rate 0.0002568 | GradientNorm 4.433\nCurrent_step 15483 | BatchLoss 0.130 | Learning rate 0.0002544 | GradientNorm 3.761\nCurrent_step 15683 | BatchLoss 0.250 | Learning rate 0.0002519 | GradientNorm 5.805\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa3f0a988034d5ebed779e6c1b99394"}},"metadata":{}},{"name":"stdout","text":"Epoch [12], ValLoss [11.488], TrainLoss [0.227]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72f0a72b391477db00ed043c9fc2fcf"}},"metadata":{}},{"name":"stdout","text":"Current_step 15890 | BatchLoss 0.091 | Learning rate 0.0002494 | GradientNorm 4.029\nCurrent_step 16090 | BatchLoss 0.087 | Learning rate 0.0002469 | GradientNorm 3.403\nCurrent_step 16290 | BatchLoss 0.067 | Learning rate 0.0002445 | GradientNorm 2.412\nCurrent_step 16490 | BatchLoss 0.114 | Learning rate 0.0002420 | GradientNorm 5.874\nCurrent_step 16690 | BatchLoss 0.214 | Learning rate 0.0002396 | GradientNorm 5.665\nCurrent_step 16890 | BatchLoss 0.375 | Learning rate 0.0002371 | GradientNorm 8.859\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3864095f62c54c1fbeac390bcb3b0330"}},"metadata":{}},{"name":"stdout","text":"Epoch [13], ValLoss [11.967], TrainLoss [0.187]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc1771ccf11444c95c2694b03530d2a"}},"metadata":{}},{"name":"stdout","text":"Current_step 17097 | BatchLoss 0.073 | Learning rate 0.0002346 | GradientNorm 2.676\nCurrent_step 17297 | BatchLoss 0.144 | Learning rate 0.0002321 | GradientNorm 6.570\nCurrent_step 17497 | BatchLoss 0.092 | Learning rate 0.0002297 | GradientNorm 5.064\nCurrent_step 17697 | BatchLoss 0.265 | Learning rate 0.0002272 | GradientNorm 5.287\nCurrent_step 17897 | BatchLoss 0.160 | Learning rate 0.0002248 | GradientNorm 5.216\nCurrent_step 18097 | BatchLoss 0.141 | Learning rate 0.0002223 | GradientNorm 6.022\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1b113808f8411d990b9135b4dbcc9a"}},"metadata":{}},{"name":"stdout","text":"Epoch [14], ValLoss [12.496], TrainLoss [0.154]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb88aa18070c4c2081f6cde3becd589a"}},"metadata":{}},{"name":"stdout","text":"Current_step 18304 | BatchLoss 0.095 | Learning rate 0.0002198 | GradientNorm 4.131\nCurrent_step 18504 | BatchLoss 0.091 | Learning rate 0.0002173 | GradientNorm 3.357\nCurrent_step 18704 | BatchLoss 0.076 | Learning rate 0.0002149 | GradientNorm 2.863\nCurrent_step 18904 | BatchLoss 0.171 | Learning rate 0.0002124 | GradientNorm 4.678\nCurrent_step 19104 | BatchLoss 0.117 | Learning rate 0.0002099 | GradientNorm 4.936\nCurrent_step 19304 | BatchLoss 0.063 | Learning rate 0.0002075 | GradientNorm 3.680\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f2526c38a994e1f87fdea095c0c6448"}},"metadata":{}},{"name":"stdout","text":"Epoch [15], ValLoss [12.885], TrainLoss [0.132]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c98326540664030a07e32ecd3f2ed6d"}},"metadata":{}},{"name":"stdout","text":"Current_step 19511 | BatchLoss 0.049 | Learning rate 0.0002050 | GradientNorm 2.109\nCurrent_step 19711 | BatchLoss 0.162 | Learning rate 0.0002025 | GradientNorm 7.564\nCurrent_step 19911 | BatchLoss 0.156 | Learning rate 0.0002000 | GradientNorm 5.697\nCurrent_step 20111 | BatchLoss 0.053 | Learning rate 0.0001976 | GradientNorm 2.113\nCurrent_step 20311 | BatchLoss 0.217 | Learning rate 0.0001951 | GradientNorm 6.852\nCurrent_step 20511 | BatchLoss 0.048 | Learning rate 0.0001927 | GradientNorm 2.056\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eaf839057cd449ea0fcedcec3e28a3f"}},"metadata":{}},{"name":"stdout","text":"Epoch [16], ValLoss [13.167], TrainLoss [0.108]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd6effe1c91448d6bd7348eae9fa869e"}},"metadata":{}},{"name":"stdout","text":"Current_step 20718 | BatchLoss 0.012 | Learning rate 0.0001901 | GradientNorm 0.713\nCurrent_step 20918 | BatchLoss 0.040 | Learning rate 0.0001877 | GradientNorm 1.620\nCurrent_step 21118 | BatchLoss 0.273 | Learning rate 0.0001852 | GradientNorm 4.300\nCurrent_step 21318 | BatchLoss 0.131 | Learning rate 0.0001828 | GradientNorm 4.756\nCurrent_step 21518 | BatchLoss 0.099 | Learning rate 0.0001803 | GradientNorm 4.060\nCurrent_step 21718 | BatchLoss 0.093 | Learning rate 0.0001779 | GradientNorm 6.609\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4212e001760549889c70a0fee776d92d"}},"metadata":{}},{"name":"stdout","text":"Epoch [17], ValLoss [13.596], TrainLoss [0.095]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b8efd9990da444ba5b6cc5b6746b533"}},"metadata":{}},{"name":"stdout","text":"Current_step 21925 | BatchLoss 0.055 | Learning rate 0.0001753 | GradientNorm 3.064\nCurrent_step 22125 | BatchLoss 0.105 | Learning rate 0.0001729 | GradientNorm 8.343\nCurrent_step 22325 | BatchLoss 0.175 | Learning rate 0.0001704 | GradientNorm 7.948\nCurrent_step 22525 | BatchLoss 0.025 | Learning rate 0.0001680 | GradientNorm 2.050\nCurrent_step 22725 | BatchLoss 0.028 | Learning rate 0.0001655 | GradientNorm 2.166\nCurrent_step 22925 | BatchLoss 0.037 | Learning rate 0.0001630 | GradientNorm 2.838\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42fc66447f945399d565b5917ab36ef"}},"metadata":{}},{"name":"stdout","text":"Epoch [18], ValLoss [13.947], TrainLoss [0.075]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742011f623af473a8e478a344935c949"}},"metadata":{}},{"name":"stdout","text":"Current_step 23132 | BatchLoss 0.026 | Learning rate 0.0001605 | GradientNorm 1.696\nCurrent_step 23332 | BatchLoss 0.010 | Learning rate 0.0001581 | GradientNorm 0.371\nCurrent_step 23532 | BatchLoss 0.005 | Learning rate 0.0001556 | GradientNorm 0.527\nCurrent_step 23732 | BatchLoss 0.096 | Learning rate 0.0001531 | GradientNorm 7.284\nCurrent_step 23932 | BatchLoss 0.019 | Learning rate 0.0001507 | GradientNorm 1.291\nCurrent_step 24132 | BatchLoss 0.013 | Learning rate 0.0001482 | GradientNorm 0.591\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff3ae43fd0914e0286fb33c32268faa1"}},"metadata":{}},{"name":"stdout","text":"Epoch [19], ValLoss [14.240], TrainLoss [0.067]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23c4606c2b9c4c18ad2a57b9c02deefd"}},"metadata":{}},{"name":"stdout","text":"Current_step 24339 | BatchLoss 0.008 | Learning rate 0.0001457 | GradientNorm 0.514\nCurrent_step 24539 | BatchLoss 0.115 | Learning rate 0.0001432 | GradientNorm 7.555\nCurrent_step 24739 | BatchLoss 0.013 | Learning rate 0.0001408 | GradientNorm 1.291\nCurrent_step 24939 | BatchLoss 0.006 | Learning rate 0.0001383 | GradientNorm 0.411\nCurrent_step 25139 | BatchLoss 0.016 | Learning rate 0.0001359 | GradientNorm 0.854\nCurrent_step 25339 | BatchLoss 0.051 | Learning rate 0.0001334 | GradientNorm 3.114\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87fb32fe69c1417ca4b12c57ed5f8df9"}},"metadata":{}},{"name":"stdout","text":"Epoch [20], ValLoss [14.525], TrainLoss [0.055]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fa2b29015cc48119e40bfd69151b0ef"}},"metadata":{}},{"name":"stdout","text":"Current_step 25546 | BatchLoss 0.221 | Learning rate 0.0001309 | GradientNorm 5.527\nCurrent_step 25746 | BatchLoss 0.141 | Learning rate 0.0001284 | GradientNorm 6.820\nCurrent_step 25946 | BatchLoss 0.009 | Learning rate 0.0001260 | GradientNorm 0.829\nCurrent_step 26146 | BatchLoss 0.265 | Learning rate 0.0001235 | GradientNorm 5.702\nCurrent_step 26346 | BatchLoss 0.013 | Learning rate 0.0001211 | GradientNorm 0.752\nCurrent_step 26546 | BatchLoss 0.119 | Learning rate 0.0001186 | GradientNorm 5.919\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bfd24f2af4043ea9192a3bdc04ce04b"}},"metadata":{}},{"name":"stdout","text":"Epoch [21], ValLoss [14.744], TrainLoss [0.047]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d84ad7c0f90d47bf881fd2b2f8216506"}},"metadata":{}},{"name":"stdout","text":"Current_step 26753 | BatchLoss 0.218 | Learning rate 0.0001161 | GradientNorm 7.049\nCurrent_step 26953 | BatchLoss 0.015 | Learning rate 0.0001136 | GradientNorm 1.867\nCurrent_step 27153 | BatchLoss 0.043 | Learning rate 0.0001112 | GradientNorm 3.217\nCurrent_step 27353 | BatchLoss 0.030 | Learning rate 0.0001087 | GradientNorm 2.230\nCurrent_step 27553 | BatchLoss 0.004 | Learning rate 0.0001062 | GradientNorm 0.383\nCurrent_step 27753 | BatchLoss 0.008 | Learning rate 0.0001038 | GradientNorm 0.563\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c00669981076422aa3603dfc578c35a9"}},"metadata":{}},{"name":"stdout","text":"Epoch [22], ValLoss [15.034], TrainLoss [0.040]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"027602b406a840c386116a7d8924975c"}},"metadata":{}},{"name":"stdout","text":"Current_step 27960 | BatchLoss 0.039 | Learning rate 0.0001012 | GradientNorm 3.817\nCurrent_step 28160 | BatchLoss 0.069 | Learning rate 0.0000988 | GradientNorm 5.254\nCurrent_step 28360 | BatchLoss 0.007 | Learning rate 0.0000963 | GradientNorm 0.568\nCurrent_step 28560 | BatchLoss 0.016 | Learning rate 0.0000939 | GradientNorm 1.367\nCurrent_step 28760 | BatchLoss 0.010 | Learning rate 0.0000914 | GradientNorm 0.935\nCurrent_step 28960 | BatchLoss 0.011 | Learning rate 0.0000890 | GradientNorm 0.961\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa2a1cd34394888b02e9d84c93fbd90"}},"metadata":{}},{"name":"stdout","text":"Epoch [23], ValLoss [15.176], TrainLoss [0.031]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60a0e356a69548fc889edc64219edba8"}},"metadata":{}},{"name":"stdout","text":"Current_step 29167 | BatchLoss 0.029 | Learning rate 0.0000864 | GradientNorm 2.827\nCurrent_step 29367 | BatchLoss 0.020 | Learning rate 0.0000840 | GradientNorm 3.011\nCurrent_step 29567 | BatchLoss 0.019 | Learning rate 0.0000815 | GradientNorm 1.769\nCurrent_step 29767 | BatchLoss 0.005 | Learning rate 0.0000791 | GradientNorm 0.721\nCurrent_step 29967 | BatchLoss 0.008 | Learning rate 0.0000766 | GradientNorm 0.848\nCurrent_step 30167 | BatchLoss 0.001 | Learning rate 0.0000742 | GradientNorm 0.086\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7beb1ad5b504866872b5de7547a79d1"}},"metadata":{}},{"name":"stdout","text":"Epoch [24], ValLoss [15.400], TrainLoss [0.028]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47a4a83e216c418bae566692cd040595"}},"metadata":{}},{"name":"stdout","text":"Current_step 30374 | BatchLoss 0.002 | Learning rate 0.0000716 | GradientNorm 0.190\nCurrent_step 30574 | BatchLoss 0.073 | Learning rate 0.0000692 | GradientNorm 4.752\nCurrent_step 30774 | BatchLoss 0.005 | Learning rate 0.0000667 | GradientNorm 0.504\nCurrent_step 30974 | BatchLoss 0.004 | Learning rate 0.0000643 | GradientNorm 0.233\nCurrent_step 31174 | BatchLoss 0.048 | Learning rate 0.0000618 | GradientNorm 3.154\nCurrent_step 31374 | BatchLoss 0.035 | Learning rate 0.0000593 | GradientNorm 4.039\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce54d687a0a14af58d442e456b879a1c"}},"metadata":{}},{"name":"stdout","text":"Epoch [25], ValLoss [15.552], TrainLoss [0.023]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cca242cc77a440ca45e0bfcf4335612"}},"metadata":{}},{"name":"stdout","text":"Current_step 31581 | BatchLoss 0.010 | Learning rate 0.0000568 | GradientNorm 0.730\nCurrent_step 31781 | BatchLoss 0.011 | Learning rate 0.0000543 | GradientNorm 1.207\nCurrent_step 31981 | BatchLoss 0.001 | Learning rate 0.0000519 | GradientNorm 0.020\nCurrent_step 32181 | BatchLoss 0.002 | Learning rate 0.0000494 | GradientNorm 0.160\nCurrent_step 32381 | BatchLoss 0.017 | Learning rate 0.0000470 | GradientNorm 2.273\nCurrent_step 32581 | BatchLoss 0.003 | Learning rate 0.0000445 | GradientNorm 0.171\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e557e708e64413ba53c56e07c7e370a"}},"metadata":{}},{"name":"stdout","text":"Epoch [26], ValLoss [15.644], TrainLoss [0.021]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dca65a6604e466c8302be28bcf87c67"}},"metadata":{}},{"name":"stdout","text":"Current_step 32788 | BatchLoss 0.002 | Learning rate 0.0000420 | GradientNorm 0.168\nCurrent_step 32988 | BatchLoss 0.036 | Learning rate 0.0000395 | GradientNorm 3.202\nCurrent_step 33188 | BatchLoss 0.026 | Learning rate 0.0000371 | GradientNorm 2.004\nCurrent_step 33388 | BatchLoss 0.006 | Learning rate 0.0000346 | GradientNorm 0.456\nCurrent_step 33588 | BatchLoss 0.004 | Learning rate 0.0000322 | GradientNorm 0.396\nCurrent_step 33788 | BatchLoss 0.005 | Learning rate 0.0000297 | GradientNorm 0.372\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2ee5cce6f043f9a3acd458ee403157"}},"metadata":{}},{"name":"stdout","text":"Epoch [27], ValLoss [15.770], TrainLoss [0.018]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecfea58ef3e84cf5867f731a1bc5e893"}},"metadata":{}},{"name":"stdout","text":"Current_step 33995 | BatchLoss 0.062 | Learning rate 0.0000272 | GradientNorm 4.477\nCurrent_step 34195 | BatchLoss 0.005 | Learning rate 0.0000247 | GradientNorm 0.364\nCurrent_step 34395 | BatchLoss 0.033 | Learning rate 0.0000223 | GradientNorm 2.305\nCurrent_step 34595 | BatchLoss 0.005 | Learning rate 0.0000198 | GradientNorm 0.431\nCurrent_step 34795 | BatchLoss 0.004 | Learning rate 0.0000174 | GradientNorm 0.478\nCurrent_step 34995 | BatchLoss 0.001 | Learning rate 0.0000149 | GradientNorm 0.075\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b2a9da7e08a4a689f817ebe9d099390"}},"metadata":{}},{"name":"stdout","text":"Epoch [28], ValLoss [15.794], TrainLoss [0.014]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f857119edd2741819fc91f6cc530f81f"}},"metadata":{}},{"name":"stdout","text":"Current_step 35202 | BatchLoss 0.002 | Learning rate 0.0000124 | GradientNorm 0.253\nCurrent_step 35402 | BatchLoss 0.009 | Learning rate 0.0000099 | GradientNorm 0.891\nCurrent_step 35602 | BatchLoss 0.002 | Learning rate 0.0000075 | GradientNorm 0.108\nCurrent_step 35802 | BatchLoss 0.001 | Learning rate 0.0000050 | GradientNorm 0.034\nCurrent_step 36002 | BatchLoss 0.004 | Learning rate 0.0000025 | GradientNorm 0.532\nCurrent_step 36202 | BatchLoss 0.002 | Learning rate 0.0000001 | GradientNorm 0.086\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f353cb13615d43e38130a32d5843cddb"}},"metadata":{}},{"name":"stdout","text":"Epoch [29], ValLoss [15.806], TrainLoss [0.012]\n","output_type":"stream"}],"execution_count":396},{"cell_type":"markdown","source":"Одним из критериев получения полного балла за задание является получение на тесте значения метрики $\\geqslant 0.7$. Скор на тестовой выборке:","metadata":{}},{"cell_type":"code","source":"test_scorer = make_scorer(test['text'].values, test['label'].values, tokenizer, maxlen=MAX_SEQLEN)\n\ntest_scorer(trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T23:58:58.184127Z","iopub.execute_input":"2024-11-21T23:58:58.184407Z","iopub.status.idle":"2024-11-21T23:58:59.100574Z","shell.execute_reply.started":"2024-11-21T23:58:58.184381Z","shell.execute_reply":"2024-11-21T23:58:59.099571Z"}},"outputs":[{"execution_count":397,"output_type":"execute_result","data":{"text/plain":"('f1', 0.7459559264198157)"},"metadata":{}}],"execution_count":397},{"cell_type":"markdown","source":"Не забудьте также приложить вместе со сделанным заданием тензорборд дообучения.","metadata":{}},{"cell_type":"markdown","source":"**Вопросы:**\n1. Попробуйте также обучить модель без предобученных весов (просто закомментировав загрузку весов). Насколько сильно просело качество?\n2. Влияет ли длительность предобучения (количество эпох) как-то существенно на дообучение, или достаточно одной эпохи?\n\n**Ваши ответы напишите здесь:**\n1. Была попытка это сделать, в итоге качество оказалось очень плохим. Модель без предобучения очень плохо понимает данные.\n2. Длительность предобучения не сильно влияет, так как после $1-2$ эпох лосс выходит на плато, и все последующие эпохи уже не дают значимый выигрыш в качестве.","metadata":{}},{"cell_type":"markdown","source":"## Бонусная часть. Большие модели (максимум 3 балла)\n\nПредлагается обучить модель побольше:\n* `hidden_size` $\\in \\{512, 768, 1024\\}$\n* `num_hidden_layers` $\\in \\{8, 12, 24\\}$\n\nНапример, BERT-base архитектура выглядит как `hidden_size=768, num_hidden_layers=12`.\n\nДля большой модели придется также использовать другие гиперпараметры - нужен learning rate поменьше, weight decay побольше, дропаут больше. Возможно потребуется больше эпох предобучения.\n\nЗа выполнение этой части можно получить **до пяти бонусных баллов**, бонус зависит от полученных на тесте значений метрики (должно быть видно существенное улучшение).","metadata":{}},{"cell_type":"markdown","source":"### Предобучение","metadata":{}},{"cell_type":"code","source":"model_bert_base = BertModel(\n    hidden_size=768, \n    vocab_size=30000,\n    max_seqlen=MAX_SEQLEN,\n    num_hidden_layers=12,\n    intermediate_size=(4 * 768),\n    num_attention_heads=(768 // 64),\n    act_func='gelu',\n    input_dropout_prob=0.1,\n    hidden_dropout_prob=0.1, \n    attention_probs_dropout_prob=0.1,\n    eps=1e-3, \n    ignore_index=-100\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:09:56.462000Z","iopub.execute_input":"2024-11-22T00:09:56.462706Z","iopub.status.idle":"2024-11-22T00:09:57.483537Z","shell.execute_reply.started":"2024-11-22T00:09:56.462671Z","shell.execute_reply":"2024-11-22T00:09:57.482556Z"}},"outputs":[],"execution_count":406},{"cell_type":"code","source":"count = 0\nfor param in model_bert_base.parameters():\n    count += param.numel()\nprint(count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:09:57.485088Z","iopub.execute_input":"2024-11-22T00:09:57.485409Z","iopub.status.idle":"2024-11-22T00:09:57.491158Z","shell.execute_reply.started":"2024-11-22T00:09:57.485366Z","shell.execute_reply":"2024-11-22T00:09:57.490253Z"}},"outputs":[{"name":"stdout","text":"109331761\n","output_type":"stream"}],"execution_count":407},{"cell_type":"code","source":"count_parameters(model_bert_base)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:09:57.492506Z","iopub.execute_input":"2024-11-22T00:09:57.492867Z","iopub.status.idle":"2024-11-22T00:09:57.505296Z","shell.execute_reply.started":"2024-11-22T00:09:57.492820Z","shell.execute_reply":"2024-11-22T00:09:57.504461Z"}},"outputs":[{"name":"stdout","text":"Layer: _classifier_head._sop.3.bias | Parameters: 1\nLayer: _backbone._embeddings.layer_norm.weight | Parameters: 768\nLayer: _backbone._embeddings.layer_norm.bias | Parameters: 768\nLayer: _backbone._bert.0._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.0._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.0._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.0._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.0._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.0._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.1._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.1._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.1._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.1._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.1._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.1._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.2._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.2._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.2._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.2._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.2._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.2._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.3._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.3._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.3._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.3._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.3._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.3._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.4._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.4._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.4._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.4._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.4._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.4._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.5._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.5._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.5._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.5._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.5._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.5._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.6._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.6._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.6._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.6._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.6._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.6._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.7._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.7._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.7._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.7._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.7._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.7._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.8._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.8._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.8._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.8._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.8._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.8._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.9._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.9._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.9._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.9._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.9._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.9._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.10._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.10._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.10._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.10._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.10._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.10._feedforward._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.11._multihead_attention._output_linear.bias | Parameters: 768\nLayer: _backbone._bert.11._multihead_attention._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.11._multihead_attention._layernorm.bias | Parameters: 768\nLayer: _backbone._bert.11._feedforward._feedforward.2.bias | Parameters: 768\nLayer: _backbone._bert.11._feedforward._layernorm.weight | Parameters: 768\nLayer: _backbone._bert.11._feedforward._layernorm.bias | Parameters: 768\nLayer: _mlm_head._mlm.0.bias | Parameters: 768\nLayer: _mlm_head._mlm.2.weight | Parameters: 768\nLayer: _mlm_head._mlm.2.bias | Parameters: 768\nLayer: _classifier_head._sop.0.bias | Parameters: 768\nLayer: _classifier_head._sop.3.weight | Parameters: 768\nLayer: _backbone._embeddings._segment_embeddings.weight | Parameters: 1536\nLayer: _backbone._bert.0._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.1._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.2._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.3._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.4._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.5._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.6._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.7._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.8._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.9._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.10._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.11._multihead_attention._projection_linear.bias | Parameters: 2304\nLayer: _backbone._bert.0._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.1._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.2._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.3._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.4._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.5._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.6._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.7._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.8._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.9._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.10._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._bert.11._feedforward._feedforward.0.bias | Parameters: 3072\nLayer: _backbone._embeddings._pos_embeddings.weight | Parameters: 20736\nLayer: _mlm_head._mlm.3.bias | Parameters: 30000\nLayer: _backbone._bert.0._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.1._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.2._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.3._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.4._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.5._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.6._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.7._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.8._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.9._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.10._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _backbone._bert.11._multihead_attention._output_linear.weight | Parameters: 589824\nLayer: _mlm_head._mlm.0.weight | Parameters: 589824\nLayer: _classifier_head._sop.0.weight | Parameters: 589824\nLayer: _backbone._bert.0._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.1._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.2._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.3._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.4._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.5._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.6._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.7._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.8._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.9._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.10._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.11._multihead_attention._projection_linear.weight | Parameters: 1769472\nLayer: _backbone._bert.0._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.0._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.1._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.1._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.2._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.2._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.3._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.3._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.4._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.4._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.5._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.5._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.6._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.6._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.7._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.7._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.8._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.8._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.9._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.9._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.10._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.10._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._bert.11._feedforward._feedforward.0.weight | Parameters: 2359296\nLayer: _backbone._bert.11._feedforward._feedforward.2.weight | Parameters: 2359296\nLayer: _backbone._embeddings._token_embeddings.weight | Parameters: 23040000\n","output_type":"stream"}],"execution_count":408},{"cell_type":"code","source":"optimizer = get_optimizer(model_bert_base, weight_decay=1e-2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:16:34.923275Z","iopub.execute_input":"2024-11-22T00:16:34.923635Z","iopub.status.idle":"2024-11-22T00:16:34.928985Z","shell.execute_reply.started":"2024-11-22T00:16:34.923609Z","shell.execute_reply":"2024-11-22T00:16:34.927913Z"}},"outputs":[],"execution_count":410},{"cell_type":"code","source":"scheduler = Scheduler(\n    optimizer,\n    init_lr=2e-5,\n    peak_lr=5e-5,\n    final_lr=1e-6,\n    num_warmup_steps=int(0.1 * len(dl) * 3),\n    num_training_steps=len(dl) * 3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:16:35.712372Z","iopub.execute_input":"2024-11-22T00:16:35.712682Z","iopub.status.idle":"2024-11-22T00:16:35.717377Z","shell.execute_reply.started":"2024-11-22T00:16:35.712657Z","shell.execute_reply":"2024-11-22T00:16:35.716248Z"}},"outputs":[],"execution_count":411},{"cell_type":"code","source":"device = torch.device('cuda')\n\ntrainer = Trainer(\n    model_bert_base,\n    optimizer,\n    scheduler,\n    tokenizer.pad_token_id,\n    device,\n    num_accum_steps=8,\n    logdir='bert_base',\n    max_grad_norm=1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:16:37.059019Z","iopub.execute_input":"2024-11-22T00:16:37.059814Z","iopub.status.idle":"2024-11-22T00:16:37.236191Z","shell.execute_reply.started":"2024-11-22T00:16:37.059779Z","shell.execute_reply":"2024-11-22T00:16:37.235378Z"}},"outputs":[],"execution_count":412},{"cell_type":"code","source":"trainer.train(dl, n_epochs=2)\n    \ntorch.save(\n    model_bert_base.state_dict(),\n    'pretrained__bertbase_weights.pt'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:16:38.716191Z","iopub.execute_input":"2024-11-22T00:16:38.716924Z","iopub.status.idle":"2024-11-22T01:14:47.313488Z","shell.execute_reply.started":"2024-11-22T00:16:38.716890Z","shell.execute_reply":"2024-11-22T01:14:47.312561Z"}},"outputs":[{"name":"stdout","text":"STEPS: 21340\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"409af5b75ea141e8985034574298f440"}},"metadata":{}},{"name":"stdout","text":"Current_step 199 | BatchLoss 30.346 | Learning rate 0.0000202 | GradientNorm 29.725\nCurrent_step 399 | BatchLoss 24.607 | Learning rate 0.0000205 | GradientNorm 17.487\nCurrent_step 599 | BatchLoss 19.210 | Learning rate 0.0000207 | GradientNorm 13.472\nCurrent_step 799 | BatchLoss 21.095 | Learning rate 0.0000209 | GradientNorm 11.376\nCurrent_step 999 | BatchLoss 16.719 | Learning rate 0.0000212 | GradientNorm 9.125\nCurrent_step 1199 | BatchLoss 16.434 | Learning rate 0.0000214 | GradientNorm 8.356\nCurrent_step 1399 | BatchLoss 15.545 | Learning rate 0.0000216 | GradientNorm 7.492\nCurrent_step 1599 | BatchLoss 15.239 | Learning rate 0.0000219 | GradientNorm 7.387\nCurrent_step 1799 | BatchLoss 15.755 | Learning rate 0.0000221 | GradientNorm 6.648\nCurrent_step 1999 | BatchLoss 15.574 | Learning rate 0.0000223 | GradientNorm 6.949\nCurrent_step 2199 | BatchLoss 15.624 | Learning rate 0.0000226 | GradientNorm 7.083\nCurrent_step 2399 | BatchLoss 14.037 | Learning rate 0.0000228 | GradientNorm 6.514\nCurrent_step 2599 | BatchLoss 13.245 | Learning rate 0.0000230 | GradientNorm 5.873\nCurrent_step 2799 | BatchLoss 12.371 | Learning rate 0.0000233 | GradientNorm 5.596\nCurrent_step 2999 | BatchLoss 12.842 | Learning rate 0.0000235 | GradientNorm 5.780\nCurrent_step 3199 | BatchLoss 15.336 | Learning rate 0.0000237 | GradientNorm 5.152\nCurrent_step 3399 | BatchLoss 13.391 | Learning rate 0.0000240 | GradientNorm 5.609\nCurrent_step 3599 | BatchLoss 12.101 | Learning rate 0.0000242 | GradientNorm 4.735\nCurrent_step 3799 | BatchLoss 12.556 | Learning rate 0.0000245 | GradientNorm 7.175\nCurrent_step 3999 | BatchLoss 14.303 | Learning rate 0.0000247 | GradientNorm 4.866\nCurrent_step 4199 | BatchLoss 12.464 | Learning rate 0.0000249 | GradientNorm 5.136\nCurrent_step 4399 | BatchLoss 11.804 | Learning rate 0.0000252 | GradientNorm 4.851\nCurrent_step 4599 | BatchLoss 10.781 | Learning rate 0.0000254 | GradientNorm 4.135\nCurrent_step 4799 | BatchLoss 11.041 | Learning rate 0.0000256 | GradientNorm 5.312\nCurrent_step 4999 | BatchLoss 10.706 | Learning rate 0.0000259 | GradientNorm 3.535\nCurrent_step 5199 | BatchLoss 11.526 | Learning rate 0.0000261 | GradientNorm 5.437\nCurrent_step 5399 | BatchLoss 11.826 | Learning rate 0.0000263 | GradientNorm 6.242\nCurrent_step 5599 | BatchLoss 10.872 | Learning rate 0.0000266 | GradientNorm 4.277\nCurrent_step 5799 | BatchLoss 10.543 | Learning rate 0.0000268 | GradientNorm 5.514\nCurrent_step 5999 | BatchLoss 10.540 | Learning rate 0.0000270 | GradientNorm 4.840\nCurrent_step 6199 | BatchLoss 10.278 | Learning rate 0.0000273 | GradientNorm 5.626\nCurrent_step 6399 | BatchLoss 10.039 | Learning rate 0.0000275 | GradientNorm 4.665\nCurrent_step 6599 | BatchLoss 9.858 | Learning rate 0.0000277 | GradientNorm 4.479\nCurrent_step 6799 | BatchLoss 10.782 | Learning rate 0.0000280 | GradientNorm 3.965\nCurrent_step 6999 | BatchLoss 10.098 | Learning rate 0.0000282 | GradientNorm 3.885\nCurrent_step 7199 | BatchLoss 10.642 | Learning rate 0.0000284 | GradientNorm 4.031\nCurrent_step 7399 | BatchLoss 9.055 | Learning rate 0.0000287 | GradientNorm 4.655\nCurrent_step 7599 | BatchLoss 9.371 | Learning rate 0.0000289 | GradientNorm 4.007\nCurrent_step 7799 | BatchLoss 9.444 | Learning rate 0.0000291 | GradientNorm 3.393\nCurrent_step 7999 | BatchLoss 8.516 | Learning rate 0.0000294 | GradientNorm 5.260\nCurrent_step 8199 | BatchLoss 8.666 | Learning rate 0.0000296 | GradientNorm 4.021\nCurrent_step 8399 | BatchLoss 8.788 | Learning rate 0.0000298 | GradientNorm 3.570\nCurrent_step 8599 | BatchLoss 9.055 | Learning rate 0.0000301 | GradientNorm 3.336\nCurrent_step 8799 | BatchLoss 9.020 | Learning rate 0.0000303 | GradientNorm 7.257\nCurrent_step 8999 | BatchLoss 8.620 | Learning rate 0.0000305 | GradientNorm 3.006\nCurrent_step 9199 | BatchLoss 8.970 | Learning rate 0.0000308 | GradientNorm 3.164\nCurrent_step 9399 | BatchLoss 8.185 | Learning rate 0.0000310 | GradientNorm 3.485\nCurrent_step 9599 | BatchLoss 8.313 | Learning rate 0.0000312 | GradientNorm 3.520\nCurrent_step 9799 | BatchLoss 8.042 | Learning rate 0.0000315 | GradientNorm 4.119\nCurrent_step 9999 | BatchLoss 8.141 | Learning rate 0.0000317 | GradientNorm nan\nCurrent_step 10199 | BatchLoss 8.390 | Learning rate 0.0000319 | GradientNorm 2.428\nCurrent_step 10399 | BatchLoss 8.938 | Learning rate 0.0000322 | GradientNorm 4.347\nCurrent_step 10599 | BatchLoss 8.507 | Learning rate 0.0000324 | GradientNorm 4.256\n\nTRAIN TIME: 1726.1109\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8756a9d82eb4a1b99bd067ff2bca7f7"}},"metadata":{}},{"name":"stdout","text":"Current_step 10869 | BatchLoss 6.845 | Learning rate 0.0000327 | GradientNorm 4.033\nCurrent_step 11069 | BatchLoss 7.901 | Learning rate 0.0000330 | GradientNorm 2.861\nCurrent_step 11269 | BatchLoss 7.011 | Learning rate 0.0000332 | GradientNorm 3.309\nCurrent_step 11469 | BatchLoss 7.514 | Learning rate 0.0000334 | GradientNorm 3.319\nCurrent_step 11669 | BatchLoss 7.170 | Learning rate 0.0000337 | GradientNorm 3.293\nCurrent_step 11869 | BatchLoss 7.133 | Learning rate 0.0000339 | GradientNorm 3.340\nCurrent_step 12069 | BatchLoss 7.647 | Learning rate 0.0000341 | GradientNorm 2.987\nCurrent_step 12269 | BatchLoss 6.350 | Learning rate 0.0000344 | GradientNorm 2.968\nCurrent_step 12469 | BatchLoss 7.492 | Learning rate 0.0000346 | GradientNorm 3.037\nCurrent_step 12669 | BatchLoss 6.323 | Learning rate 0.0000348 | GradientNorm 3.134\nCurrent_step 12869 | BatchLoss 7.638 | Learning rate 0.0000351 | GradientNorm 2.487\nCurrent_step 13069 | BatchLoss 7.137 | Learning rate 0.0000353 | GradientNorm 3.125\nCurrent_step 13269 | BatchLoss 7.615 | Learning rate 0.0000355 | GradientNorm 2.707\nCurrent_step 13469 | BatchLoss 7.008 | Learning rate 0.0000358 | GradientNorm 2.691\nCurrent_step 13669 | BatchLoss 6.950 | Learning rate 0.0000360 | GradientNorm 3.323\nCurrent_step 13869 | BatchLoss 7.729 | Learning rate 0.0000363 | GradientNorm 2.868\nCurrent_step 14069 | BatchLoss 7.106 | Learning rate 0.0000365 | GradientNorm 3.445\nCurrent_step 14269 | BatchLoss 6.947 | Learning rate 0.0000367 | GradientNorm 2.843\nCurrent_step 14469 | BatchLoss 6.848 | Learning rate 0.0000370 | GradientNorm 3.345\nCurrent_step 14669 | BatchLoss 7.609 | Learning rate 0.0000372 | GradientNorm 3.186\nCurrent_step 14869 | BatchLoss 7.067 | Learning rate 0.0000374 | GradientNorm 3.043\nCurrent_step 15069 | BatchLoss 7.101 | Learning rate 0.0000377 | GradientNorm 3.385\nCurrent_step 15269 | BatchLoss 6.927 | Learning rate 0.0000379 | GradientNorm 3.904\nCurrent_step 15469 | BatchLoss 7.826 | Learning rate 0.0000381 | GradientNorm 4.433\nCurrent_step 15669 | BatchLoss 7.768 | Learning rate 0.0000384 | GradientNorm 4.141\nCurrent_step 15869 | BatchLoss 7.704 | Learning rate 0.0000386 | GradientNorm 3.950\nCurrent_step 16069 | BatchLoss 7.845 | Learning rate 0.0000388 | GradientNorm 4.651\nCurrent_step 16269 | BatchLoss 7.224 | Learning rate 0.0000391 | GradientNorm 4.890\nCurrent_step 16469 | BatchLoss 7.973 | Learning rate 0.0000393 | GradientNorm 6.985\nCurrent_step 16669 | BatchLoss 7.869 | Learning rate 0.0000395 | GradientNorm 5.309\nCurrent_step 16869 | BatchLoss 8.185 | Learning rate 0.0000398 | GradientNorm 4.300\nCurrent_step 17069 | BatchLoss 9.042 | Learning rate 0.0000400 | GradientNorm 4.329\nCurrent_step 17269 | BatchLoss 7.635 | Learning rate 0.0000402 | GradientNorm 5.525\nCurrent_step 17469 | BatchLoss 9.029 | Learning rate 0.0000405 | GradientNorm 9.288\nCurrent_step 17669 | BatchLoss 8.529 | Learning rate 0.0000407 | GradientNorm 8.385\nCurrent_step 17869 | BatchLoss 8.819 | Learning rate 0.0000409 | GradientNorm 11.330\nCurrent_step 18069 | BatchLoss 8.718 | Learning rate 0.0000412 | GradientNorm 18.774\nCurrent_step 18269 | BatchLoss 9.005 | Learning rate 0.0000414 | GradientNorm 9.634\nCurrent_step 18469 | BatchLoss 10.187 | Learning rate 0.0000416 | GradientNorm 7.091\nCurrent_step 18669 | BatchLoss 10.611 | Learning rate 0.0000419 | GradientNorm 10.708\nCurrent_step 18869 | BatchLoss 11.679 | Learning rate 0.0000421 | GradientNorm 42.183\nCurrent_step 19069 | BatchLoss 14.207 | Learning rate 0.0000423 | GradientNorm 11.832\nCurrent_step 19269 | BatchLoss 17.991 | Learning rate 0.0000426 | GradientNorm 12.478\nCurrent_step 19469 | BatchLoss 20.374 | Learning rate 0.0000428 | GradientNorm 0.000\nCurrent_step 19669 | BatchLoss 18.296 | Learning rate 0.0000430 | GradientNorm 0.000\nCurrent_step 19869 | BatchLoss 19.952 | Learning rate 0.0000433 | GradientNorm 0.000\nCurrent_step 20069 | BatchLoss 18.987 | Learning rate 0.0000435 | GradientNorm 0.000\nCurrent_step 20269 | BatchLoss 19.290 | Learning rate 0.0000437 | GradientNorm 0.000\nCurrent_step 20469 | BatchLoss 20.523 | Learning rate 0.0000440 | GradientNorm 0.000\nCurrent_step 20669 | BatchLoss 20.162 | Learning rate 0.0000442 | GradientNorm nan\nCurrent_step 20869 | BatchLoss 20.524 | Learning rate 0.0000445 | GradientNorm 0.000\nCurrent_step 21069 | BatchLoss 20.812 | Learning rate 0.0000447 | GradientNorm 0.000\nCurrent_step 21269 | BatchLoss 21.409 | Learning rate 0.0000449 | GradientNorm 0.000\n\nTRAIN TIME: 1761.8046\n\n","output_type":"stream"}],"execution_count":413},{"cell_type":"markdown","source":"### Файн-тьюнинг","metadata":{}},{"cell_type":"code","source":"bert_base_finetune_model = BertFinetuneModel(\n    hidden_size=768, \n    vocab_size=30000,\n    max_seqlen=MAX_SEQLEN,\n    num_hidden_layers=4,\n    intermediate_size=(4 * 768),\n    num_attention_heads=(768 // 64),\n    num_classes=96,\n    act_func='gelu',\n    input_dropout_prob=0.1,\n    hidden_dropout_prob=0.1, \n    attention_probs_dropout_prob=0.1,\n    eps=1e-3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T01:14:47.315661Z","iopub.execute_input":"2024-11-22T01:14:47.316601Z","iopub.status.idle":"2024-11-22T01:14:47.768952Z","shell.execute_reply.started":"2024-11-22T01:14:47.316552Z","shell.execute_reply":"2024-11-22T01:14:47.767925Z"}},"outputs":[],"execution_count":414},{"cell_type":"code","source":"found = load_weights(bert_base_finetune_model, 'pretrained__bertbase_weights.pt')\n\nprint('Amount of found weights: {}'.format(len(found)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T01:14:47.769954Z","iopub.execute_input":"2024-11-22T01:14:47.770207Z","iopub.status.idle":"2024-11-22T01:14:48.228976Z","shell.execute_reply.started":"2024-11-22T01:14:47.770183Z","shell.execute_reply":"2024-11-22T01:14:48.227895Z"}},"outputs":[{"name":"stdout","text":"Amount of found weights: 55\n","output_type":"stream"}],"execution_count":415},{"cell_type":"code","source":"optimizer = get_optimizer(bert_base_finetune_model, weight_decay=1e-2)\nscheduler = Scheduler(\n    optimizer,\n    init_lr=2e-5,\n    peak_lr=5e-5,\n    final_lr=1e-6,\n    num_warmup_steps=int(0.1 * len(dataloaders['train']) * 30),\n    num_training_steps=len(dataloaders['train']) * 30\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T01:14:48.230819Z","iopub.execute_input":"2024-11-22T01:14:48.231122Z","iopub.status.idle":"2024-11-22T01:14:48.236913Z","shell.execute_reply.started":"2024-11-22T01:14:48.231094Z","shell.execute_reply":"2024-11-22T01:14:48.235870Z"}},"outputs":[],"execution_count":416},{"cell_type":"code","source":"device = torch.device('cuda')\n\ntrainer = FinetuneTrainer(\n    bert_base_finetune_model,\n    optimizer,\n    scheduler,\n    tokenizer.pad_token_id,\n    device,\n    logdir='finetuned_bert_base',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T01:14:48.237961Z","iopub.execute_input":"2024-11-22T01:14:48.238267Z","iopub.status.idle":"2024-11-22T01:14:48.312158Z","shell.execute_reply.started":"2024-11-22T01:14:48.238223Z","shell.execute_reply":"2024-11-22T01:14:48.311412Z"}},"outputs":[],"execution_count":417},{"cell_type":"code","source":"val_scorer(trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T01:14:48.313256Z","iopub.execute_input":"2024-11-22T01:14:48.313546Z","iopub.status.idle":"2024-11-22T01:14:50.610209Z","shell.execute_reply.started":"2024-11-22T01:14:48.313518Z","shell.execute_reply":"2024-11-22T01:14:50.609217Z"}},"outputs":[{"execution_count":418,"output_type":"execute_result","data":{"text/plain":"('f1', 0.0007267183627712797)"},"metadata":{}}],"execution_count":418},{"cell_type":"code","source":"trainer.train(dataloaders, n_epochs=35, scorer=val_scorer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T01:14:50.611493Z","iopub.execute_input":"2024-11-22T01:14:50.611817Z","iopub.status.idle":"2024-11-22T01:39:38.707118Z","shell.execute_reply.started":"2024-11-22T01:14:50.611786Z","shell.execute_reply":"2024-11-22T01:39:38.706202Z"}},"outputs":[{"name":"stdout","text":"TrainSteps: 42245\nEvalSteps: 5285\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27682e26a18b4f0591cec5631c9cbd0b"}},"metadata":{}},{"name":"stdout","text":"Current_step 199 | BatchLoss 3.673 | Learning rate 0.0000217 | GradientNorm 3.610\nCurrent_step 399 | BatchLoss 3.376 | Learning rate 0.0000233 | GradientNorm 4.528\nCurrent_step 599 | BatchLoss 2.794 | Learning rate 0.0000250 | GradientNorm 5.625\nCurrent_step 799 | BatchLoss 2.422 | Learning rate 0.0000266 | GradientNorm 5.432\nCurrent_step 999 | BatchLoss 3.253 | Learning rate 0.0000283 | GradientNorm 6.224\nCurrent_step 1199 | BatchLoss 2.516 | Learning rate 0.0000299 | GradientNorm 5.870\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1bf7520fb2d4865b6f19e8c0c4611f8"}},"metadata":{}},{"name":"stdout","text":"Epoch [0], ValLoss [4.899], TrainLoss [2.968]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b753b84b498f46d4a80758ffb9bb2256"}},"metadata":{}},{"name":"stdout","text":"Current_step 1406 | BatchLoss 2.147 | Learning rate 0.0000317 | GradientNorm 4.925\nCurrent_step 1606 | BatchLoss 1.997 | Learning rate 0.0000333 | GradientNorm 7.916\nCurrent_step 1806 | BatchLoss 2.324 | Learning rate 0.0000350 | GradientNorm 7.272\nCurrent_step 2006 | BatchLoss 2.179 | Learning rate 0.0000366 | GradientNorm 8.282\nCurrent_step 2206 | BatchLoss 1.999 | Learning rate 0.0000383 | GradientNorm 7.494\nCurrent_step 2406 | BatchLoss 1.996 | Learning rate 0.0000399 | GradientNorm 6.897\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"402fdaf5d0854352870c143b04df9af4"}},"metadata":{}},{"name":"stdout","text":"Epoch [1], ValLoss [5.627], TrainLoss [2.160]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"346847d24dcc487f94fcc73dbd26c993"}},"metadata":{}},{"name":"stdout","text":"Current_step 2613 | BatchLoss 2.128 | Learning rate 0.0000417 | GradientNorm 10.090\nCurrent_step 2813 | BatchLoss 2.481 | Learning rate 0.0000433 | GradientNorm 7.843\nCurrent_step 3013 | BatchLoss 2.260 | Learning rate 0.0000450 | GradientNorm 8.168\nCurrent_step 3213 | BatchLoss 1.693 | Learning rate 0.0000466 | GradientNorm 8.929\nCurrent_step 3413 | BatchLoss 1.879 | Learning rate 0.0000483 | GradientNorm 7.533\nCurrent_step 3613 | BatchLoss 2.255 | Learning rate 0.0000499 | GradientNorm 10.291\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc50fe8065984bc29cf96796cab70ec8"}},"metadata":{}},{"name":"stdout","text":"Epoch [2], ValLoss [6.262], TrainLoss [1.812]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a2277424df48feb8f4463c54cf8ee4"}},"metadata":{}},{"name":"stdout","text":"Current_step 3820 | BatchLoss 2.143 | Learning rate 0.0000497 | GradientNorm 8.426\nCurrent_step 4020 | BatchLoss 1.446 | Learning rate 0.0000494 | GradientNorm 8.428\nCurrent_step 4220 | BatchLoss 1.169 | Learning rate 0.0000491 | GradientNorm 7.870\nCurrent_step 4420 | BatchLoss 1.158 | Learning rate 0.0000488 | GradientNorm 11.490\nCurrent_step 4620 | BatchLoss 1.051 | Learning rate 0.0000485 | GradientNorm 5.879\nCurrent_step 4820 | BatchLoss 1.185 | Learning rate 0.0000482 | GradientNorm 8.975\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35d812db508f44d9aeb9865cd25d4cba"}},"metadata":{}},{"name":"stdout","text":"Epoch [3], ValLoss [6.760], TrainLoss [1.577]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b56e2a3ac0b41fc9f46c547f131d0e3"}},"metadata":{}},{"name":"stdout","text":"Current_step 5027 | BatchLoss 1.201 | Learning rate 0.0000479 | GradientNorm 8.220\nCurrent_step 5227 | BatchLoss 2.204 | Learning rate 0.0000476 | GradientNorm 8.773\nCurrent_step 5427 | BatchLoss 1.695 | Learning rate 0.0000473 | GradientNorm 8.000\nCurrent_step 5627 | BatchLoss 1.297 | Learning rate 0.0000470 | GradientNorm 7.469\nCurrent_step 5827 | BatchLoss 1.114 | Learning rate 0.0000467 | GradientNorm 6.922\nCurrent_step 6027 | BatchLoss 1.362 | Learning rate 0.0000464 | GradientNorm 6.776\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb85b7781f574c1fafda3d36c4781e85"}},"metadata":{}},{"name":"stdout","text":"Epoch [4], ValLoss [7.030], TrainLoss [1.398]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06bf4988cdd49b0a7a29b390a893c62"}},"metadata":{}},{"name":"stdout","text":"Current_step 6234 | BatchLoss 2.022 | Learning rate 0.0000461 | GradientNorm 10.142\nCurrent_step 6434 | BatchLoss 0.666 | Learning rate 0.0000458 | GradientNorm 4.336\nCurrent_step 6634 | BatchLoss 1.068 | Learning rate 0.0000455 | GradientNorm 5.873\nCurrent_step 6834 | BatchLoss 1.241 | Learning rate 0.0000452 | GradientNorm 8.412\nCurrent_step 7034 | BatchLoss 1.480 | Learning rate 0.0000449 | GradientNorm 8.012\nCurrent_step 7234 | BatchLoss 1.180 | Learning rate 0.0000446 | GradientNorm 8.553\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b0a032e955b48ecbe59e4cfed36c8c1"}},"metadata":{}},{"name":"stdout","text":"Epoch [5], ValLoss [7.546], TrainLoss [1.265]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93f41d09c7984b8cadc9b3f3a423dee2"}},"metadata":{}},{"name":"stdout","text":"Current_step 7441 | BatchLoss 0.885 | Learning rate 0.0000443 | GradientNorm 8.919\nCurrent_step 7641 | BatchLoss 1.337 | Learning rate 0.0000440 | GradientNorm 10.683\nCurrent_step 7841 | BatchLoss 1.055 | Learning rate 0.0000437 | GradientNorm 7.815\nCurrent_step 8041 | BatchLoss 1.673 | Learning rate 0.0000434 | GradientNorm 9.339\nCurrent_step 8241 | BatchLoss 0.951 | Learning rate 0.0000431 | GradientNorm 7.871\nCurrent_step 8441 | BatchLoss 1.076 | Learning rate 0.0000428 | GradientNorm 8.501\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cdf6e0f07be44f796c1b6960f81900a"}},"metadata":{}},{"name":"stdout","text":"Epoch [6], ValLoss [7.638], TrainLoss [1.157]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d036e30b6f1b4260a93bab29bbabfc87"}},"metadata":{}},{"name":"stdout","text":"Current_step 8648 | BatchLoss 1.185 | Learning rate 0.0000424 | GradientNorm 9.394\nCurrent_step 8848 | BatchLoss 1.101 | Learning rate 0.0000421 | GradientNorm 10.067\nCurrent_step 9048 | BatchLoss 1.160 | Learning rate 0.0000418 | GradientNorm 8.954\nCurrent_step 9248 | BatchLoss 0.806 | Learning rate 0.0000415 | GradientNorm 8.027\nCurrent_step 9448 | BatchLoss 1.109 | Learning rate 0.0000412 | GradientNorm 8.601\nCurrent_step 9648 | BatchLoss 1.215 | Learning rate 0.0000409 | GradientNorm 7.699\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ca9599c0bbc4196ab9fcd559241a0e5"}},"metadata":{}},{"name":"stdout","text":"Epoch [7], ValLoss [7.953], TrainLoss [1.069]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c09631c15f2743f5aa21738bac1df1f2"}},"metadata":{}},{"name":"stdout","text":"Current_step 9855 | BatchLoss 1.014 | Learning rate 0.0000406 | GradientNorm 7.452\nCurrent_step 10055 | BatchLoss 1.073 | Learning rate 0.0000403 | GradientNorm 10.329\nCurrent_step 10255 | BatchLoss 1.016 | Learning rate 0.0000400 | GradientNorm 8.940\nCurrent_step 10455 | BatchLoss 1.319 | Learning rate 0.0000397 | GradientNorm 10.833\nCurrent_step 10655 | BatchLoss 1.078 | Learning rate 0.0000394 | GradientNorm 10.864\nCurrent_step 10855 | BatchLoss 0.959 | Learning rate 0.0000391 | GradientNorm 8.895\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1b0801d7ba4ac5aa0a395621ca4af2"}},"metadata":{}},{"name":"stdout","text":"Epoch [8], ValLoss [8.064], TrainLoss [0.987]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a08db264accb4b979ba06426be978ec2"}},"metadata":{}},{"name":"stdout","text":"Current_step 11062 | BatchLoss 0.799 | Learning rate 0.0000388 | GradientNorm 8.970\nCurrent_step 11262 | BatchLoss 0.752 | Learning rate 0.0000385 | GradientNorm 8.123\nCurrent_step 11462 | BatchLoss 1.118 | Learning rate 0.0000382 | GradientNorm 9.460\nCurrent_step 11662 | BatchLoss 0.573 | Learning rate 0.0000379 | GradientNorm 7.112\nCurrent_step 11862 | BatchLoss 0.978 | Learning rate 0.0000376 | GradientNorm 10.417\nCurrent_step 12062 | BatchLoss 1.339 | Learning rate 0.0000373 | GradientNorm 12.366\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47390fcca6224666a5717f5e0ed304dd"}},"metadata":{}},{"name":"stdout","text":"Epoch [9], ValLoss [8.438], TrainLoss [0.911]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54294e04e524a39b689e3b6c525541f"}},"metadata":{}},{"name":"stdout","text":"Current_step 12269 | BatchLoss 1.078 | Learning rate 0.0000370 | GradientNorm 11.560\nCurrent_step 12469 | BatchLoss 1.021 | Learning rate 0.0000367 | GradientNorm 10.176\nCurrent_step 12669 | BatchLoss 0.938 | Learning rate 0.0000364 | GradientNorm 12.848\nCurrent_step 12869 | BatchLoss 1.116 | Learning rate 0.0000361 | GradientNorm 10.337\nCurrent_step 13069 | BatchLoss 1.167 | Learning rate 0.0000358 | GradientNorm 10.090\nCurrent_step 13269 | BatchLoss 0.466 | Learning rate 0.0000355 | GradientNorm 6.543\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a5634363b0141e2ad45adbecb8a54b1"}},"metadata":{}},{"name":"stdout","text":"Epoch [10], ValLoss [8.539], TrainLoss [0.844]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"674f2d1ca7da4313a81f1086d870b51e"}},"metadata":{}},{"name":"stdout","text":"Current_step 13476 | BatchLoss 0.918 | Learning rate 0.0000352 | GradientNorm 9.970\nCurrent_step 13676 | BatchLoss 0.661 | Learning rate 0.0000349 | GradientNorm 8.468\nCurrent_step 13876 | BatchLoss 0.767 | Learning rate 0.0000346 | GradientNorm 10.150\nCurrent_step 14076 | BatchLoss 0.515 | Learning rate 0.0000343 | GradientNorm 8.201\nCurrent_step 14276 | BatchLoss 1.516 | Learning rate 0.0000340 | GradientNorm 11.218\nCurrent_step 14476 | BatchLoss 0.534 | Learning rate 0.0000337 | GradientNorm 5.593\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8d67a1c18344665a18bfcbd263840dd"}},"metadata":{}},{"name":"stdout","text":"Epoch [11], ValLoss [8.773], TrainLoss [0.784]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f319c744aeb64eee9a936a098f28e34f"}},"metadata":{}},{"name":"stdout","text":"Current_step 14683 | BatchLoss 0.704 | Learning rate 0.0000334 | GradientNorm 7.994\nCurrent_step 14883 | BatchLoss 0.512 | Learning rate 0.0000331 | GradientNorm 6.952\nCurrent_step 15083 | BatchLoss 0.390 | Learning rate 0.0000328 | GradientNorm 5.640\nCurrent_step 15283 | BatchLoss 0.871 | Learning rate 0.0000325 | GradientNorm 9.326\nCurrent_step 15483 | BatchLoss 0.895 | Learning rate 0.0000322 | GradientNorm 10.105\nCurrent_step 15683 | BatchLoss 0.590 | Learning rate 0.0000319 | GradientNorm 6.409\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3cd42ab45594a2bae59a50b8af4e264"}},"metadata":{}},{"name":"stdout","text":"Epoch [12], ValLoss [9.015], TrainLoss [0.728]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7efe02f09f3a442fadcbab91fe3dbaa4"}},"metadata":{}},{"name":"stdout","text":"Current_step 15890 | BatchLoss 0.616 | Learning rate 0.0000316 | GradientNorm 9.152\nCurrent_step 16090 | BatchLoss 0.931 | Learning rate 0.0000313 | GradientNorm 11.825\nCurrent_step 16290 | BatchLoss 0.543 | Learning rate 0.0000309 | GradientNorm 7.159\nCurrent_step 16490 | BatchLoss 0.900 | Learning rate 0.0000306 | GradientNorm 11.565\nCurrent_step 16690 | BatchLoss 0.827 | Learning rate 0.0000303 | GradientNorm 6.982\nCurrent_step 16890 | BatchLoss 0.846 | Learning rate 0.0000300 | GradientNorm 11.122\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"191bebf2168b49d685828a8a928f85c7"}},"metadata":{}},{"name":"stdout","text":"Epoch [13], ValLoss [9.102], TrainLoss [0.679]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64c80c5491f94044967a380a83217fbf"}},"metadata":{}},{"name":"stdout","text":"Current_step 17097 | BatchLoss 0.862 | Learning rate 0.0000297 | GradientNorm 11.096\nCurrent_step 17297 | BatchLoss 0.394 | Learning rate 0.0000294 | GradientNorm 7.647\nCurrent_step 17497 | BatchLoss 0.503 | Learning rate 0.0000291 | GradientNorm 8.312\nCurrent_step 17697 | BatchLoss 0.365 | Learning rate 0.0000288 | GradientNorm 9.607\nCurrent_step 17897 | BatchLoss 1.069 | Learning rate 0.0000285 | GradientNorm 10.779\nCurrent_step 18097 | BatchLoss 0.433 | Learning rate 0.0000282 | GradientNorm 6.978\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef00766a0cd5467c82a6411649d76904"}},"metadata":{}},{"name":"stdout","text":"Epoch [14], ValLoss [9.290], TrainLoss [0.634]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90af7439e4a44960b6a6486d8a0c22dd"}},"metadata":{}},{"name":"stdout","text":"Current_step 18304 | BatchLoss 0.621 | Learning rate 0.0000279 | GradientNorm 12.123\nCurrent_step 18504 | BatchLoss 0.600 | Learning rate 0.0000276 | GradientNorm 5.948\nCurrent_step 18704 | BatchLoss 0.439 | Learning rate 0.0000273 | GradientNorm 7.546\nCurrent_step 18904 | BatchLoss 0.349 | Learning rate 0.0000270 | GradientNorm 7.066\nCurrent_step 19104 | BatchLoss 0.871 | Learning rate 0.0000267 | GradientNorm 12.785\nCurrent_step 19304 | BatchLoss 0.717 | Learning rate 0.0000264 | GradientNorm 10.911\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61e1eb0276c6465db52a8b7b4729ff97"}},"metadata":{}},{"name":"stdout","text":"Epoch [15], ValLoss [9.517], TrainLoss [0.588]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"740d0731eae243a09178f90ee1e64b4b"}},"metadata":{}},{"name":"stdout","text":"Current_step 19511 | BatchLoss 0.415 | Learning rate 0.0000261 | GradientNorm 7.322\nCurrent_step 19711 | BatchLoss 0.614 | Learning rate 0.0000258 | GradientNorm 8.714\nCurrent_step 19911 | BatchLoss 0.387 | Learning rate 0.0000255 | GradientNorm 7.896\nCurrent_step 20111 | BatchLoss 0.289 | Learning rate 0.0000252 | GradientNorm 5.889\nCurrent_step 20311 | BatchLoss 0.441 | Learning rate 0.0000249 | GradientNorm 7.337\nCurrent_step 20511 | BatchLoss 0.729 | Learning rate 0.0000246 | GradientNorm 10.331\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a87ee1d532974245bd22087ec2b5b94e"}},"metadata":{}},{"name":"stdout","text":"Epoch [16], ValLoss [9.644], TrainLoss [0.554]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee56f7540fce45ee81d2ce62d718995f"}},"metadata":{}},{"name":"stdout","text":"Current_step 20718 | BatchLoss 0.716 | Learning rate 0.0000243 | GradientNorm 11.309\nCurrent_step 20918 | BatchLoss 0.550 | Learning rate 0.0000240 | GradientNorm 7.949\nCurrent_step 21118 | BatchLoss 0.349 | Learning rate 0.0000237 | GradientNorm 9.433\nCurrent_step 21318 | BatchLoss 0.317 | Learning rate 0.0000234 | GradientNorm 7.149\nCurrent_step 21518 | BatchLoss 0.359 | Learning rate 0.0000231 | GradientNorm 7.563\nCurrent_step 21718 | BatchLoss 0.714 | Learning rate 0.0000228 | GradientNorm 9.130\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba798a0a94c84d3cba53d77528d4ebf5"}},"metadata":{}},{"name":"stdout","text":"Epoch [17], ValLoss [9.797], TrainLoss [0.519]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf65b9da438448088603690615c2c175"}},"metadata":{}},{"name":"stdout","text":"Current_step 21925 | BatchLoss 0.188 | Learning rate 0.0000225 | GradientNorm 5.875\nCurrent_step 22125 | BatchLoss 0.296 | Learning rate 0.0000222 | GradientNorm 5.410\nCurrent_step 22325 | BatchLoss 0.792 | Learning rate 0.0000219 | GradientNorm 8.831\nCurrent_step 22525 | BatchLoss 0.564 | Learning rate 0.0000216 | GradientNorm 10.772\nCurrent_step 22725 | BatchLoss 0.468 | Learning rate 0.0000213 | GradientNorm 10.346\nCurrent_step 22925 | BatchLoss 0.577 | Learning rate 0.0000210 | GradientNorm 8.629\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3768650298104202885f32edbbee67a7"}},"metadata":{}},{"name":"stdout","text":"Epoch [18], ValLoss [9.884], TrainLoss [0.488]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e49408121b640b38318cba00e5f6170"}},"metadata":{}},{"name":"stdout","text":"Current_step 23132 | BatchLoss 0.690 | Learning rate 0.0000207 | GradientNorm 11.938\nCurrent_step 23332 | BatchLoss 0.317 | Learning rate 0.0000204 | GradientNorm 6.746\nCurrent_step 23532 | BatchLoss 0.396 | Learning rate 0.0000201 | GradientNorm 8.328\nCurrent_step 23732 | BatchLoss 0.676 | Learning rate 0.0000198 | GradientNorm 11.242\nCurrent_step 23932 | BatchLoss 0.599 | Learning rate 0.0000195 | GradientNorm 8.635\nCurrent_step 24132 | BatchLoss 0.260 | Learning rate 0.0000192 | GradientNorm 5.653\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d59be70abd34e009c12c9b68b4e65ec"}},"metadata":{}},{"name":"stdout","text":"Epoch [19], ValLoss [10.061], TrainLoss [0.458]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8463da6e4d114688b1341132601a4904"}},"metadata":{}},{"name":"stdout","text":"Current_step 24339 | BatchLoss 0.299 | Learning rate 0.0000188 | GradientNorm 6.220\nCurrent_step 24539 | BatchLoss 0.602 | Learning rate 0.0000185 | GradientNorm 8.904\nCurrent_step 24739 | BatchLoss 0.588 | Learning rate 0.0000182 | GradientNorm 10.864\nCurrent_step 24939 | BatchLoss 0.521 | Learning rate 0.0000179 | GradientNorm 9.621\nCurrent_step 25139 | BatchLoss 0.235 | Learning rate 0.0000176 | GradientNorm 7.504\nCurrent_step 25339 | BatchLoss 0.389 | Learning rate 0.0000173 | GradientNorm 8.105\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e607ac9f2eb4a98b81b6ee806d7e3ab"}},"metadata":{}},{"name":"stdout","text":"Epoch [20], ValLoss [10.194], TrainLoss [0.437]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c756d34cc4e141a395001eeabedcd5cd"}},"metadata":{}},{"name":"stdout","text":"Current_step 25546 | BatchLoss 0.102 | Learning rate 0.0000170 | GradientNorm 3.387\nCurrent_step 25746 | BatchLoss 0.352 | Learning rate 0.0000167 | GradientNorm 8.261\nCurrent_step 25946 | BatchLoss 0.401 | Learning rate 0.0000164 | GradientNorm 8.658\nCurrent_step 26146 | BatchLoss 0.342 | Learning rate 0.0000161 | GradientNorm 8.935\nCurrent_step 26346 | BatchLoss 0.432 | Learning rate 0.0000158 | GradientNorm 8.534\nCurrent_step 26546 | BatchLoss 0.395 | Learning rate 0.0000155 | GradientNorm 12.436\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d037f07ac3e441ef8643b1811a6b374a"}},"metadata":{}},{"name":"stdout","text":"Epoch [21], ValLoss [10.355], TrainLoss [0.409]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08461db0a8d4295ac3b3df3232dc2ee"}},"metadata":{}},{"name":"stdout","text":"Current_step 26753 | BatchLoss 0.547 | Learning rate 0.0000152 | GradientNorm 9.752\nCurrent_step 26953 | BatchLoss 0.417 | Learning rate 0.0000149 | GradientNorm 7.948\nCurrent_step 27153 | BatchLoss 0.481 | Learning rate 0.0000146 | GradientNorm 6.915\nCurrent_step 27353 | BatchLoss 0.465 | Learning rate 0.0000143 | GradientNorm 7.392\nCurrent_step 27553 | BatchLoss 0.505 | Learning rate 0.0000140 | GradientNorm 11.752\nCurrent_step 27753 | BatchLoss 0.591 | Learning rate 0.0000137 | GradientNorm 12.698\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fab2f5868264f9db642f9f4d230b3a7"}},"metadata":{}},{"name":"stdout","text":"Epoch [22], ValLoss [10.401], TrainLoss [0.389]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd51e6b52b7b4eafbd596907390de89d"}},"metadata":{}},{"name":"stdout","text":"Current_step 27960 | BatchLoss 0.422 | Learning rate 0.0000134 | GradientNorm 7.627\nCurrent_step 28160 | BatchLoss 0.458 | Learning rate 0.0000131 | GradientNorm 7.260\nCurrent_step 28360 | BatchLoss 0.542 | Learning rate 0.0000128 | GradientNorm 8.792\nCurrent_step 28560 | BatchLoss 0.456 | Learning rate 0.0000125 | GradientNorm 9.077\nCurrent_step 28760 | BatchLoss 0.273 | Learning rate 0.0000122 | GradientNorm 6.821\nCurrent_step 28960 | BatchLoss 0.535 | Learning rate 0.0000119 | GradientNorm 7.975\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc24c7c6ed6e4a1eba7deebe3d2e3475"}},"metadata":{}},{"name":"stdout","text":"Epoch [23], ValLoss [10.583], TrainLoss [0.368]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0067d8162d56405d89e0e085f724fc4f"}},"metadata":{}},{"name":"stdout","text":"Current_step 29167 | BatchLoss 0.251 | Learning rate 0.0000116 | GradientNorm 6.703\nCurrent_step 29367 | BatchLoss 0.247 | Learning rate 0.0000113 | GradientNorm 4.601\nCurrent_step 29567 | BatchLoss 0.261 | Learning rate 0.0000110 | GradientNorm 6.351\nCurrent_step 29767 | BatchLoss 0.496 | Learning rate 0.0000107 | GradientNorm 8.801\nCurrent_step 29967 | BatchLoss 0.179 | Learning rate 0.0000104 | GradientNorm 5.610\nCurrent_step 30167 | BatchLoss 0.429 | Learning rate 0.0000101 | GradientNorm 9.021\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e114f80eb6174588a684cf756161872c"}},"metadata":{}},{"name":"stdout","text":"Epoch [24], ValLoss [10.676], TrainLoss [0.356]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3ec04b6805f4507ab041e2bb5713cd3"}},"metadata":{}},{"name":"stdout","text":"Current_step 30374 | BatchLoss 0.108 | Learning rate 0.0000098 | GradientNorm 4.972\nCurrent_step 30574 | BatchLoss 0.257 | Learning rate 0.0000095 | GradientNorm 5.649\nCurrent_step 30774 | BatchLoss 0.271 | Learning rate 0.0000092 | GradientNorm 9.189\nCurrent_step 30974 | BatchLoss 0.174 | Learning rate 0.0000089 | GradientNorm 5.618\nCurrent_step 31174 | BatchLoss 0.152 | Learning rate 0.0000086 | GradientNorm 5.952\nCurrent_step 31374 | BatchLoss 0.322 | Learning rate 0.0000083 | GradientNorm 7.213\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d370cc6c96b94f52800f3b9661eb76b3"}},"metadata":{}},{"name":"stdout","text":"Epoch [25], ValLoss [10.727], TrainLoss [0.338]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6857f8e64a6455cb65a9ca10a733665"}},"metadata":{}},{"name":"stdout","text":"Current_step 31581 | BatchLoss 0.499 | Learning rate 0.0000080 | GradientNorm 10.444\nCurrent_step 31781 | BatchLoss 0.268 | Learning rate 0.0000077 | GradientNorm 8.109\nCurrent_step 31981 | BatchLoss 0.313 | Learning rate 0.0000074 | GradientNorm 6.361\nCurrent_step 32181 | BatchLoss 0.270 | Learning rate 0.0000071 | GradientNorm 6.913\nCurrent_step 32381 | BatchLoss 0.130 | Learning rate 0.0000068 | GradientNorm 4.754\nCurrent_step 32581 | BatchLoss 0.201 | Learning rate 0.0000065 | GradientNorm 4.752\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e820fc47d02040dd8a80dd670805287e"}},"metadata":{}},{"name":"stdout","text":"Epoch [26], ValLoss [10.776], TrainLoss [0.328]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94757e3661ea41fc9db6c81114c41e80"}},"metadata":{}},{"name":"stdout","text":"Current_step 32788 | BatchLoss 0.323 | Learning rate 0.0000061 | GradientNorm 7.244\nCurrent_step 32988 | BatchLoss 0.365 | Learning rate 0.0000058 | GradientNorm 7.534\nCurrent_step 33188 | BatchLoss 0.560 | Learning rate 0.0000055 | GradientNorm 11.707\nCurrent_step 33388 | BatchLoss 0.325 | Learning rate 0.0000052 | GradientNorm 8.470\nCurrent_step 33588 | BatchLoss 0.428 | Learning rate 0.0000049 | GradientNorm 10.566\nCurrent_step 33788 | BatchLoss 0.245 | Learning rate 0.0000046 | GradientNorm 6.638\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00a93d274a66434f88c5bc92be01d625"}},"metadata":{}},{"name":"stdout","text":"Epoch [27], ValLoss [10.830], TrainLoss [0.315]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90de4f8f045944239a3bc7a4196516d6"}},"metadata":{}},{"name":"stdout","text":"Current_step 33995 | BatchLoss 0.324 | Learning rate 0.0000043 | GradientNorm 6.849\nCurrent_step 34195 | BatchLoss 0.279 | Learning rate 0.0000040 | GradientNorm 7.172\nCurrent_step 34395 | BatchLoss 0.224 | Learning rate 0.0000037 | GradientNorm 7.122\nCurrent_step 34595 | BatchLoss 0.626 | Learning rate 0.0000034 | GradientNorm 8.829\nCurrent_step 34795 | BatchLoss 0.281 | Learning rate 0.0000031 | GradientNorm 7.002\nCurrent_step 34995 | BatchLoss 0.369 | Learning rate 0.0000028 | GradientNorm 7.437\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5614564ba91436c86b1b7c2a6619bf9"}},"metadata":{}},{"name":"stdout","text":"Epoch [28], ValLoss [10.884], TrainLoss [0.304]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeca81888e464a5f87c4b4e04fe5fa98"}},"metadata":{}},{"name":"stdout","text":"Current_step 35202 | BatchLoss 0.342 | Learning rate 0.0000025 | GradientNorm 7.967\nCurrent_step 35402 | BatchLoss 0.201 | Learning rate 0.0000022 | GradientNorm 5.681\nCurrent_step 35602 | BatchLoss 0.162 | Learning rate 0.0000019 | GradientNorm 5.226\nCurrent_step 35802 | BatchLoss 0.377 | Learning rate 0.0000016 | GradientNorm 7.990\nCurrent_step 36002 | BatchLoss 0.199 | Learning rate 0.0000013 | GradientNorm 7.779\nCurrent_step 36202 | BatchLoss 0.355 | Learning rate 0.0000010 | GradientNorm 8.187\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a77d29aeec442f9b5d22999f26aeeea"}},"metadata":{}},{"name":"stdout","text":"Epoch [29], ValLoss [10.887], TrainLoss [0.299]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26826730ab9426d80643ba5f9b20477"}},"metadata":{}},{"name":"stdout","text":"Current_step 36409 | BatchLoss 0.241 | Learning rate 0.0000010 | GradientNorm 5.741\nCurrent_step 36609 | BatchLoss 0.225 | Learning rate 0.0000010 | GradientNorm 7.129\nCurrent_step 36809 | BatchLoss 0.325 | Learning rate 0.0000010 | GradientNorm 8.047\nCurrent_step 37009 | BatchLoss 0.364 | Learning rate 0.0000010 | GradientNorm 11.678\nCurrent_step 37209 | BatchLoss 0.252 | Learning rate 0.0000010 | GradientNorm 6.722\nCurrent_step 37409 | BatchLoss 0.333 | Learning rate 0.0000010 | GradientNorm 9.342\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f97ddd22cb58442db31bc919c92c756e"}},"metadata":{}},{"name":"stdout","text":"Epoch [30], ValLoss [10.901], TrainLoss [0.294]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df8f19eec9504cf28c1f063904ff9a8e"}},"metadata":{}},{"name":"stdout","text":"Current_step 37616 | BatchLoss 0.292 | Learning rate 0.0000010 | GradientNorm 6.374\nCurrent_step 37816 | BatchLoss 0.214 | Learning rate 0.0000010 | GradientNorm 6.836\nCurrent_step 38016 | BatchLoss 0.431 | Learning rate 0.0000010 | GradientNorm 8.430\nCurrent_step 38216 | BatchLoss 0.368 | Learning rate 0.0000010 | GradientNorm 8.888\nCurrent_step 38416 | BatchLoss 0.117 | Learning rate 0.0000010 | GradientNorm 5.152\nCurrent_step 38616 | BatchLoss 0.245 | Learning rate 0.0000010 | GradientNorm 8.970\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb4504a3a5e349fa87024d6f3a964a69"}},"metadata":{}},{"name":"stdout","text":"Epoch [31], ValLoss [10.913], TrainLoss [0.293]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"937f8105079e49d48e20ab29e3329a14"}},"metadata":{}},{"name":"stdout","text":"Current_step 38823 | BatchLoss 0.714 | Learning rate 0.0000010 | GradientNorm 9.899\nCurrent_step 39023 | BatchLoss 0.348 | Learning rate 0.0000010 | GradientNorm 8.036\nCurrent_step 39223 | BatchLoss 0.420 | Learning rate 0.0000010 | GradientNorm 10.377\nCurrent_step 39423 | BatchLoss 0.340 | Learning rate 0.0000010 | GradientNorm 6.584\nCurrent_step 39623 | BatchLoss 0.287 | Learning rate 0.0000010 | GradientNorm 10.133\nCurrent_step 39823 | BatchLoss 0.616 | Learning rate 0.0000010 | GradientNorm 8.078\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"279df409cee64a17a1eb08e1fcb564d4"}},"metadata":{}},{"name":"stdout","text":"Epoch [32], ValLoss [10.914], TrainLoss [0.292]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c246996ff6fc4a129837735b0b25f58f"}},"metadata":{}},{"name":"stdout","text":"Current_step 40030 | BatchLoss 0.340 | Learning rate 0.0000010 | GradientNorm 8.500\nCurrent_step 40230 | BatchLoss 0.231 | Learning rate 0.0000010 | GradientNorm 8.040\nCurrent_step 40430 | BatchLoss 0.427 | Learning rate 0.0000010 | GradientNorm 10.029\nCurrent_step 40630 | BatchLoss 0.277 | Learning rate 0.0000010 | GradientNorm 8.343\nCurrent_step 40830 | BatchLoss 0.324 | Learning rate 0.0000010 | GradientNorm 6.591\nCurrent_step 41030 | BatchLoss 0.277 | Learning rate 0.0000010 | GradientNorm 6.688\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e54d01fda34a1e8f954b6c9b40828e"}},"metadata":{}},{"name":"stdout","text":"Epoch [33], ValLoss [10.929], TrainLoss [0.290]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b74b5c413842d090822e49d2a53fe4"}},"metadata":{}},{"name":"stdout","text":"Current_step 41237 | BatchLoss 0.379 | Learning rate 0.0000010 | GradientNorm 9.989\nCurrent_step 41437 | BatchLoss 0.234 | Learning rate 0.0000010 | GradientNorm 7.993\nCurrent_step 41637 | BatchLoss 0.411 | Learning rate 0.0000010 | GradientNorm 11.872\nCurrent_step 41837 | BatchLoss 0.139 | Learning rate 0.0000010 | GradientNorm 6.005\nCurrent_step 42037 | BatchLoss 0.283 | Learning rate 0.0000010 | GradientNorm 6.701\nCurrent_step 42237 | BatchLoss 0.310 | Learning rate 0.0000010 | GradientNorm 6.978\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc346168b3ba4bff8ac79ff39aece035"}},"metadata":{}},{"name":"stdout","text":"Epoch [34], ValLoss [10.940], TrainLoss [0.288]\n","output_type":"stream"}],"execution_count":419},{"cell_type":"code","source":"test_scorer = make_scorer(test['text'].values, test['label'].values, tokenizer, maxlen=MAX_SEQLEN)\ntest_scorer(trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T01:39:38.708289Z","iopub.execute_input":"2024-11-22T01:39:38.708622Z","iopub.status.idle":"2024-11-22T01:39:41.273659Z","shell.execute_reply.started":"2024-11-22T01:39:38.708592Z","shell.execute_reply":"2024-11-22T01:39:41.272583Z"}},"outputs":[{"execution_count":420,"output_type":"execute_result","data":{"text/plain":"('f1', 0.7530229876859422)"},"metadata":{}}],"execution_count":420}]}